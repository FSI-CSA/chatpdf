$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: qaragcogsearchlc
model: azureml:qaragcogsearchlc-model:1
  # You can also specify model files path inline
  # path: examples/flows/chat/basic-chat
environment: 
  name: chatpdfenv
  image: akshata13/chatpdfenv:latest
  # inference config is used to build a serving container for online deployments
  inference_config:
    liveness_route:
      path: /health
      port: 8080
    readiness_route:
      path: /health
      port: 8080
    scoring_route:
      path: /score
      port: 8080
instance_type: Standard_E16s_v3
instance_count: 1
environment_variables:

  # "compute" mode is the default mode, if you want to deploy to serving mode, you need to set this env variable to "serving"
  PROMPTFLOW_RUN_MODE: serving
  UAI_CLIENT_ID: dfd89515-f588-441f-9cbd-3f0082572e5c

  # for pulling connections from workspace
  PRT_CONFIG_OVERRIDE: deployment.subscription_id=e2171f6d-2650-45e6-af7e-6d6e44ca92b1,deployment.resource_group=dataai,deployment.workspace_name=dataaiamlwks,deployment.endpoint_name=qaragcogsearchlc,deployment_name=blue,mt_service_endpoint=https://southcentralus.api.azureml.ms
  #PRT_CONFIG_OVERRIDE: deployment.subscription_id=<sub-id>,deployment.resource_group=o<resource-group>,deployment.workspace_name=<workspace-name>,deployment.endpoint_name=<endpoint-name>,deployment.deployment_name=<deployment-name>

  # (Optional) When there are multiple fields in the response, using this env variable will filter the fields to expose in the response.
  # For example, if there are 2 flow outputs: "answer", "context", and I only want to have "answer" in the endpoint response, I can set this env variable to '["answer"]'.
  # If you don't set this environment, by default all flow outputs will be included in the endpoint response.
  # PROMPTFLOW_RESPONSE_INCLUDED_FIELDS: '["category", "evidence"]'