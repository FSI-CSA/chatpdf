{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\n",
    "- Data-aware: connect a language model to other sources of data\n",
    "- Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The LangChain framework is designed around these principles.\n",
    "\n",
    "We will use Langchain framework for rest of the workshop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering over the docs/index\n",
    "Question answering in this context refers to question answering over your document data.  For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURESEARCH_FIELDS_ID\"] = \"id\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT\"] = \"content\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"contentVector\"\n",
    "\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(os.getenv(\"AZURESEARCH_FIELDS_METADATA\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer for a question from the document we already indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\embeddings\\openai.py:217: UserWarning: WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI, ChatLiteLLM\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"{OpenAiEndPoint}\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(engine=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a unified platform that provides a comprehensive suite of analytics services, including data engineering, data science, data integration, and business intelligence. It brings together components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. Fabric offers a shared SaaS foundation, allowing for centralized administration, governance, and access to assets. It simplifies analytics needs and eliminates the need to piece together services from multiple vendors. Fabric is currently in preview and may undergo substantial modifications before its release.\n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "# The simplest way of using Langchain and LLM is to use load_qa_chain and run it with a query and a list of documents.\n",
    "\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use Langchain Azure Search Implmentation to search data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "() Invalid expression: Could not find a property named 'metadata' on type 'search.document'.\r\nParameter name: $select\nCode: \nMessage: Invalid expression: Could not find a property named 'metadata' on type 'search.document'.\r\nParameter name: $select",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\repos\\chatpdf\\Workshop\\2_AskQuestion.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m vectorStore: AzureSearch \u001b[39m=\u001b[39m AzureSearch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     azure_search_endpoint\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m{\u001b[39;00mSearchService\u001b[39m}\u001b[39;00m\u001b[39m.search.windows.net\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     azure_search_key\u001b[39m=\u001b[39mSearchKey,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     embedding_function\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39membed_query,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Perform a similarity search\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m docs \u001b[39m=\u001b[39m vectorStore\u001b[39m.\u001b[39;49msimilarity_search(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat is Microsoft Fabric\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     search_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msimilarity\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/chatpdf/Workshop/2_AskQuestion.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpage_content)\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:320\u001b[0m, in \u001b[0;36mAzureSearch.similarity_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m search_type \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msearch_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type)\n\u001b[0;32m    319\u001b[0m \u001b[39mif\u001b[39;00m search_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 320\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_search(query, k\u001b[39m=\u001b[39mk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[39melif\u001b[39;00m search_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhybrid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    322\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhybrid_search(query, k\u001b[39m=\u001b[39mk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:351\u001b[0m, in \u001b[0;36mAzureSearch.vector_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvector_search\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    341\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39m    Returns the most similar indexed documents to the query text.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39m        List[Document]: A list of documents that are most similar to the query text.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvector_search_with_score(\n\u001b[0;32m    352\u001b[0m         query, k\u001b[39m=\u001b[39;49mk, filters\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mfilters\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    354\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:385\u001b[0m, in \u001b[0;36mAzureSearch.vector_search_with_score\u001b[1;34m(self, query, k, filters)\u001b[0m\n\u001b[0;32m    370\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    371\u001b[0m     search_text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    372\u001b[0m     vectors\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    384\u001b[0m \u001b[39m# Convert results to Document objects\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    386\u001b[0m     (\n\u001b[0;32m    387\u001b[0m         Document(\n\u001b[0;32m    388\u001b[0m             page_content\u001b[39m=\u001b[39mresult[FIELDS_CONTENT],\n\u001b[0;32m    389\u001b[0m             metadata\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(result[FIELDS_METADATA]),\n\u001b[0;32m    390\u001b[0m         ),\n\u001b[0;32m    391\u001b[0m         \u001b[39mfloat\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39m@search.score\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results\n\u001b[0;32m    394\u001b[0m ]\n\u001b[0;32m    395\u001b[0m \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    370\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    371\u001b[0m     search_text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    372\u001b[0m     vectors\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    384\u001b[0m \u001b[39m# Convert results to Document objects\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    386\u001b[0m     (\n\u001b[0;32m    387\u001b[0m         Document(\n\u001b[0;32m    388\u001b[0m             page_content\u001b[39m=\u001b[39mresult[FIELDS_CONTENT],\n\u001b[0;32m    389\u001b[0m             metadata\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(result[FIELDS_METADATA]),\n\u001b[0;32m    390\u001b[0m         ),\n\u001b[0;32m    391\u001b[0m         \u001b[39mfloat\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39m@search.score\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results\n\u001b[0;32m    394\u001b[0m ]\n\u001b[0;32m    395\u001b[0m \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\search\\documents\\_paging.py:54\u001b[0m, in \u001b[0;36mSearchItemPaged.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m     first_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_iterator_instance()\n\u001b[0;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_page_iterator \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(first_iterator)\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_page_iterator)\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\core\\paging.py:75\u001b[0m, in \u001b[0;36mPageIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEnd of paging\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_next(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontinuation_token)\n\u001b[0;32m     76\u001b[0m \u001b[39mexcept\u001b[39;00m AzureError \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m error\u001b[39m.\u001b[39mcontinuation_token:\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\search\\documents\\_paging.py:124\u001b[0m, in \u001b[0;36mSearchPageIterator._get_next_cb\u001b[1;34m(self, continuation_token)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_next_cb\u001b[39m(\u001b[39mself\u001b[39m, continuation_token):\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m continuation_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mdocuments\u001b[39m.\u001b[39msearch_post(search_request\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_query\u001b[39m.\u001b[39mrequest, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs)\n\u001b[0;32m    126\u001b[0m     _next_link, next_page_request \u001b[39m=\u001b[39m unpack_continuation_token(continuation_token)\n\u001b[0;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mdocuments\u001b[39m.\u001b[39msearch_post(search_request\u001b[39m=\u001b[39mnext_page_request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\search\\documents\\_generated\\operations\\_documents_operations.py:787\u001b[0m, in \u001b[0;36mDocumentsOperations.search_post\u001b[1;34m(self, search_request, request_options, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     map_error(status_code\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code, response\u001b[39m=\u001b[39mresponse, error_map\u001b[39m=\u001b[39merror_map)\n\u001b[0;32m    786\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize\u001b[39m.\u001b[39mfailsafe_deserialize(_models\u001b[39m.\u001b[39mSearchError, pipeline_response)\n\u001b[1;32m--> 787\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(response\u001b[39m=\u001b[39mresponse, model\u001b[39m=\u001b[39merror)\n\u001b[0;32m    789\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize(\u001b[39m\"\u001b[39m\u001b[39mSearchDocumentsResult\u001b[39m\u001b[39m\"\u001b[39m, pipeline_response)\n\u001b[0;32m    791\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: () Invalid expression: Could not find a property named 'metadata' on type 'search.document'.\r\nParameter name: $select\nCode: \nMessage: Invalid expression: Could not find a property named 'metadata' on type 'search.document'.\r\nParameter name: $select"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    ScoringProfile,\n",
    "    TextWeights,\n",
    ")\n",
    "\n",
    "\n",
    "fields=[\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String,\n",
    "                        searchable=True, retrievable=True, analyzer_name=\"en.microsoft\"),\n",
    "        SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"vectorConfig\"),\n",
    "        SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True),\n",
    "]\n",
    "vectorStore: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "    azure_search_key=SearchKey,\n",
    "    index_name=indexName,\n",
    "    fields=fields,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "# Perform a similarity search\n",
    "docs = vectorStore.similarity_search(\n",
    "    query=\"What is Microsoft Fabric\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about we ask a question for which the answer is not in the document we have indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't know any jokes.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Tell me a Joke\"\n",
    "#query = \"Who is the CEO of Microsoft\"\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't want to have LLM answer the question outside of the document we have indexed in Vector Store. We can use the custom prompt to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {question}\n",
    "            =========\n",
    "            {summaries}\n",
    "            =========\n",
    "            \"\"\"\n",
    "#qaPrompt = load_prompt('lc://prompts/qa_with_sources/stuff/basic.json')\n",
    "qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain type\n",
    "This category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents.\n",
    "\n",
    "A big part of this is understanding how to pass multiple documents to the language model. There are a few different methods, or chains, for doing so. LangChain supports four of the more common ones - and we are actively looking to include more, so if you have any ideas please reach out! Note that there is not one best method - the decision of which one to use is often very context specific. In order from simplest to most complex\n",
    "\n",
    "##### Stuff\n",
    "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
    "\n",
    "- Pros: Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
    "- Cons: Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "\n",
    "##### Map-Reduce\n",
    "This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
    "\n",
    "- Pros: Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combined call.\n",
    "\n",
    "##### Refine\n",
    "This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "\n",
    "- Pros: Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents.\n",
    "\n",
    "##### Map-Rerank\n",
    "This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "- Pros: Similar pros as MapReduceDocumentsChain. Requires fewer calls, compared to MapReduceDocumentsChain.\n",
    "- Cons: Cannot combine information between documents. This means it is most useful when you expect there to be a single simple answer in a single document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's test the same question with Map Reduce Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a platform that brings together components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. It provides a shared SaaS foundation for experiences such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI. Fabric allows creators to concentrate on producing their best work without the need to integrate, manage, or understand the underlying infrastructure. It offers advantages such as extensive integrated analytics, shared experiences, easy access to assets, a unified data lake, and centralized administration and governance. Fabric includes industry-leading experiences in categories such as Data Engineering, Data Factory, and Data Science, providing a comprehensive suite of services for enterprises. With Fabric, users can enjoy a highly integrated, end-to-end, and easy-to-use product that simplifies their analytics needs.\n",
      "\n",
      "SOURCES:\n",
      "- Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"map_reduce\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text.\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            Relevant text, if any :\"\"\"\n",
    "\n",
    "qaPrompt = PromptTemplate(\n",
    "    template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combinePromptTemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\"\"\"\n",
    "combinePrompt = PromptTemplate(\n",
    "    template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, \n",
    "                                     combine_prompt=combinePrompt, \n",
    "                                     return_intermediate_steps=True)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query})\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience. Microsoft Fabric offers the comprehensive set of analytics experiences designed to work together seamlessly. Each experience is tailored to a specific persona and a specific task. Fabric includes industry-leading experiences in the following categories for an end-to- end analytical need. Data Engineering - Data Engineering experience provides a world class Spark platform with great authoring experiences, enabling data engineers to perform large scale data transformation and democratize data through the lakehouse. Microsoft Fabric Spark's integration with Data Factory enables notebooks and spark jobs to be scheduled and orchestrated. For more information, see What is Data engineering in Microsoft Fabric? Data Factory - Azure Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. You can use more than 200 native connectors to connect to data sources on-premises and in the cloud. For more information, see What is Data Factory in Microsoft Fabric? Data Science - Data Science experience enables you to build, deploy, and operationalize machine learning models seamlessly within your Fabric experience. It integrates with Azure Machine Learning to provide built-in experiment tracking."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that brings together components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. It provides a shared SaaS foundation for experiences such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI. It offers advantages such as extensive integrated analytics, shared experiences, easy access to assets, a unified data lake, and centralized administration and governance."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This time with Refine Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Fabric brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. This integration provides advantages such as an extensive range of deeply integrated analytics, shared experiences across familiar and easy-to-learn interfaces, easy access and reuse of assets for developers, a unified data lake that retains data where it is while using preferred analytics tools, and centralized administration and governance across all experiences. The Microsoft Fabric SaaS experience ensures seamless integration of data and services, with centrally configured enterprise capabilities and automatic application of permissions and data sensitivity labels across the suite.\n",
      "\n",
      "Source: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"refine\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "    \n",
    "refineTemplate = (\n",
    "                    \"The original question is as follows: {question}\\n\"\n",
    "                    \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "                    \"We have the opportunity to refine the existing answer\"\n",
    "                    \"(only if needed) with some more context below.\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"Given the new context, refine the original answer to better \"\n",
    "                    \"If you do update it, please update the sources as well. \"\n",
    "                    \"If the context isn't useful, return the original answer.\"\n",
    "                )\n",
    "refinePrompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refineTemplate,\n",
    ")\n",
    "\n",
    "qaTemplate = (\n",
    "    \"Answer the question as truthfully as possible using the provided text below, and if the answer is not contained within the text below, say \\\"I don't know\\\"\\n\"\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {question}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "qaPrompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"], template=qaTemplate\n",
    ")\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, refine_prompt=refinePrompt,\n",
    "                                     return_intermediate_steps=True)\n",
    "\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "modifiedAnswer = answer['output_text']\n",
    "print(modifiedAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It includes industry-leading experiences in data engineering, data factory, and data science."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that offers a comprehensive set of analytics experiences designed to work together seamlessly. It brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. Fabric includes experiences such as Data Engineering, Data Factory, Data Science, Data Warehouse, Real-Time Analytics, and Power BI, all built on a shared SaaS foundation. This integration provides advantages such as extensive range of deeply integrated analytics, shared experiences across familiar and easy-to-learn interfaces, easy access and reuse of assets for developers, a unified data lake that retains data where it is while using preferred analytics tools, and centralized administration and governance across all experiences. The Microsoft Fabric SaaS experience ensures seamless integration of data and services, with centrally configured enterprise capabilities and automatic application of permissions and data sensitivity labels across the suite.\n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Fabric brings together new and existing components from Power BI, Azure Synapse, and Azure Data Explorer into a single integrated environment. This integration provides advantages such as an extensive range of deeply integrated analytics, shared experiences across familiar and easy-to-learn interfaces, easy access and reuse of assets for developers, a unified data lake that retains data where it is while using preferred analytics tools, and centralized administration and governance across all experiences. The Microsoft Fabric SaaS experience ensures seamless integration of data and services, with centrally configured enterprise capabilities and automatic application of permissions and data sensitivity labels across the suite.\n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
