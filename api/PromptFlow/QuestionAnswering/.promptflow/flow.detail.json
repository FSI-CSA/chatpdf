{
  "flow_runs": [
    {
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "status": "Completed",
      "error": null,
      "inputs": {
        "chainType": "stuff",
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        },
        "question": "What is the purpose of BERT?"
      },
      "output": {
        "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
        "context": [
          "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
                ],
                "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
                "error": ""
              }
            }
          ]
        }
      },
      "metrics": null,
      "request": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "root_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "source_run_id": null,
      "flow_id": "default_flow_id",
      "start_time": "2023-09-15T21:55:10.022265Z",
      "end_time": "2023-09-15T21:55:12.494933Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1694832910.022265,
          "end_time": 1694832910.022265,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        },
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1694832910.037893,
          "end_time": 1694832910.037893,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        },
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "What is the purpose of BERT?"
          },
          "output": [
            -0.022036384791135788,
            -0.0164797380566597,
            0.0005999039276503026,
            -0.007227716967463493,
            0.003868594765663147,
            0.01381010189652443,
            -0.006500869523733854,
            -0.011711074970662594,
            -0.01653408259153366,
            -0.015121144242584705,
            0.0027137903962284327,
            0.027362070977687836,
            -0.025758931413292885,
            0.00944222416728735,
            3.94045164284762e-05,
            0.0016226699808612466,
            0.016561252996325493,
            0.015487965196371078,
            0.004052004776895046,
            0.0009892258094623685,
            -0.005791004281491041,
            -0.014170128852128983,
            -0.019210509955883026,
            0.020990267395973206,
            -0.006324252113699913,
            -0.02627519704401493,
            0.05154503509402275,
            -0.033502914011478424,
            0.00033030801569111645,
            -0.013558762148022652,
            0.027144696563482285,
            0.030269460752606392,
            -0.010019626468420029,
            -0.008131181821227074,
            -0.013069668784737587,
            0.0003339167742524296,
            0.01770247146487236,
            -0.002297721104696393,
            0.017009589821100235,
            -0.01224092673510313,
            0.015800440683960915,
            0.01892520673573017,
            0.013878031633794308,
            -0.001744094304740429,
            -0.0005765530513599515,
            0.003309873165562749,
            0.01252623088657856,
            -0.004153899382799864,
            0.0005892898770980537,
            -0.023788969963788986,
            0.018653487786650658,
            0.01949581503868103,
            -0.02384331449866295,
            -0.002343573607504368,
            -0.03442675620317459,
            -0.0030534386169165373,
            -0.0077575682662427425,
            0.02582686021924019,
            0.009068611077964306,
            -0.02248472161591053,
            0.010753266513347626,
            0.0006746265571564436,
            -0.015664581209421158,
            0.019156167283654213,
            0.0048841433599591255,
            -0.009299571625888348,
            -0.022946642711758614,
            0.029101070016622543,
            0.010603821836411953,
            0.008885201066732407,
            0.007628501858562231,
            0.020623447373509407,
            0.019455058500170708,
            -0.011086122132837772,
            0.03654616326093674,
            -0.002844554837793112,
            -0.01502604316920042,
            -0.005852140951901674,
            0.008036079816520214,
            0.0004491849394980818,
            0.006405767984688282,
            0.02306891605257988,
            -0.02109895646572113,
            0.020922338590025902,
            0.013062875717878342,
            -0.007594536989927292,
            0.013103633187711239,
            0.01690090261399746,
            -0.019577331840991974,
            -0.0003455921832937747,
            0.006201978772878647,
            0.002939656376838684,
            0.03314967826008797,
            0.004282965790480375,
            0.000431353400927037,
            0.03024228848516941,
            -0.004602235276252031,
            0.029101070016622543,
            0.0016498418990522623,
            -0.035106055438518524,
            0.009027853608131409,
            -0.0075266072526574135,
            -0.007940978743135929,
            -0.01224092673510313,
            -0.019224096089601517,
            0.0021533705294132233,
            0.00983621645718813,
            -0.010372860357165337,
            0.028150055557489395,
            -0.008076838217675686,
            -0.007716810330748558,
            0.03203563392162323,
            -0.0034202588722109795,
            -0.020215868949890137,
            0.005529474932700396,
            0.021533705294132233,
            0.0010299836285412312,
            0.0005413145408965647,
            0.017457924783229828,
            0.020732134580612183,
            0.03328553959727287,
            -0.002212808933109045,
            0.0011565026361495256,
            -0.01827308163046837,
            0.022348862141370773,
            -0.008137974888086319,
            -0.021615220233798027,
            -0.019998494535684586,
            0.008701791055500507,
            -0.029481476172804832,
            0.006055930163711309,
            -0.0036444268189370632,
            0.0013798214495182037,
            -0.023816142231225967,
            -0.002878519706428051,
            0.023191189393401146,
            -0.00796135701239109,
            -0.010107934474945068,
            -0.026424642652273178,
            -0.01768888533115387,
            0.016819385811686516,
            0.022593408823013306,
            -0.023177603259682655,
            -0.03518756851553917,
            -0.026601258665323257,
            0.02546004019677639,
            0.0007625105208717287,
            0.02842177450656891,
            0.010114727541804314,
            0.002683222061023116,
            0.0042456043884158134,
            -0.031138960272073746,
            -0.018055707216262817,
            0.009591669775545597,
            0.0022875317372381687,
            0.013979925774037838,
            -0.00032075541093945503,
            0.0026458606589585543,
            -0.000471262086648494,
            -0.00011494974023662508,
            0.0030279650818556547,
            0.014550535008311272,
            0.011554837226867676,
            -0.009598462842404842,
            0.03760586678981781,
            0.04051325470209122,
            -0.004279569257050753,
            -0.0026611448265612125,
            0.012397164478898048,
            0.003573100548237562,
            -0.008688204921782017,
            0.01570533961057663,
            -0.026981664821505547,
            0.004629407078027725,
            0.015433620661497116,
            0.00746547058224678,
            0.017430752515792847,
            0.007805119268596172,
            0.0007680298294872046,
            -0.013361766003072262,
            -0.0051320865750312805,
            0.006398974917829037,
            0.03399200737476349,
            0.005003020167350769,
            -0.015257003717124462,
            -0.002204317832365632,
            0.003770096693187952,
            -0.035350602120161057,
            0.0028004006016999483,
            -0.044643379747867584,
            0.018205150961875916,
            0.019061066210269928,
            0.011711074970662594,
            -0.004673561546951532,
            -0.6364738345146179,
            -0.014455433934926987,
            0.016629183664917946,
            -0.00835534930229187,
            0.021329917013645172,
            -0.013083254918456078,
            -0.010352482087910175,
            -0.00566193787381053,
            -0.01171786803752184,
            0.03263341262936592,
            0.005447959527373314,
            -0.012607746757566929,
            -0.005906485021114349,
            -0.01502604316920042,
            -0.0013458565808832645,
            -0.022511892020702362,
            -0.00936070829629898,
            -0.00402143644168973,
            0.01869424618780613,
            -0.003973885904997587,
            -0.02446826733648777,
            0.02124840021133423,
            -0.04380105063319206,
            0.005852140951901674,
            -0.01752585545182228,
            0.007132615428417921,
            0.002350366674363613,
            -0.009313157759606838,
            0.007295646704733372,
            -0.007268474902957678,
            -0.01323269959539175,
            0.03336705267429352,
            0.03956224024295807,
            -0.015284175984561443,
            0.031709570437669754,
            0.011860520578920841,
            -0.002382633276283741,
            0.015759684145450592,
            -0.031193304806947708,
            0.0256638303399086,
            -0.00866782572120428,
            -0.019618088379502296,
            0.02329987660050392,
            -0.007010342087596655,
            0.03217149153351784,
            0.007248095702379942,
            0.01232923474162817,
            0.021425018087029457,
            0.0018884448800235987,
            0.007431505713611841,
            -0.027606617659330368,
            0.008722169324755669,
            0.0014333160361275077,
            -0.008464036509394646,
            0.014401090331375599,
            -0.0018341010436415672,
            0.022837955504655838,
            -0.014061441645026207,
            0.0002553731028456241,
            -0.022606994956731796,
            0.011643145233392715,
            0.034345243126153946,
            -0.010902712121605873,
            -0.02567741461098194,
            -0.011004606261849403,
            0.015229832381010056,
            0.0035323428455740213,
            0.012770777568221092,
            0.011887691915035248,
            -0.04477923735976219,
            0.010481548495590687,
            0.024984532967209816,
            0.0006393880466930568,
            -0.0006792966742068529,
            0.02008001133799553,
            0.017743229866027832,
            -0.00228923000395298,
            -0.015121144242584705,
            -0.0004844234499614686,
            0.022973814979195595,
            -0.009252021089196205,
            -0.005587215535342693,
            0.012607746757566929,
            -0.0028190813027322292,
            -0.018748588860034943,
            0.017797574400901794,
            -0.006918637081980705,
            -0.02309608832001686,
            0.005352857988327742,
            0.026207266375422478,
            -0.0012125446228310466,
            0.02782399207353592,
            -0.021601635962724686,
            -0.03942637890577316,
            0.02248472161591053,
            0.012369993142783642,
            0.003501774510368705,
            -0.016642769798636436,
            0.03247038275003433,
            -0.007241302635520697,
            0.002798702334985137,
            -0.017607370391488075,
            0.020243041217327118,
            0.023761799558997154,
            0.00856593158096075,
            0.016438979655504227,
            0.02270209603011608,
            0.020637033507227898,
            0.002071854891255498,
            -0.017199791967868805,
            0.004792438354343176,
            -0.016357464715838432,
            0.00825345516204834,
            0.008776513859629631,
            -0.006633332464843988,
            -0.0329594761133194,
            0.014183714985847473,
            0.0027817199006676674,
            0.012220547534525394,
            -0.021234814077615738,
            0.0019427885999903083,
            -0.009598462842404842,
            0.012444715946912766,
            -0.016642769798636436,
            0.019210509955883026,
            0.017675301060080528,
            -0.0031536349561065435,
            0.004310137592256069,
            -0.020650619640946388,
            -0.005298514384776354,
            -0.021832596510648727,
            -0.006426146719604731,
            0.0053800297901034355,
            -0.012152617797255516,
            0.04393691197037697,
            0.03899163007736206,
            0.03260624036192894,
            -0.0015937999123707414,
            -0.02050117403268814,
            0.005006416700780392,
            -0.012315649539232254,
            -0.010495133697986603,
            0.01567816734313965,
            -0.005740057211369276,
            -0.052034128457307816,
            -0.016167260706424713,
            0.008891994133591652,
            0.003070421051234007,
            0.001690599718131125,
            -0.011554837226867676,
            0.0015428526094183326,
            -0.02624802477657795,
            -0.018585557118058205,
            -0.005899691954255104,
            -0.013090047053992748,
            -0.028557633981108665,
            -0.009408259764313698,
            -0.04668127000331879,
            -0.019536573439836502,
            -0.031709570437669754,
            0.010325309820473194,
            0.00373273529112339,
            -0.011989586986601353,
            -0.012770777568221092,
            0.0011378219351172447,
            -0.03497019410133362,
            0.005515889264643192,
            0.022213002666831017,
            -0.027525102719664574,
            -0.033910490572452545,
            0.028938040137290955,
            -0.01846328377723694,
            -0.01033210288733244,
            0.005774022080004215,
            -0.026682773604989052,
            0.013776137493550777,
            -0.03325836732983589,
            -0.016017816960811615,
            0.0003791324852500111,
            -0.016044987365603447,
            0.005030191969126463,
            0.030704211443662643,
            -0.010012833401560783,
            -0.009224848821759224,
            0.012220547534525394,
            0.0020344937220215797,
            0.017172621563076973,
            -0.00310948072001338,
            -0.02488943189382553,
            0.00974111445248127,
            0.00874934159219265,
            0.026587672531604767,
            -0.003061929950490594,
            0.02090875245630741,
            0.012573782354593277,
            0.006110273767262697,
            0.016357464715838432,
            -0.009122954681515694,
            0.0008754436275921762,
            -0.005172844510525465,
            0.025297008454799652,
            0.010732888244092464,
            0.00034325712476857007,
            -0.012940602377057076,
            -0.008294212631881237,
            0.017457924783229828,
            -8.363840606762096e-05,
            -0.016438979655504227,
            0.008219489827752113,
            -0.00042880602995865047,
            0.01461846474558115,
            -0.029508648440241814,
            -0.02325911819934845,
            -0.0005476829828694463,
            -0.008708584122359753,
            0.021044611930847168,
            -0.009068611077964306,
            0.02289229817688465,
            -0.034263726323843,
            0.0033591222018003464,
            0.003783682594075799,
            -0.01793343387544155,
            0.004975848365575075,
            0.0009900749428197742,
            -0.011765418574213982,
            0.006297080311924219,
            -0.008097216486930847,
            -0.012702848762273788,
            0.012410750612616539,
            -0.022362448275089264,
            0.0005455601494759321,
            0.028285915032029152,
            0.02999774180352688,
            0.020025666803121567,
            -0.0012032042723149061,
            -0.00856593158096075,
            0.014469020068645477,
            -0.010196243412792683,
            0.025595899671316147,
            -0.017159035429358482,
            0.0026017064228653908,
            0.01154804416000843,
            0.013110426254570484,
            -0.0015776666114106774,
            0.03545928746461868,
            0.015814026817679405,
            0.03244321048259735,
            0.010651372373104095,
            -0.016072159633040428,
            0.02287871204316616,
            -0.01431957446038723,
            -0.008878407999873161,
            -0.030731383711099625,
            0.0011514079524204135,
            0.0240606889128685,
            -0.00924522802233696,
            0.0040757800452411175,
            0.012899843975901604,
            -0.0057094888761639595,
            0.033883318305015564,
            0.016357464715838432,
            0.008362142369151115,
            0.022797197103500366,
            0.007805119268596172,
            0.019183339551091194,
            -0.014455433934926987,
            -0.014591293409466743,
            -0.0058079869486391544,
            -0.009184091351926327,
            -0.004938486963510513,
            -0.017254136502742767,
            -0.015080386772751808,
            0.0010206432780250907,
            -0.010617407038807869,
            0.01441467646509409,
            0.03024228848516941,
            -0.02490301802754402,
            0.021438604220747948,
            -0.0067929672077298164,
            0.034127864986658096,
            -0.0023045141715556383,
            -0.029916226863861084,
            0.03160088136792183,
            -0.006130652967840433,
            -0.027959851548075676,
            -0.0171318631619215,
            0.0032283575274050236,
            -0.02169673703610897,
            -0.004269379656761885,
            -0.018830103799700737,
            0.0016956944018602371,
            0.009007474407553673,
            -0.0019852446857839823,
            0.025120392441749573,
            0.00111829221714288,
            0.013293836265802383,
            0.007302439771592617,
            -0.01986263506114483,
            0.004432410933077335,
            0.004486754536628723,
            0.0019190132152289152,
            0.00785946287214756,
            -0.006490679923444986,
            -0.028965210542082787,
            0.02029738575220108,
            0.001125934300944209,
            -0.029209759086370468,
            -0.016642769798636436,
            -0.016466151922941208,
            -0.015977058559656143,
            0.038257990032434464,
            0.009462603367865086,
            0.01868066005408764,
            -0.019346369430422783,
            -5.301698547555134e-05,
            0.0292369294911623,
            0.007893427275121212,
            -0.006996755953878164,
            0.008511587977409363,
            0.004408635664731264,
            -0.028313087299466133,
            -0.012268098071217537,
            -0.01671069860458374,
            0.008436865173280239,
            0.04317609965801239,
            0.04755076766014099,
            -0.004174278117716312,
            0.019020307809114456,
            -0.012757192365825176,
            0.03635596111416817,
            -0.025758931413292885,
            -0.0389372855424881,
            -0.021194057539105415,
            -0.01649332419037819,
            0.0015802140114828944,
            -0.02664201706647873,
            0.006531437858939171,
            -0.022810783237218857,
            0.005254359915852547,
            0.01025058701634407,
            -0.0003099291061516851,
            -0.02626161091029644,
            0.033747460693120956,
            -0.016547666862607002,
            -0.0009994152933359146,
            0.013701414689421654,
            0.006076308898627758,
            0.010467962361872196,
            0.001453694887459278,
            -0.019427886232733727,
            -0.012254512868821621,
            0.010807610116899014,
            -0.0035323428455740213,
            -0.011588801629841328,
            -0.016561252996325493,
            -0.022579822689294815,
            -0.01753944158554077,
            0.00787304900586605,
            -0.02051476016640663,
            0.03939921036362648,
            0.001176881487481296,
            0.006545023526996374,
            0.011004606261849403,
            0.020025666803121567,
            0.0029600353445857763,
            0.02662843093276024,
            -0.002523587318137288,
            -0.01540644932538271,
            7.790684321662411e-05,
            -0.030378147959709167,
            0.004031626041978598,
            0.016574839130043983,
            0.011317082680761814,
            -0.002888709306716919,
            0.0012337726075202227,
            -0.004809420555830002,
            -0.04111103713512421,
            -0.005013209767639637,
            0.014469020068645477,
            -0.014699980616569519,
            0.012471887283027172,
            0.010196243412792683,
            -0.01827308163046837,
            -0.02385690063238144,
            -0.017403582111001015,
            -0.014047855511307716,
            0.01872141659259796,
            -0.007275267504155636,
            0.00365801271982491,
            0.004639596678316593,
            -0.017362823709845543,
            -0.0244275089353323,
            -0.015596652403473854,
            0.04627368971705437,
            -0.008212696760892868,
            -0.04032305255532265,
            -0.04727904871106148,
            -0.016765043139457703,
            0.01221375446766615,
            0.011486907489597797,
            0.03260624036192894,
            -0.020786479115486145,
            -0.001669371617026627,
            -0.0023147035390138626,
            -0.007574158255010843,
            -0.03445392847061157,
            0.0223081037402153,
            -0.04051325470209122,
            0.0038006650283932686,
            -0.01102498546242714,
            -0.00746547058224678,
            -0.027565861120820045,
            -0.019332783296704292,
            0.0354321151971817,
            0.002744358731433749,
            0.005644955672323704,
            0.02961733564734459,
            -0.028965210542082787,
            0.01846328377723694,
            0.019984908401966095,
            0.0072005451656877995,
            0.005634766072034836,
            0.012886258773505688,
            -0.028313087299466133,
            -0.011955621652305126,
            -0.009285985492169857,
            -0.010420411825180054,
            -0.03325836732983589,
            0.03165522590279579,
            -0.006307269912213087,
            0.0036274443846195936,
            -0.021126126870512962,
            0.01733565144240856,
            -0.01072609517723322,
            0.008925958536565304,
            -0.023394977673888206,
            -0.00310948072001338,
            -0.01711827702820301,
            0.0313563346862793,
            -0.0038855771999806166,
            -0.001333968946710229,
            0.024970946833491325,
            -0.012268098071217537,
            -0.0019478832837194204,
            0.0025490608531981707,
            -0.02308250218629837,
            0.016588425263762474,
            0.019006721675395966,
            -0.0026017064228653908,
            0.021520119160413742,
            0.005498906597495079,
            -0.01688731648027897,
            -0.007037513889372349,
            0.025609485805034637,
            -0.024509025737643242,
            0.01668352633714676,
            -0.0017101294361054897,
            -0.0278511643409729,
            -0.03143785148859024,
            -0.016832971945405006,
            -0.002584723988547921,
            0.008932751603424549,
            -0.01461846474558115,
            -0.0012813233770430088,
            0.012179790064692497,
            -0.0006381143466569483,
            0.009992454200983047,
            -0.013225906528532505,
            0.01889803446829319,
            -0.022960228845477104,
            -0.014808667823672295,
            0.008531966246664524,
            0.001946185017004609,
            0.03206280618906021,
            -0.017634542658925056,
            0.02999774180352688,
            -0.030079258605837822,
            -0.0076692597940564156,
            0.0005748548428528011,
            -0.02983471192419529,
            -0.0004895181627944112,
            0.004462979268282652,
            0.03673636540770531,
            0.02324553392827511,
            0.03184542804956436,
            -0.000315448414767161,
            0.01082798931747675,
            0.01501245703548193,
            -0.0043169306591153145,
            -0.006378596182912588,
            -0.028394602239131927,
            -0.003245339961722493,
            -0.022117899730801582,
            0.017172621563076973,
            -0.001829006359912455,
            -0.013633484952151775,
            -0.008260248228907585,
            0.0026662396267056465,
            0.003905955934897065,
            0.002542268019169569,
            0.0028343654703348875,
            0.006945808883756399,
            0.0010180958779528737,
            -0.005757039412856102,
            -0.03635596111416817,
            0.02325911819934845,
            -0.004198053851723671,
            -0.0004954620380885899,
            -0.042279426008462906,
            -0.025595899671316147,
            0.013484039343893528,
            0.011065742932260036,
            0.014903769828379154,
            0.012397164478898048,
            0.013721792958676815,
            -0.015243417583405972,
            0.0006584932561963797,
            -0.008531966246664524,
            -0.003461016807705164,
            -0.001688052318058908,
            -0.014890183694660664,
            -0.029454305768013,
            -0.016846558079123497,
            0.013966340571641922,
            0.028557633981108665,
            -0.021139713004231453,
            -0.003255529562011361,
            -0.010603821836411953,
            0.01628953404724598,
            0.008321384899318218,
            0.01709110476076603,
            -0.008606689050793648,
            0.004378067329525948,
            -0.0013467057142406702,
            -0.009768286719918251,
            -0.03638312965631485,
            -0.028693493455648422,
            -0.0037633036263287067,
            0.036301616579294205,
            0.0034525254741311073,
            -0.02328629046678543,
            0.026519743725657463,
            0.011969207786023617,
            0.0026203871238976717,
            -0.01569175347685814,
            0.004496944136917591,
            0.05230584740638733,
            0.02051476016640663,
            0.011887691915035248,
            0.029481476172804832,
            -0.0005655144923366606,
            0.0016761645674705505,
            0.01870783045887947,
            -0.0005846197018399835,
            -0.014346746727824211,
            -0.004975848365575075,
            -0.007594536989927292,
            0.006755605805665255,
            -0.008321384899318218,
            0.02429164946079254,
            0.02844894677400589,
            -0.018802933394908905,
            -0.01164993830025196,
            0.0006028758361935616,
            -0.00596422515809536,
            0.012471887283027172,
            0.013572348281741142,
            -0.01690090261399746,
            -0.00231130700558424,
            0.012397164478898048,
            -0.026981664821505547,
            0.016602011397480965,
            -0.006602764129638672,
            -0.005070949904620647,
            0.012132239528000355,
            0.006830328144133091,
            -0.012927016243338585,
            0.014428261667490005,
            0.020392486825585365,
            0.0012550007086247206,
            0.001064797630533576,
            0.02070496417582035,
            -0.015841199085116386,
            -0.027389243245124817,
            0.02861197665333748,
            0.030514007434248924,
            -0.014958113431930542,
            0.015134730376303196,
            0.008973509073257446,
            0.014958113431930542,
            0.01589554361999035,
            0.012763985432684422,
            -0.00374971772544086,
            0.027008837088942528,
            -0.035731006413698196,
            -0.008593102917075157,
            -0.013253078795969486,
            -0.024128619581460953,
            0.025568727403879166,
            -0.0280685406178236,
            0.003950110170990229,
            -0.0007968999561853707,
            -0.017362823709845543,
            0.0157460980117321,
            0.03200846165418625,
            0.013171562924981117,
            -0.0250796340405941,
            0.005325686186552048,
            -0.006157824769616127,
            0.01313080545514822,
            -0.009904146194458008,
            0.0034915851429104805,
            0.02168315090239048,
            -0.03043249249458313,
            -0.022158658131957054,
            0.010026419535279274,
            -0.0067623988725245,
            0.01692807301878929,
            -0.015460792928934097,
            -0.0007009492837823927,
            0.006840517744421959,
            0.028476117178797722,
            -0.04290438070893288,
            -0.027511516585946083,
            -0.005994793493300676,
            0.0066061606630682945,
            -0.01849045604467392,
            -0.005111707840114832,
            -0.006235943641513586,
            -0.01806929148733616,
            0.01493094116449356,
            -0.011466528289020061,
            0.005991396959871054,
            -0.011018192395567894,
            -0.003729338990524411,
            -0.009095782414078712,
            -0.0018952378304675221,
            -0.03757869452238083,
            -0.016955245286226273,
            0.0071801659651100636,
            0.016411809250712395,
            -0.006619746331125498,
            -0.009720736183226109,
            -0.010046797804534435,
            0.022593408823013306,
            -0.0049724518321454525,
            0.013667449355125427,
            -0.03423655405640602,
            -0.003525549778714776,
            0.0023265911731868982,
            0.007397541310638189,
            -0.011969207786023617,
            -0.042279426008462906,
            -0.004378067329525948,
            -0.018639901652932167,
            0.006735226605087519,
            -0.018599143251776695,
            -0.011344254948198795,
            0.006898257881402969,
            0.005896295420825481,
            -0.0025303801521658897,
            -0.013762551359832287,
            -0.00305683515034616,
            0.0001466856338083744,
            0.013803308829665184,
            -0.008348556235432625,
            0.017648128792643547,
            0.0006079705781303346,
            -0.012302063405513763,
            -0.020976681262254715,
            -0.029345616698265076,
            0.001504642190411687,
            -0.014849426224827766,
            -0.023394977673888206,
            -0.014672808349132538,
            0.031193304806947708,
            0.010685336776077747,
            -0.01571892574429512,
            -0.01691448874771595,
            -0.020840823650360107,
            -0.03855688124895096,
            -0.00805645901709795,
            -0.007723603397607803,
            0.017267722636461258,
            -0.0014180318685248494,
            0.017403582111001015,
            0.027593031525611877,
            0.02425089292228222,
            0.007105443626642227,
            -0.003943317569792271,
            0.0024964152835309505,
            0.000519237422849983,
            -0.0002857291838154197,
            -0.02801419608294964,
            -0.019061066210269928,
            -0.0009433733066543937,
            -0.047414910048246384,
            -0.000902615487575531,
            0.04279569163918495,
            -0.018667073920369148,
            0.017607370391488075,
            0.05630011111497879,
            -0.008993888273835182,
            -0.016425393521785736,
            -0.001525870175100863,
            0.015161902643740177,
            0.010196243412792683,
            0.02980753965675831,
            0.015977058559656143,
            -0.026207266375422478,
            -0.00731602543964982,
            0.026709945872426033,
            0.00895992387086153,
            -0.0329594761133194,
            -0.0018120239255949855,
            0.01653408259153366,
            -0.003362518735229969,
            0.003048344049602747,
            -0.019210509955883026,
            0.01650691032409668,
            -0.0030636282172054052,
            -0.008858028799295425,
            0.0046531823463737965,
            0.0147271528840065,
            -0.02967168018221855,
            -0.0028530461713671684,
            0.029101070016622543,
            -0.019400713965296745,
            -0.021397845819592476,
            0.022606994956731796,
            -0.01400709804147482,
            -0.0023690471425652504,
            0.016221605241298676,
            -0.014740738086402416,
            0.006843914277851582,
            -0.004707525949925184,
            0.0009951696265488863,
            -0.01847686991095543,
            -0.0004294428799767047,
            -0.029345616698265076,
            0.028122883290052414,
            -0.023150430992245674,
            0.030731383711099625,
            0.011106501333415508,
            -0.02942713350057602,
            0.006497472990304232,
            0.0005608443170785904,
            -0.020243041217327118,
            -0.014686394482851028,
            -0.014876597560942173,
            0.009299571625888348,
            -0.009007474407553673,
            0.014088613912463188,
            0.0007591140456497669,
            -0.01351121161133051,
            -0.015610238537192345,
            -0.00285983900539577,
            0.01832742430269718,
            -0.006847310811281204,
            -0.00011325149534968659,
            0.23715606331825256,
            -0.03877425566315651,
            0.010936676524579525,
            0.0009178996551781893,
            -0.019183339551091194,
            0.0034678096417337656,
            -0.008531966246664524,
            0.010950262658298016,
            -0.009170505218207836,
            -0.0037667001597583294,
            0.02168315090239048,
            0.002808891935274005,
            -0.010705715976655483,
            0.0050437781028449535,
            0.016411809250712395,
            -0.01331421546638012,
            -0.013090047053992748,
            -0.021139713004231453,
            -0.015542308799922466,
            0.005030191969126463,
            0.010006040334701538,
            0.020637033507227898,
            -0.013769344426691532,
            -0.02226734533905983,
            0.03228018060326576,
            -0.002003925386816263,
            0.0007862859056331217,
            -0.004143709782510996,
            0.0354321151971817,
            0.011446149088442326,
            -0.009041438810527325,
            0.005003020167350769,
            -0.000519237422849983,
            -0.0013653864152729511,
            -0.0029005969408899546,
            0.003051740350201726,
            0.006072912365198135,
            0.009428638033568859,
            0.019061066210269928,
            -0.0006410862552002072,
            0.04154578596353531,
            0.021166885271668434,
            -0.008708584122359753,
            -0.019781120121479034,
            0.010637786239385605,
            0.009795458056032658,
            -0.020759306848049164,
            -0.012302063405513763,
            -0.008008908480405807,
            0.029698852449655533,
            -0.032089974731206894,
            0.022905884310603142,
            0.02465846948325634,
            0.037306975573301315,
            -0.0007421316695399582,
            -0.01732206530869007,
            0.00954411830753088,
            0.0015487964265048504,
            0.03279644623398781,
            -0.012994945980608463,
            -0.017362823709845543,
            0.009292778559029102,
            -0.005828365683555603,
            0.023367807269096375,
            -0.0037395283579826355,
            0.02627519704401493,
            -0.038692738860845566,
            0.04312175512313843,
            0.02268850989639759,
            0.014102199114859104,
            0.008919165469706059,
            -0.00975470058619976,
            -0.013558762148022652,
            0.004568270407617092,
            -0.02624802477657795,
            -0.02490301802754402,
            0.016941659152507782,
            0.01709110476076603,
            0.015555894933640957,
            0.03448110073804855,
            0.002479432849213481,
            -0.019047480076551437,
            -0.027538688853383064,
            -0.02209072932600975,
            -0.018585557118058205,
            -0.04298589378595352,
            -0.0010792326647788286,
            -0.020025666803121567,
            0.0010299836285412312,
            -0.03521474078297615,
            0.01729489490389824,
            -0.032334521412849426,
            -0.006168013904243708,
            -0.013490832410752773,
            0.02327270433306694,
            0.010732888244092464,
            0.004731301683932543,
            0.018177980557084084,
            0.010848368518054485,
            -0.010026419535279274,
            -0.0349973663687706,
            0.056626174598932266,
            0.017580198124051094,
            -0.0033455363009124994,
            -0.014292402192950249,
            0.004659975413233042,
            0.0033828974701464176,
            0.0019954340532422066,
            0.012383579276502132,
            -0.01074647344648838,
            0.0038482157979160547,
            -0.01532493345439434,
            0.013354972936213017,
            -0.016221605241298676,
            0.0037157528568059206,
            0.0009289382142014802,
            0.011955621652305126,
            0.012369993142783642,
            0.03024228848516941,
            -0.018001362681388855,
            0.003722545923665166,
            -0.019414300099015236,
            -0.0002935835509561002,
            0.030079258605837822,
            0.0014358634361997247,
            -0.01493094116449356,
            0.003742924891412258,
            0.007234510034322739,
            -0.02310967445373535,
            0.0024318823125213385,
            -0.004102952312678099,
            -0.016004230827093124,
            0.01123556774109602,
            -0.02107178419828415,
            -0.03575817868113518,
            0.013993511907756329,
            0.009965282864868641,
            -0.004975848365575075,
            -0.017226964235305786,
            0.012118653394281864,
            0.00012758040975313634,
            0.004038419108837843,
            0.005233981180936098,
            -0.01830025389790535,
            -0.011758625507354736,
            -0.009584876708686352,
            0.013205528259277344,
            -0.015868371352553368,
            -0.006881275679916143,
            -0.008776513859629631,
            -0.01811004988849163,
            -0.028394602239131927,
            -0.0058079869486391544,
            -0.03524191305041313,
            0.020039252936840057,
            0.010603821836411953,
            -0.008389314636588097,
            -0.022376032546162605,
            0.017362823709845543,
            0.009109368547797203,
            -0.020840823650360107,
            -0.005729867611080408,
            0.0032860978972166777,
            -0.023924829438328743,
            -0.001446052803657949,
            -0.004181071184575558,
            -0.17389994859695435,
            -0.011317082680761814,
            0.016778629273176193,
            -0.040241535753011703,
            0.008898787200450897,
            -0.0012066008057445288,
            0.01709110476076603,
            -0.02171032316982746,
            -0.038502536714076996,
            0.012981359846889973,
            0.02085440792143345,
            0.00885123573243618,
            -0.027008837088942528,
            -0.017063932493329048,
            -0.017036762088537216,
            0.004619217477738857,
            0.0025337766855955124,
            0.021737493574619293,
            0.0030670245178043842,
            0.0015411543427035213,
            0.042034879326820374,
            -0.0326605848968029,
            0.017661714926362038,
            -0.014075027778744698,
            0.00945581030100584,
            -0.0028615372721105814,
            -0.002742660464718938,
            0.009985661134123802,
            -0.0026662396267056465,
            -0.03722545877099037,
            0.008599895983934402,
            0.006239340174943209,
            0.025691000744700432,
            -0.009184091351926327,
            0.0023147035390138626,
            0.017199791967868805,
            0.016438979655504227,
            -0.006888068746775389,
            -0.009347123093903065,
            0.0025949133560061455,
            0.031111789867281914,
            0.01929202675819397,
            0.016547666862607002,
            -0.01400709804147482,
            -0.019061066210269928,
            0.020786479115486145,
            0.020134354010224342,
            -0.014387504197657108,
            0.021723909303545952,
            -0.019726775586605072,
            0.00318420329131186,
            -0.00817193929105997,
            -0.0016048384131863713,
            0.013151183724403381,
            0.023965587839484215,
            0.009190884418785572,
            -0.0033795011695474386,
            0.01666994020342827,
            -0.014795082621276379,
            0.004391652997583151,
            -0.00019243202405050397,
            -0.024617712944746017,
            0.011330668814480305,
            -0.017145449295639992,
            -0.02725338377058506,
            -0.01262133289128542,
            -0.017756815999746323,
            0.0047245086170732975,
            0.0006156126619316638,
            0.011004606261849403,
            -0.015447206795215607,
            -0.013844067230820656,
            -0.0068608964793384075,
            0.00015676894690841436,
            0.010474755428731441,
            0.032905131578445435,
            -0.009238434955477715,
            -0.005444562993943691,
            0.019821878522634506,
            0.009978868998587132,
            -0.013185149058699608,
            0.032714929431676865,
            -0.018599143251776695,
            0.018965963274240494,
            -0.006385388784110546,
            0.0009892258094623685,
            0.0009255417389795184,
            0.018558386713266373,
            -0.010624200105667114,
            -0.01869424618780613,
            0.005186430178582668,
            -0.03024228848516941,
            0.013205528259277344,
            0.0012210358399897814,
            0.0019852446857839823,
            0.011371427215635777,
            0.02310967445373535,
            0.0039161453023552895,
            0.004201449919492006,
            -0.015474379062652588,
            0.0053902193903923035,
            0.003841422963887453,
            0.009319950826466084,
            0.029399961233139038,
            0.011588801629841328,
            0.027959851548075676,
            -0.024413922801613808,
            0.013816894963383675,
            0.029726022854447365,
            0.0038244405295699835,
            -0.02429164946079254,
            -0.016642769798636436,
            0.015053214505314827,
            0.020650619640946388,
            0.013796515762805939,
            0.015352105721831322,
            -0.0035459287464618683,
            -0.017947018146514893,
            0.011208395473659039,
            -0.014061441645026207,
            0.03415503725409508,
            0.0034763009753078222,
            -0.01793343387544155,
            0.02168315090239048,
            -0.02207714319229126,
            -0.04717036336660385,
            -0.09417769312858582,
            -0.038094960153102875,
            -0.019658846780657768,
            0.0047143190167844296,
            -0.003844819264486432,
            0.016574839130043983,
            0.02688656374812126,
            0.018830103799700737,
            0.00039420436951331794,
            0.03282361850142479,
            -0.019047480076551437,
            -0.013558762148022652,
            0.002316401805728674,
            -0.02687297761440277,
            0.008104009553790092,
            0.0024607523810118437,
            0.0016260665142908692,
            0.004710922483354807,
            -0.017960604280233383,
            0.01969960518181324,
            0.0030789123848080635,
            -0.030785726383328438,
            0.032714929431676865,
            -0.03184542804956436,
            0.031138960272073746,
            -0.026207266375422478,
            -0.031763914972543716,
            0.008307798765599728,
            -0.0024641486816108227,
            -0.016778629273176193,
            -0.012336027808487415,
            -0.02926410175859928,
            9.860416321316734e-05,
            -0.014944527298212051,
            0.0032657189294695854,
            -0.008756134659051895,
            -0.016194432973861694,
            -0.027348484843969345,
            -0.003950110170990229,
            -0.029970571398735046,
            0.0020327954553067684,
            0.0007306685438379645,
            0.0004997076466679573,
            -0.01592271402478218,
            -0.012132239528000355,
            -0.04002416133880615,
            -0.010963848792016506,
            0.014387504197657108,
            0.011317082680761814,
            -0.007295646704733372,
            -0.014876597560942173,
            -0.008919165469706059,
            -0.03399200737476349,
            -0.007064685691148043,
            0.01313080545514822,
            0.012852293439209461,
            0.0032708137296140194,
            0.004452789667993784,
            -0.011147258803248405,
            -0.02171032316982746,
            0.007954563945531845,
            -0.007397541310638189,
            -0.01892520673573017,
            0.0072888536378741264,
            0.0010936676990240812,
            -0.004371274262666702,
            -0.00934033002704382,
            -0.025568727403879166,
            0.012600953690707684,
            -0.0306226946413517,
            0.0029464494436979294,
            0.014469020068645477,
            0.0035798936150968075,
            0.019604502245783806,
            -0.021207643672823906,
            -0.010705715976655483,
            -0.015515136532485485,
            0.021574463695287704,
            0.005845348350703716,
            -0.01609933190047741,
            -0.03228018060326576,
            -0.022729268297553062,
            0.01729489490389824,
            -0.025025291368365288,
            -0.006803156342357397,
            -0.00766246672719717,
            -0.024644885212183,
            -0.00037127811810933053,
            -0.0038040615618228912,
            -0.029698852449655533,
            0.012668883427977562,
            0.03045966476202011,
            -0.0013450074475258589,
            -0.013191942125558853,
            -0.006235943641513586,
            0.0068608964793384075,
            -0.025758931413292885,
            0.004374670796096325,
            0.006096688099205494,
            0.045458536595106125,
            -0.01364027801901102,
            -0.00641595758497715,
            -0.052822113037109375,
            0.008803685195744038,
            0.0031909963581711054,
            -0.022946642711758614,
            0.009218056686222553,
            0.023775383830070496,
            0.037497177720069885,
            0.009421844966709614,
            -0.013470453210175037,
            0.030731383711099625,
            -0.01073968131095171,
            0.02168315090239048,
            0.013049289584159851,
            0.0001267312909476459,
            -0.012994945980608463,
            -0.024821501225233078,
            0.02385690063238144,
            -0.021968455985188484,
            0.015814026817679405,
            0.024726400151848793,
            -0.003197789192199707,
            -0.011378219351172447,
            0.012859086506068707,
            0.01253981702029705,
            -0.0019173149485141039,
            -0.0010130011942237616,
            -0.015433620661497116,
            0.014686394482851028,
            -0.01733565144240856,
            -0.02268850989639759,
            -0.002095630392432213,
            -0.019590916112065315,
            0.006246133241802454,
            0.00995169673115015,
            -0.010889125987887383,
            -0.002598309889435768,
            0.0007387351361103356,
            0.008830857463181019,
            0.02268850989639759,
            0.009462603367865086,
            -0.01194203644990921,
            -0.026112165302038193,
            0.02882935293018818,
            -0.025976305827498436,
            0.0027494532987475395,
            -0.0033964836038649082,
            0.004632803611457348,
            -0.002212808933109045,
            -0.003817647462710738,
            -0.006483886856585741,
            0.024155789986252785,
            0.02108537033200264,
            0.009435431100428104,
            -0.022973814979195595,
            -0.01631670631468296,
            0.005447959527373314,
            0.013551969081163406,
            0.026152923703193665,
            -0.026913736015558243,
            -0.009809044189751148,
            0.017607370391488075,
            0.007825498469173908,
            0.002411503344774246,
            -0.00325383129529655,
            0.0016421998152509332,
            0.012859086506068707,
            0.019237682223320007,
            -0.0017186206532642245,
            -0.009197677485644817,
            -0.034535445272922516,
            -0.0014018985675647855,
            -0.0013008532114326954,
            0.012071102857589722,
            0.011575215496122837,
            0.03564948961138725,
            0.006229150574654341,
            0.00017884607950691134,
            0.00716658029705286,
            -0.01463205087929964,
            0.03662768006324768,
            0.015596652403473854,
            -0.028965210542082787,
            -0.01342969574034214,
            -0.017770402133464813,
            0.004829799756407738,
            -0.0027358673978596926,
            0.006796363741159439,
            -0.0024556575808674097,
            -0.01402068417519331,
            -0.0037870791275054216,
            0.005712885409593582,
            0.007988529279828072,
            0.002396219177171588,
            0.0053800297901034355,
            -0.004089366178959608,
            0.009809044189751148,
            0.010984227992594242,
            0.00021461529831867665,
            0.012607746757566929,
            -0.013592727482318878,
            0.001277926960028708,
            0.0013322706799954176,
            -0.004595442209392786,
            -0.005634766072034836,
            -0.014536949805915356,
            0.020990267395973206,
            -0.019020307809114456,
            -0.028313087299466133,
            0.00040587977855466306,
            0.0034151640720665455,
            0.010298137553036213,
            0.020243041217327118,
            -0.015257003717124462,
            0.0015292667085304856,
            -0.02861197665333748,
            0.03423655405640602,
            0.013735379092395306,
            -0.017457924783229828,
            -0.008498001843690872,
            0.007995322346687317,
            0.007085064426064491,
            -0.00015952858666423708,
            0.028557633981108665,
            0.0013874635333195329,
            0.01592271402478218,
            0.021642392501235008,
            0.04121972247958183,
            -0.010481548495590687,
            0.00894633773714304,
            0.008287419565021992,
            -0.013253078795969486,
            -0.017145449295639992,
            -0.005529474932700396,
            -0.019020307809114456,
            -0.00023456964117940515,
            0.0033217607997357845,
            0.0027137903962284327,
            0.022172244265675545,
            -0.0034253536723554134,
            0.0892324149608612,
            0.02586761862039566,
            0.004996227100491524,
            0.0027341691311448812,
            -0.0010401731124147773,
            0.025813274085521698,
            0.019210509955883026,
            0.014346746727824211,
            -0.0023418753407895565,
            0.00481961015611887,
            0.02449543960392475,
            0.01633029244840145,
            0.006545023526996374,
            -0.012492266483604908,
            -0.014795082621276379,
            0.006239340174943209,
            0.011595594696700573,
            -0.0030924982856959105,
            0.01787908934056759,
            -0.013103633187711239,
            0.046056315302848816,
            -0.026492571458220482,
            0.007458677981048822,
            0.011955621652305126,
            -0.03461695834994316,
            -0.006049137096852064,
            0.010760059580206871,
            0.015664581209421158,
            -0.018667073920369148,
            -0.039073146879673004,
            0.019658846780657768,
            -0.002095630392432213,
            -0.028150055557489395,
            -0.012845500372350216,
            0.022213002666831017,
            -0.002409805078059435,
            -0.001168390386737883,
            -0.005818176083266735,
            0.018245909363031387,
            0.010617407038807869,
            0.01501245703548193,
            0.01690090261399746,
            -0.004459582734853029,
            -0.03497019410133362,
            -0.01194203644990921,
            -0.0059676216915249825,
            -0.02008001133799553,
            -0.027348484843969345,
            -0.016588425263762474
          ],
          "start_time": 1694832910.037893,
          "end_time": 1694832910.628216,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "What is the purpose of BERT?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.022036384791135788,
                      -0.0164797380566597,
                      0.0005999039276503026,
                      -0.007227716967463493,
                      0.003868594765663147,
                      0.01381010189652443,
                      -0.006500869523733854,
                      -0.011711074970662594,
                      -0.01653408259153366,
                      -0.015121144242584705,
                      0.0027137903962284327,
                      0.027362070977687836,
                      -0.025758931413292885,
                      0.00944222416728735,
                      3.94045164284762e-05,
                      0.0016226699808612466,
                      0.016561252996325493,
                      0.015487965196371078,
                      0.004052004776895046,
                      0.0009892258094623685,
                      -0.005791004281491041,
                      -0.014170128852128983,
                      -0.019210509955883026,
                      0.020990267395973206,
                      -0.006324252113699913,
                      -0.02627519704401493,
                      0.05154503509402275,
                      -0.033502914011478424,
                      0.00033030801569111645,
                      -0.013558762148022652,
                      0.027144696563482285,
                      0.030269460752606392,
                      -0.010019626468420029,
                      -0.008131181821227074,
                      -0.013069668784737587,
                      0.0003339167742524296,
                      0.01770247146487236,
                      -0.002297721104696393,
                      0.017009589821100235,
                      -0.01224092673510313,
                      0.015800440683960915,
                      0.01892520673573017,
                      0.013878031633794308,
                      -0.001744094304740429,
                      -0.0005765530513599515,
                      0.003309873165562749,
                      0.01252623088657856,
                      -0.004153899382799864,
                      0.0005892898770980537,
                      -0.023788969963788986,
                      0.018653487786650658,
                      0.01949581503868103,
                      -0.02384331449866295,
                      -0.002343573607504368,
                      -0.03442675620317459,
                      -0.0030534386169165373,
                      -0.0077575682662427425,
                      0.02582686021924019,
                      0.009068611077964306,
                      -0.02248472161591053,
                      0.010753266513347626,
                      0.0006746265571564436,
                      -0.015664581209421158,
                      0.019156167283654213,
                      0.0048841433599591255,
                      -0.009299571625888348,
                      -0.022946642711758614,
                      0.029101070016622543,
                      0.010603821836411953,
                      0.008885201066732407,
                      0.007628501858562231,
                      0.020623447373509407,
                      0.019455058500170708,
                      -0.011086122132837772,
                      0.03654616326093674,
                      -0.002844554837793112,
                      -0.01502604316920042,
                      -0.005852140951901674,
                      0.008036079816520214,
                      0.0004491849394980818,
                      0.006405767984688282,
                      0.02306891605257988,
                      -0.02109895646572113,
                      0.020922338590025902,
                      0.013062875717878342,
                      -0.007594536989927292,
                      0.013103633187711239,
                      0.01690090261399746,
                      -0.019577331840991974,
                      -0.0003455921832937747,
                      0.006201978772878647,
                      0.002939656376838684,
                      0.03314967826008797,
                      0.004282965790480375,
                      0.000431353400927037,
                      0.03024228848516941,
                      -0.004602235276252031,
                      0.029101070016622543,
                      0.0016498418990522623,
                      -0.035106055438518524,
                      0.009027853608131409,
                      -0.0075266072526574135,
                      -0.007940978743135929,
                      -0.01224092673510313,
                      -0.019224096089601517,
                      0.0021533705294132233,
                      0.00983621645718813,
                      -0.010372860357165337,
                      0.028150055557489395,
                      -0.008076838217675686,
                      -0.007716810330748558,
                      0.03203563392162323,
                      -0.0034202588722109795,
                      -0.020215868949890137,
                      0.005529474932700396,
                      0.021533705294132233,
                      0.0010299836285412312,
                      0.0005413145408965647,
                      0.017457924783229828,
                      0.020732134580612183,
                      0.03328553959727287,
                      -0.002212808933109045,
                      0.0011565026361495256,
                      -0.01827308163046837,
                      0.022348862141370773,
                      -0.008137974888086319,
                      -0.021615220233798027,
                      -0.019998494535684586,
                      0.008701791055500507,
                      -0.029481476172804832,
                      0.006055930163711309,
                      -0.0036444268189370632,
                      0.0013798214495182037,
                      -0.023816142231225967,
                      -0.002878519706428051,
                      0.023191189393401146,
                      -0.00796135701239109,
                      -0.010107934474945068,
                      -0.026424642652273178,
                      -0.01768888533115387,
                      0.016819385811686516,
                      0.022593408823013306,
                      -0.023177603259682655,
                      -0.03518756851553917,
                      -0.026601258665323257,
                      0.02546004019677639,
                      0.0007625105208717287,
                      0.02842177450656891,
                      0.010114727541804314,
                      0.002683222061023116,
                      0.0042456043884158134,
                      -0.031138960272073746,
                      -0.018055707216262817,
                      0.009591669775545597,
                      0.0022875317372381687,
                      0.013979925774037838,
                      -0.00032075541093945503,
                      0.0026458606589585543,
                      -0.000471262086648494,
                      -0.00011494974023662508,
                      0.0030279650818556547,
                      0.014550535008311272,
                      0.011554837226867676,
                      -0.009598462842404842,
                      0.03760586678981781,
                      0.04051325470209122,
                      -0.004279569257050753,
                      -0.0026611448265612125,
                      0.012397164478898048,
                      0.003573100548237562,
                      -0.008688204921782017,
                      0.01570533961057663,
                      -0.026981664821505547,
                      0.004629407078027725,
                      0.015433620661497116,
                      0.00746547058224678,
                      0.017430752515792847,
                      0.007805119268596172,
                      0.0007680298294872046,
                      -0.013361766003072262,
                      -0.0051320865750312805,
                      0.006398974917829037,
                      0.03399200737476349,
                      0.005003020167350769,
                      -0.015257003717124462,
                      -0.002204317832365632,
                      0.003770096693187952,
                      -0.035350602120161057,
                      0.0028004006016999483,
                      -0.044643379747867584,
                      0.018205150961875916,
                      0.019061066210269928,
                      0.011711074970662594,
                      -0.004673561546951532,
                      -0.6364738345146179,
                      -0.014455433934926987,
                      0.016629183664917946,
                      -0.00835534930229187,
                      0.021329917013645172,
                      -0.013083254918456078,
                      -0.010352482087910175,
                      -0.00566193787381053,
                      -0.01171786803752184,
                      0.03263341262936592,
                      0.005447959527373314,
                      -0.012607746757566929,
                      -0.005906485021114349,
                      -0.01502604316920042,
                      -0.0013458565808832645,
                      -0.022511892020702362,
                      -0.00936070829629898,
                      -0.00402143644168973,
                      0.01869424618780613,
                      -0.003973885904997587,
                      -0.02446826733648777,
                      0.02124840021133423,
                      -0.04380105063319206,
                      0.005852140951901674,
                      -0.01752585545182228,
                      0.007132615428417921,
                      0.002350366674363613,
                      -0.009313157759606838,
                      0.007295646704733372,
                      -0.007268474902957678,
                      -0.01323269959539175,
                      0.03336705267429352,
                      0.03956224024295807,
                      -0.015284175984561443,
                      0.031709570437669754,
                      0.011860520578920841,
                      -0.002382633276283741,
                      0.015759684145450592,
                      -0.031193304806947708,
                      0.0256638303399086,
                      -0.00866782572120428,
                      -0.019618088379502296,
                      0.02329987660050392,
                      -0.007010342087596655,
                      0.03217149153351784,
                      0.007248095702379942,
                      0.01232923474162817,
                      0.021425018087029457,
                      0.0018884448800235987,
                      0.007431505713611841,
                      -0.027606617659330368,
                      0.008722169324755669,
                      0.0014333160361275077,
                      -0.008464036509394646,
                      0.014401090331375599,
                      -0.0018341010436415672,
                      0.022837955504655838,
                      -0.014061441645026207,
                      0.0002553731028456241,
                      -0.022606994956731796,
                      0.011643145233392715,
                      0.034345243126153946,
                      -0.010902712121605873,
                      -0.02567741461098194,
                      -0.011004606261849403,
                      0.015229832381010056,
                      0.0035323428455740213,
                      0.012770777568221092,
                      0.011887691915035248,
                      -0.04477923735976219,
                      0.010481548495590687,
                      0.024984532967209816,
                      0.0006393880466930568,
                      -0.0006792966742068529,
                      0.02008001133799553,
                      0.017743229866027832,
                      -0.00228923000395298,
                      -0.015121144242584705,
                      -0.0004844234499614686,
                      0.022973814979195595,
                      -0.009252021089196205,
                      -0.005587215535342693,
                      0.012607746757566929,
                      -0.0028190813027322292,
                      -0.018748588860034943,
                      0.017797574400901794,
                      -0.006918637081980705,
                      -0.02309608832001686,
                      0.005352857988327742,
                      0.026207266375422478,
                      -0.0012125446228310466,
                      0.02782399207353592,
                      -0.021601635962724686,
                      -0.03942637890577316,
                      0.02248472161591053,
                      0.012369993142783642,
                      0.003501774510368705,
                      -0.016642769798636436,
                      0.03247038275003433,
                      -0.007241302635520697,
                      0.002798702334985137,
                      -0.017607370391488075,
                      0.020243041217327118,
                      0.023761799558997154,
                      0.00856593158096075,
                      0.016438979655504227,
                      0.02270209603011608,
                      0.020637033507227898,
                      0.002071854891255498,
                      -0.017199791967868805,
                      0.004792438354343176,
                      -0.016357464715838432,
                      0.00825345516204834,
                      0.008776513859629631,
                      -0.006633332464843988,
                      -0.0329594761133194,
                      0.014183714985847473,
                      0.0027817199006676674,
                      0.012220547534525394,
                      -0.021234814077615738,
                      0.0019427885999903083,
                      -0.009598462842404842,
                      0.012444715946912766,
                      -0.016642769798636436,
                      0.019210509955883026,
                      0.017675301060080528,
                      -0.0031536349561065435,
                      0.004310137592256069,
                      -0.020650619640946388,
                      -0.005298514384776354,
                      -0.021832596510648727,
                      -0.006426146719604731,
                      0.0053800297901034355,
                      -0.012152617797255516,
                      0.04393691197037697,
                      0.03899163007736206,
                      0.03260624036192894,
                      -0.0015937999123707414,
                      -0.02050117403268814,
                      0.005006416700780392,
                      -0.012315649539232254,
                      -0.010495133697986603,
                      0.01567816734313965,
                      -0.005740057211369276,
                      -0.052034128457307816,
                      -0.016167260706424713,
                      0.008891994133591652,
                      0.003070421051234007,
                      0.001690599718131125,
                      -0.011554837226867676,
                      0.0015428526094183326,
                      -0.02624802477657795,
                      -0.018585557118058205,
                      -0.005899691954255104,
                      -0.013090047053992748,
                      -0.028557633981108665,
                      -0.009408259764313698,
                      -0.04668127000331879,
                      -0.019536573439836502,
                      -0.031709570437669754,
                      0.010325309820473194,
                      0.00373273529112339,
                      -0.011989586986601353,
                      -0.012770777568221092,
                      0.0011378219351172447,
                      -0.03497019410133362,
                      0.005515889264643192,
                      0.022213002666831017,
                      -0.027525102719664574,
                      -0.033910490572452545,
                      0.028938040137290955,
                      -0.01846328377723694,
                      -0.01033210288733244,
                      0.005774022080004215,
                      -0.026682773604989052,
                      0.013776137493550777,
                      -0.03325836732983589,
                      -0.016017816960811615,
                      0.0003791324852500111,
                      -0.016044987365603447,
                      0.005030191969126463,
                      0.030704211443662643,
                      -0.010012833401560783,
                      -0.009224848821759224,
                      0.012220547534525394,
                      0.0020344937220215797,
                      0.017172621563076973,
                      -0.00310948072001338,
                      -0.02488943189382553,
                      0.00974111445248127,
                      0.00874934159219265,
                      0.026587672531604767,
                      -0.003061929950490594,
                      0.02090875245630741,
                      0.012573782354593277,
                      0.006110273767262697,
                      0.016357464715838432,
                      -0.009122954681515694,
                      0.0008754436275921762,
                      -0.005172844510525465,
                      0.025297008454799652,
                      0.010732888244092464,
                      0.00034325712476857007,
                      -0.012940602377057076,
                      -0.008294212631881237,
                      0.017457924783229828,
                      -8.363840606762096e-05,
                      -0.016438979655504227,
                      0.008219489827752113,
                      -0.00042880602995865047,
                      0.01461846474558115,
                      -0.029508648440241814,
                      -0.02325911819934845,
                      -0.0005476829828694463,
                      -0.008708584122359753,
                      0.021044611930847168,
                      -0.009068611077964306,
                      0.02289229817688465,
                      -0.034263726323843,
                      0.0033591222018003464,
                      0.003783682594075799,
                      -0.01793343387544155,
                      0.004975848365575075,
                      0.0009900749428197742,
                      -0.011765418574213982,
                      0.006297080311924219,
                      -0.008097216486930847,
                      -0.012702848762273788,
                      0.012410750612616539,
                      -0.022362448275089264,
                      0.0005455601494759321,
                      0.028285915032029152,
                      0.02999774180352688,
                      0.020025666803121567,
                      -0.0012032042723149061,
                      -0.00856593158096075,
                      0.014469020068645477,
                      -0.010196243412792683,
                      0.025595899671316147,
                      -0.017159035429358482,
                      0.0026017064228653908,
                      0.01154804416000843,
                      0.013110426254570484,
                      -0.0015776666114106774,
                      0.03545928746461868,
                      0.015814026817679405,
                      0.03244321048259735,
                      0.010651372373104095,
                      -0.016072159633040428,
                      0.02287871204316616,
                      -0.01431957446038723,
                      -0.008878407999873161,
                      -0.030731383711099625,
                      0.0011514079524204135,
                      0.0240606889128685,
                      -0.00924522802233696,
                      0.0040757800452411175,
                      0.012899843975901604,
                      -0.0057094888761639595,
                      0.033883318305015564,
                      0.016357464715838432,
                      0.008362142369151115,
                      0.022797197103500366,
                      0.007805119268596172,
                      0.019183339551091194,
                      -0.014455433934926987,
                      -0.014591293409466743,
                      -0.0058079869486391544,
                      -0.009184091351926327,
                      -0.004938486963510513,
                      -0.017254136502742767,
                      -0.015080386772751808,
                      0.0010206432780250907,
                      -0.010617407038807869,
                      0.01441467646509409,
                      0.03024228848516941,
                      -0.02490301802754402,
                      0.021438604220747948,
                      -0.0067929672077298164,
                      0.034127864986658096,
                      -0.0023045141715556383,
                      -0.029916226863861084,
                      0.03160088136792183,
                      -0.006130652967840433,
                      -0.027959851548075676,
                      -0.0171318631619215,
                      0.0032283575274050236,
                      -0.02169673703610897,
                      -0.004269379656761885,
                      -0.018830103799700737,
                      0.0016956944018602371,
                      0.009007474407553673,
                      -0.0019852446857839823,
                      0.025120392441749573,
                      0.00111829221714288,
                      0.013293836265802383,
                      0.007302439771592617,
                      -0.01986263506114483,
                      0.004432410933077335,
                      0.004486754536628723,
                      0.0019190132152289152,
                      0.00785946287214756,
                      -0.006490679923444986,
                      -0.028965210542082787,
                      0.02029738575220108,
                      0.001125934300944209,
                      -0.029209759086370468,
                      -0.016642769798636436,
                      -0.016466151922941208,
                      -0.015977058559656143,
                      0.038257990032434464,
                      0.009462603367865086,
                      0.01868066005408764,
                      -0.019346369430422783,
                      -5.301698547555134e-05,
                      0.0292369294911623,
                      0.007893427275121212,
                      -0.006996755953878164,
                      0.008511587977409363,
                      0.004408635664731264,
                      -0.028313087299466133,
                      -0.012268098071217537,
                      -0.01671069860458374,
                      0.008436865173280239,
                      0.04317609965801239,
                      0.04755076766014099,
                      -0.004174278117716312,
                      0.019020307809114456,
                      -0.012757192365825176,
                      0.03635596111416817,
                      -0.025758931413292885,
                      -0.0389372855424881,
                      -0.021194057539105415,
                      -0.01649332419037819,
                      0.0015802140114828944,
                      -0.02664201706647873,
                      0.006531437858939171,
                      -0.022810783237218857,
                      0.005254359915852547,
                      0.01025058701634407,
                      -0.0003099291061516851,
                      -0.02626161091029644,
                      0.033747460693120956,
                      -0.016547666862607002,
                      -0.0009994152933359146,
                      0.013701414689421654,
                      0.006076308898627758,
                      0.010467962361872196,
                      0.001453694887459278,
                      -0.019427886232733727,
                      -0.012254512868821621,
                      0.010807610116899014,
                      -0.0035323428455740213,
                      -0.011588801629841328,
                      -0.016561252996325493,
                      -0.022579822689294815,
                      -0.01753944158554077,
                      0.00787304900586605,
                      -0.02051476016640663,
                      0.03939921036362648,
                      0.001176881487481296,
                      0.006545023526996374,
                      0.011004606261849403,
                      0.020025666803121567,
                      0.0029600353445857763,
                      0.02662843093276024,
                      -0.002523587318137288,
                      -0.01540644932538271,
                      7.790684321662411e-05,
                      -0.030378147959709167,
                      0.004031626041978598,
                      0.016574839130043983,
                      0.011317082680761814,
                      -0.002888709306716919,
                      0.0012337726075202227,
                      -0.004809420555830002,
                      -0.04111103713512421,
                      -0.005013209767639637,
                      0.014469020068645477,
                      -0.014699980616569519,
                      0.012471887283027172,
                      0.010196243412792683,
                      -0.01827308163046837,
                      -0.02385690063238144,
                      -0.017403582111001015,
                      -0.014047855511307716,
                      0.01872141659259796,
                      -0.007275267504155636,
                      0.00365801271982491,
                      0.004639596678316593,
                      -0.017362823709845543,
                      -0.0244275089353323,
                      -0.015596652403473854,
                      0.04627368971705437,
                      -0.008212696760892868,
                      -0.04032305255532265,
                      -0.04727904871106148,
                      -0.016765043139457703,
                      0.01221375446766615,
                      0.011486907489597797,
                      0.03260624036192894,
                      -0.020786479115486145,
                      -0.001669371617026627,
                      -0.0023147035390138626,
                      -0.007574158255010843,
                      -0.03445392847061157,
                      0.0223081037402153,
                      -0.04051325470209122,
                      0.0038006650283932686,
                      -0.01102498546242714,
                      -0.00746547058224678,
                      -0.027565861120820045,
                      -0.019332783296704292,
                      0.0354321151971817,
                      0.002744358731433749,
                      0.005644955672323704,
                      0.02961733564734459,
                      -0.028965210542082787,
                      0.01846328377723694,
                      0.019984908401966095,
                      0.0072005451656877995,
                      0.005634766072034836,
                      0.012886258773505688,
                      -0.028313087299466133,
                      -0.011955621652305126,
                      -0.009285985492169857,
                      -0.010420411825180054,
                      -0.03325836732983589,
                      0.03165522590279579,
                      -0.006307269912213087,
                      0.0036274443846195936,
                      -0.021126126870512962,
                      0.01733565144240856,
                      -0.01072609517723322,
                      0.008925958536565304,
                      -0.023394977673888206,
                      -0.00310948072001338,
                      -0.01711827702820301,
                      0.0313563346862793,
                      -0.0038855771999806166,
                      -0.001333968946710229,
                      0.024970946833491325,
                      -0.012268098071217537,
                      -0.0019478832837194204,
                      0.0025490608531981707,
                      -0.02308250218629837,
                      0.016588425263762474,
                      0.019006721675395966,
                      -0.0026017064228653908,
                      0.021520119160413742,
                      0.005498906597495079,
                      -0.01688731648027897,
                      -0.007037513889372349,
                      0.025609485805034637,
                      -0.024509025737643242,
                      0.01668352633714676,
                      -0.0017101294361054897,
                      -0.0278511643409729,
                      -0.03143785148859024,
                      -0.016832971945405006,
                      -0.002584723988547921,
                      0.008932751603424549,
                      -0.01461846474558115,
                      -0.0012813233770430088,
                      0.012179790064692497,
                      -0.0006381143466569483,
                      0.009992454200983047,
                      -0.013225906528532505,
                      0.01889803446829319,
                      -0.022960228845477104,
                      -0.014808667823672295,
                      0.008531966246664524,
                      0.001946185017004609,
                      0.03206280618906021,
                      -0.017634542658925056,
                      0.02999774180352688,
                      -0.030079258605837822,
                      -0.0076692597940564156,
                      0.0005748548428528011,
                      -0.02983471192419529,
                      -0.0004895181627944112,
                      0.004462979268282652,
                      0.03673636540770531,
                      0.02324553392827511,
                      0.03184542804956436,
                      -0.000315448414767161,
                      0.01082798931747675,
                      0.01501245703548193,
                      -0.0043169306591153145,
                      -0.006378596182912588,
                      -0.028394602239131927,
                      -0.003245339961722493,
                      -0.022117899730801582,
                      0.017172621563076973,
                      -0.001829006359912455,
                      -0.013633484952151775,
                      -0.008260248228907585,
                      0.0026662396267056465,
                      0.003905955934897065,
                      0.002542268019169569,
                      0.0028343654703348875,
                      0.006945808883756399,
                      0.0010180958779528737,
                      -0.005757039412856102,
                      -0.03635596111416817,
                      0.02325911819934845,
                      -0.004198053851723671,
                      -0.0004954620380885899,
                      -0.042279426008462906,
                      -0.025595899671316147,
                      0.013484039343893528,
                      0.011065742932260036,
                      0.014903769828379154,
                      0.012397164478898048,
                      0.013721792958676815,
                      -0.015243417583405972,
                      0.0006584932561963797,
                      -0.008531966246664524,
                      -0.003461016807705164,
                      -0.001688052318058908,
                      -0.014890183694660664,
                      -0.029454305768013,
                      -0.016846558079123497,
                      0.013966340571641922,
                      0.028557633981108665,
                      -0.021139713004231453,
                      -0.003255529562011361,
                      -0.010603821836411953,
                      0.01628953404724598,
                      0.008321384899318218,
                      0.01709110476076603,
                      -0.008606689050793648,
                      0.004378067329525948,
                      -0.0013467057142406702,
                      -0.009768286719918251,
                      -0.03638312965631485,
                      -0.028693493455648422,
                      -0.0037633036263287067,
                      0.036301616579294205,
                      0.0034525254741311073,
                      -0.02328629046678543,
                      0.026519743725657463,
                      0.011969207786023617,
                      0.0026203871238976717,
                      -0.01569175347685814,
                      0.004496944136917591,
                      0.05230584740638733,
                      0.02051476016640663,
                      0.011887691915035248,
                      0.029481476172804832,
                      -0.0005655144923366606,
                      0.0016761645674705505,
                      0.01870783045887947,
                      -0.0005846197018399835,
                      -0.014346746727824211,
                      -0.004975848365575075,
                      -0.007594536989927292,
                      0.006755605805665255,
                      -0.008321384899318218,
                      0.02429164946079254,
                      0.02844894677400589,
                      -0.018802933394908905,
                      -0.01164993830025196,
                      0.0006028758361935616,
                      -0.00596422515809536,
                      0.012471887283027172,
                      0.013572348281741142,
                      -0.01690090261399746,
                      -0.00231130700558424,
                      0.012397164478898048,
                      -0.026981664821505547,
                      0.016602011397480965,
                      -0.006602764129638672,
                      -0.005070949904620647,
                      0.012132239528000355,
                      0.006830328144133091,
                      -0.012927016243338585,
                      0.014428261667490005,
                      0.020392486825585365,
                      0.0012550007086247206,
                      0.001064797630533576,
                      0.02070496417582035,
                      -0.015841199085116386,
                      -0.027389243245124817,
                      0.02861197665333748,
                      0.030514007434248924,
                      -0.014958113431930542,
                      0.015134730376303196,
                      0.008973509073257446,
                      0.014958113431930542,
                      0.01589554361999035,
                      0.012763985432684422,
                      -0.00374971772544086,
                      0.027008837088942528,
                      -0.035731006413698196,
                      -0.008593102917075157,
                      -0.013253078795969486,
                      -0.024128619581460953,
                      0.025568727403879166,
                      -0.0280685406178236,
                      0.003950110170990229,
                      -0.0007968999561853707,
                      -0.017362823709845543,
                      0.0157460980117321,
                      0.03200846165418625,
                      0.013171562924981117,
                      -0.0250796340405941,
                      0.005325686186552048,
                      -0.006157824769616127,
                      0.01313080545514822,
                      -0.009904146194458008,
                      0.0034915851429104805,
                      0.02168315090239048,
                      -0.03043249249458313,
                      -0.022158658131957054,
                      0.010026419535279274,
                      -0.0067623988725245,
                      0.01692807301878929,
                      -0.015460792928934097,
                      -0.0007009492837823927,
                      0.006840517744421959,
                      0.028476117178797722,
                      -0.04290438070893288,
                      -0.027511516585946083,
                      -0.005994793493300676,
                      0.0066061606630682945,
                      -0.01849045604467392,
                      -0.005111707840114832,
                      -0.006235943641513586,
                      -0.01806929148733616,
                      0.01493094116449356,
                      -0.011466528289020061,
                      0.005991396959871054,
                      -0.011018192395567894,
                      -0.003729338990524411,
                      -0.009095782414078712,
                      -0.0018952378304675221,
                      -0.03757869452238083,
                      -0.016955245286226273,
                      0.0071801659651100636,
                      0.016411809250712395,
                      -0.006619746331125498,
                      -0.009720736183226109,
                      -0.010046797804534435,
                      0.022593408823013306,
                      -0.0049724518321454525,
                      0.013667449355125427,
                      -0.03423655405640602,
                      -0.003525549778714776,
                      0.0023265911731868982,
                      0.007397541310638189,
                      -0.011969207786023617,
                      -0.042279426008462906,
                      -0.004378067329525948,
                      -0.018639901652932167,
                      0.006735226605087519,
                      -0.018599143251776695,
                      -0.011344254948198795,
                      0.006898257881402969,
                      0.005896295420825481,
                      -0.0025303801521658897,
                      -0.013762551359832287,
                      -0.00305683515034616,
                      0.0001466856338083744,
                      0.013803308829665184,
                      -0.008348556235432625,
                      0.017648128792643547,
                      0.0006079705781303346,
                      -0.012302063405513763,
                      -0.020976681262254715,
                      -0.029345616698265076,
                      0.001504642190411687,
                      -0.014849426224827766,
                      -0.023394977673888206,
                      -0.014672808349132538,
                      0.031193304806947708,
                      0.010685336776077747,
                      -0.01571892574429512,
                      -0.01691448874771595,
                      -0.020840823650360107,
                      -0.03855688124895096,
                      -0.00805645901709795,
                      -0.007723603397607803,
                      0.017267722636461258,
                      -0.0014180318685248494,
                      0.017403582111001015,
                      0.027593031525611877,
                      0.02425089292228222,
                      0.007105443626642227,
                      -0.003943317569792271,
                      0.0024964152835309505,
                      0.000519237422849983,
                      -0.0002857291838154197,
                      -0.02801419608294964,
                      -0.019061066210269928,
                      -0.0009433733066543937,
                      -0.047414910048246384,
                      -0.000902615487575531,
                      0.04279569163918495,
                      -0.018667073920369148,
                      0.017607370391488075,
                      0.05630011111497879,
                      -0.008993888273835182,
                      -0.016425393521785736,
                      -0.001525870175100863,
                      0.015161902643740177,
                      0.010196243412792683,
                      0.02980753965675831,
                      0.015977058559656143,
                      -0.026207266375422478,
                      -0.00731602543964982,
                      0.026709945872426033,
                      0.00895992387086153,
                      -0.0329594761133194,
                      -0.0018120239255949855,
                      0.01653408259153366,
                      -0.003362518735229969,
                      0.003048344049602747,
                      -0.019210509955883026,
                      0.01650691032409668,
                      -0.0030636282172054052,
                      -0.008858028799295425,
                      0.0046531823463737965,
                      0.0147271528840065,
                      -0.02967168018221855,
                      -0.0028530461713671684,
                      0.029101070016622543,
                      -0.019400713965296745,
                      -0.021397845819592476,
                      0.022606994956731796,
                      -0.01400709804147482,
                      -0.0023690471425652504,
                      0.016221605241298676,
                      -0.014740738086402416,
                      0.006843914277851582,
                      -0.004707525949925184,
                      0.0009951696265488863,
                      -0.01847686991095543,
                      -0.0004294428799767047,
                      -0.029345616698265076,
                      0.028122883290052414,
                      -0.023150430992245674,
                      0.030731383711099625,
                      0.011106501333415508,
                      -0.02942713350057602,
                      0.006497472990304232,
                      0.0005608443170785904,
                      -0.020243041217327118,
                      -0.014686394482851028,
                      -0.014876597560942173,
                      0.009299571625888348,
                      -0.009007474407553673,
                      0.014088613912463188,
                      0.0007591140456497669,
                      -0.01351121161133051,
                      -0.015610238537192345,
                      -0.00285983900539577,
                      0.01832742430269718,
                      -0.006847310811281204,
                      -0.00011325149534968659,
                      0.23715606331825256,
                      -0.03877425566315651,
                      0.010936676524579525,
                      0.0009178996551781893,
                      -0.019183339551091194,
                      0.0034678096417337656,
                      -0.008531966246664524,
                      0.010950262658298016,
                      -0.009170505218207836,
                      -0.0037667001597583294,
                      0.02168315090239048,
                      0.002808891935274005,
                      -0.010705715976655483,
                      0.0050437781028449535,
                      0.016411809250712395,
                      -0.01331421546638012,
                      -0.013090047053992748,
                      -0.021139713004231453,
                      -0.015542308799922466,
                      0.005030191969126463,
                      0.010006040334701538,
                      0.020637033507227898,
                      -0.013769344426691532,
                      -0.02226734533905983,
                      0.03228018060326576,
                      -0.002003925386816263,
                      0.0007862859056331217,
                      -0.004143709782510996,
                      0.0354321151971817,
                      0.011446149088442326,
                      -0.009041438810527325,
                      0.005003020167350769,
                      -0.000519237422849983,
                      -0.0013653864152729511,
                      -0.0029005969408899546,
                      0.003051740350201726,
                      0.006072912365198135,
                      0.009428638033568859,
                      0.019061066210269928,
                      -0.0006410862552002072,
                      0.04154578596353531,
                      0.021166885271668434,
                      -0.008708584122359753,
                      -0.019781120121479034,
                      0.010637786239385605,
                      0.009795458056032658,
                      -0.020759306848049164,
                      -0.012302063405513763,
                      -0.008008908480405807,
                      0.029698852449655533,
                      -0.032089974731206894,
                      0.022905884310603142,
                      0.02465846948325634,
                      0.037306975573301315,
                      -0.0007421316695399582,
                      -0.01732206530869007,
                      0.00954411830753088,
                      0.0015487964265048504,
                      0.03279644623398781,
                      -0.012994945980608463,
                      -0.017362823709845543,
                      0.009292778559029102,
                      -0.005828365683555603,
                      0.023367807269096375,
                      -0.0037395283579826355,
                      0.02627519704401493,
                      -0.038692738860845566,
                      0.04312175512313843,
                      0.02268850989639759,
                      0.014102199114859104,
                      0.008919165469706059,
                      -0.00975470058619976,
                      -0.013558762148022652,
                      0.004568270407617092,
                      -0.02624802477657795,
                      -0.02490301802754402,
                      0.016941659152507782,
                      0.01709110476076603,
                      0.015555894933640957,
                      0.03448110073804855,
                      0.002479432849213481,
                      -0.019047480076551437,
                      -0.027538688853383064,
                      -0.02209072932600975,
                      -0.018585557118058205,
                      -0.04298589378595352,
                      -0.0010792326647788286,
                      -0.020025666803121567,
                      0.0010299836285412312,
                      -0.03521474078297615,
                      0.01729489490389824,
                      -0.032334521412849426,
                      -0.006168013904243708,
                      -0.013490832410752773,
                      0.02327270433306694,
                      0.010732888244092464,
                      0.004731301683932543,
                      0.018177980557084084,
                      0.010848368518054485,
                      -0.010026419535279274,
                      -0.0349973663687706,
                      0.056626174598932266,
                      0.017580198124051094,
                      -0.0033455363009124994,
                      -0.014292402192950249,
                      0.004659975413233042,
                      0.0033828974701464176,
                      0.0019954340532422066,
                      0.012383579276502132,
                      -0.01074647344648838,
                      0.0038482157979160547,
                      -0.01532493345439434,
                      0.013354972936213017,
                      -0.016221605241298676,
                      0.0037157528568059206,
                      0.0009289382142014802,
                      0.011955621652305126,
                      0.012369993142783642,
                      0.03024228848516941,
                      -0.018001362681388855,
                      0.003722545923665166,
                      -0.019414300099015236,
                      -0.0002935835509561002,
                      0.030079258605837822,
                      0.0014358634361997247,
                      -0.01493094116449356,
                      0.003742924891412258,
                      0.007234510034322739,
                      -0.02310967445373535,
                      0.0024318823125213385,
                      -0.004102952312678099,
                      -0.016004230827093124,
                      0.01123556774109602,
                      -0.02107178419828415,
                      -0.03575817868113518,
                      0.013993511907756329,
                      0.009965282864868641,
                      -0.004975848365575075,
                      -0.017226964235305786,
                      0.012118653394281864,
                      0.00012758040975313634,
                      0.004038419108837843,
                      0.005233981180936098,
                      -0.01830025389790535,
                      -0.011758625507354736,
                      -0.009584876708686352,
                      0.013205528259277344,
                      -0.015868371352553368,
                      -0.006881275679916143,
                      -0.008776513859629631,
                      -0.01811004988849163,
                      -0.028394602239131927,
                      -0.0058079869486391544,
                      -0.03524191305041313,
                      0.020039252936840057,
                      0.010603821836411953,
                      -0.008389314636588097,
                      -0.022376032546162605,
                      0.017362823709845543,
                      0.009109368547797203,
                      -0.020840823650360107,
                      -0.005729867611080408,
                      0.0032860978972166777,
                      -0.023924829438328743,
                      -0.001446052803657949,
                      -0.004181071184575558,
                      -0.17389994859695435,
                      -0.011317082680761814,
                      0.016778629273176193,
                      -0.040241535753011703,
                      0.008898787200450897,
                      -0.0012066008057445288,
                      0.01709110476076603,
                      -0.02171032316982746,
                      -0.038502536714076996,
                      0.012981359846889973,
                      0.02085440792143345,
                      0.00885123573243618,
                      -0.027008837088942528,
                      -0.017063932493329048,
                      -0.017036762088537216,
                      0.004619217477738857,
                      0.0025337766855955124,
                      0.021737493574619293,
                      0.0030670245178043842,
                      0.0015411543427035213,
                      0.042034879326820374,
                      -0.0326605848968029,
                      0.017661714926362038,
                      -0.014075027778744698,
                      0.00945581030100584,
                      -0.0028615372721105814,
                      -0.002742660464718938,
                      0.009985661134123802,
                      -0.0026662396267056465,
                      -0.03722545877099037,
                      0.008599895983934402,
                      0.006239340174943209,
                      0.025691000744700432,
                      -0.009184091351926327,
                      0.0023147035390138626,
                      0.017199791967868805,
                      0.016438979655504227,
                      -0.006888068746775389,
                      -0.009347123093903065,
                      0.0025949133560061455,
                      0.031111789867281914,
                      0.01929202675819397,
                      0.016547666862607002,
                      -0.01400709804147482,
                      -0.019061066210269928,
                      0.020786479115486145,
                      0.020134354010224342,
                      -0.014387504197657108,
                      0.021723909303545952,
                      -0.019726775586605072,
                      0.00318420329131186,
                      -0.00817193929105997,
                      -0.0016048384131863713,
                      0.013151183724403381,
                      0.023965587839484215,
                      0.009190884418785572,
                      -0.0033795011695474386,
                      0.01666994020342827,
                      -0.014795082621276379,
                      0.004391652997583151,
                      -0.00019243202405050397,
                      -0.024617712944746017,
                      0.011330668814480305,
                      -0.017145449295639992,
                      -0.02725338377058506,
                      -0.01262133289128542,
                      -0.017756815999746323,
                      0.0047245086170732975,
                      0.0006156126619316638,
                      0.011004606261849403,
                      -0.015447206795215607,
                      -0.013844067230820656,
                      -0.0068608964793384075,
                      0.00015676894690841436,
                      0.010474755428731441,
                      0.032905131578445435,
                      -0.009238434955477715,
                      -0.005444562993943691,
                      0.019821878522634506,
                      0.009978868998587132,
                      -0.013185149058699608,
                      0.032714929431676865,
                      -0.018599143251776695,
                      0.018965963274240494,
                      -0.006385388784110546,
                      0.0009892258094623685,
                      0.0009255417389795184,
                      0.018558386713266373,
                      -0.010624200105667114,
                      -0.01869424618780613,
                      0.005186430178582668,
                      -0.03024228848516941,
                      0.013205528259277344,
                      0.0012210358399897814,
                      0.0019852446857839823,
                      0.011371427215635777,
                      0.02310967445373535,
                      0.0039161453023552895,
                      0.004201449919492006,
                      -0.015474379062652588,
                      0.0053902193903923035,
                      0.003841422963887453,
                      0.009319950826466084,
                      0.029399961233139038,
                      0.011588801629841328,
                      0.027959851548075676,
                      -0.024413922801613808,
                      0.013816894963383675,
                      0.029726022854447365,
                      0.0038244405295699835,
                      -0.02429164946079254,
                      -0.016642769798636436,
                      0.015053214505314827,
                      0.020650619640946388,
                      0.013796515762805939,
                      0.015352105721831322,
                      -0.0035459287464618683,
                      -0.017947018146514893,
                      0.011208395473659039,
                      -0.014061441645026207,
                      0.03415503725409508,
                      0.0034763009753078222,
                      -0.01793343387544155,
                      0.02168315090239048,
                      -0.02207714319229126,
                      -0.04717036336660385,
                      -0.09417769312858582,
                      -0.038094960153102875,
                      -0.019658846780657768,
                      0.0047143190167844296,
                      -0.003844819264486432,
                      0.016574839130043983,
                      0.02688656374812126,
                      0.018830103799700737,
                      0.00039420436951331794,
                      0.03282361850142479,
                      -0.019047480076551437,
                      -0.013558762148022652,
                      0.002316401805728674,
                      -0.02687297761440277,
                      0.008104009553790092,
                      0.0024607523810118437,
                      0.0016260665142908692,
                      0.004710922483354807,
                      -0.017960604280233383,
                      0.01969960518181324,
                      0.0030789123848080635,
                      -0.030785726383328438,
                      0.032714929431676865,
                      -0.03184542804956436,
                      0.031138960272073746,
                      -0.026207266375422478,
                      -0.031763914972543716,
                      0.008307798765599728,
                      -0.0024641486816108227,
                      -0.016778629273176193,
                      -0.012336027808487415,
                      -0.02926410175859928,
                      9.860416321316734e-05,
                      -0.014944527298212051,
                      0.0032657189294695854,
                      -0.008756134659051895,
                      -0.016194432973861694,
                      -0.027348484843969345,
                      -0.003950110170990229,
                      -0.029970571398735046,
                      0.0020327954553067684,
                      0.0007306685438379645,
                      0.0004997076466679573,
                      -0.01592271402478218,
                      -0.012132239528000355,
                      -0.04002416133880615,
                      -0.010963848792016506,
                      0.014387504197657108,
                      0.011317082680761814,
                      -0.007295646704733372,
                      -0.014876597560942173,
                      -0.008919165469706059,
                      -0.03399200737476349,
                      -0.007064685691148043,
                      0.01313080545514822,
                      0.012852293439209461,
                      0.0032708137296140194,
                      0.004452789667993784,
                      -0.011147258803248405,
                      -0.02171032316982746,
                      0.007954563945531845,
                      -0.007397541310638189,
                      -0.01892520673573017,
                      0.0072888536378741264,
                      0.0010936676990240812,
                      -0.004371274262666702,
                      -0.00934033002704382,
                      -0.025568727403879166,
                      0.012600953690707684,
                      -0.0306226946413517,
                      0.0029464494436979294,
                      0.014469020068645477,
                      0.0035798936150968075,
                      0.019604502245783806,
                      -0.021207643672823906,
                      -0.010705715976655483,
                      -0.015515136532485485,
                      0.021574463695287704,
                      0.005845348350703716,
                      -0.01609933190047741,
                      -0.03228018060326576,
                      -0.022729268297553062,
                      0.01729489490389824,
                      -0.025025291368365288,
                      -0.006803156342357397,
                      -0.00766246672719717,
                      -0.024644885212183,
                      -0.00037127811810933053,
                      -0.0038040615618228912,
                      -0.029698852449655533,
                      0.012668883427977562,
                      0.03045966476202011,
                      -0.0013450074475258589,
                      -0.013191942125558853,
                      -0.006235943641513586,
                      0.0068608964793384075,
                      -0.025758931413292885,
                      0.004374670796096325,
                      0.006096688099205494,
                      0.045458536595106125,
                      -0.01364027801901102,
                      -0.00641595758497715,
                      -0.052822113037109375,
                      0.008803685195744038,
                      0.0031909963581711054,
                      -0.022946642711758614,
                      0.009218056686222553,
                      0.023775383830070496,
                      0.037497177720069885,
                      0.009421844966709614,
                      -0.013470453210175037,
                      0.030731383711099625,
                      -0.01073968131095171,
                      0.02168315090239048,
                      0.013049289584159851,
                      0.0001267312909476459,
                      -0.012994945980608463,
                      -0.024821501225233078,
                      0.02385690063238144,
                      -0.021968455985188484,
                      0.015814026817679405,
                      0.024726400151848793,
                      -0.003197789192199707,
                      -0.011378219351172447,
                      0.012859086506068707,
                      0.01253981702029705,
                      -0.0019173149485141039,
                      -0.0010130011942237616,
                      -0.015433620661497116,
                      0.014686394482851028,
                      -0.01733565144240856,
                      -0.02268850989639759,
                      -0.002095630392432213,
                      -0.019590916112065315,
                      0.006246133241802454,
                      0.00995169673115015,
                      -0.010889125987887383,
                      -0.002598309889435768,
                      0.0007387351361103356,
                      0.008830857463181019,
                      0.02268850989639759,
                      0.009462603367865086,
                      -0.01194203644990921,
                      -0.026112165302038193,
                      0.02882935293018818,
                      -0.025976305827498436,
                      0.0027494532987475395,
                      -0.0033964836038649082,
                      0.004632803611457348,
                      -0.002212808933109045,
                      -0.003817647462710738,
                      -0.006483886856585741,
                      0.024155789986252785,
                      0.02108537033200264,
                      0.009435431100428104,
                      -0.022973814979195595,
                      -0.01631670631468296,
                      0.005447959527373314,
                      0.013551969081163406,
                      0.026152923703193665,
                      -0.026913736015558243,
                      -0.009809044189751148,
                      0.017607370391488075,
                      0.007825498469173908,
                      0.002411503344774246,
                      -0.00325383129529655,
                      0.0016421998152509332,
                      0.012859086506068707,
                      0.019237682223320007,
                      -0.0017186206532642245,
                      -0.009197677485644817,
                      -0.034535445272922516,
                      -0.0014018985675647855,
                      -0.0013008532114326954,
                      0.012071102857589722,
                      0.011575215496122837,
                      0.03564948961138725,
                      0.006229150574654341,
                      0.00017884607950691134,
                      0.00716658029705286,
                      -0.01463205087929964,
                      0.03662768006324768,
                      0.015596652403473854,
                      -0.028965210542082787,
                      -0.01342969574034214,
                      -0.017770402133464813,
                      0.004829799756407738,
                      -0.0027358673978596926,
                      0.006796363741159439,
                      -0.0024556575808674097,
                      -0.01402068417519331,
                      -0.0037870791275054216,
                      0.005712885409593582,
                      0.007988529279828072,
                      0.002396219177171588,
                      0.0053800297901034355,
                      -0.004089366178959608,
                      0.009809044189751148,
                      0.010984227992594242,
                      0.00021461529831867665,
                      0.012607746757566929,
                      -0.013592727482318878,
                      0.001277926960028708,
                      0.0013322706799954176,
                      -0.004595442209392786,
                      -0.005634766072034836,
                      -0.014536949805915356,
                      0.020990267395973206,
                      -0.019020307809114456,
                      -0.028313087299466133,
                      0.00040587977855466306,
                      0.0034151640720665455,
                      0.010298137553036213,
                      0.020243041217327118,
                      -0.015257003717124462,
                      0.0015292667085304856,
                      -0.02861197665333748,
                      0.03423655405640602,
                      0.013735379092395306,
                      -0.017457924783229828,
                      -0.008498001843690872,
                      0.007995322346687317,
                      0.007085064426064491,
                      -0.00015952858666423708,
                      0.028557633981108665,
                      0.0013874635333195329,
                      0.01592271402478218,
                      0.021642392501235008,
                      0.04121972247958183,
                      -0.010481548495590687,
                      0.00894633773714304,
                      0.008287419565021992,
                      -0.013253078795969486,
                      -0.017145449295639992,
                      -0.005529474932700396,
                      -0.019020307809114456,
                      -0.00023456964117940515,
                      0.0033217607997357845,
                      0.0027137903962284327,
                      0.022172244265675545,
                      -0.0034253536723554134,
                      0.0892324149608612,
                      0.02586761862039566,
                      0.004996227100491524,
                      0.0027341691311448812,
                      -0.0010401731124147773,
                      0.025813274085521698,
                      0.019210509955883026,
                      0.014346746727824211,
                      -0.0023418753407895565,
                      0.00481961015611887,
                      0.02449543960392475,
                      0.01633029244840145,
                      0.006545023526996374,
                      -0.012492266483604908,
                      -0.014795082621276379,
                      0.006239340174943209,
                      0.011595594696700573,
                      -0.0030924982856959105,
                      0.01787908934056759,
                      -0.013103633187711239,
                      0.046056315302848816,
                      -0.026492571458220482,
                      0.007458677981048822,
                      0.011955621652305126,
                      -0.03461695834994316,
                      -0.006049137096852064,
                      0.010760059580206871,
                      0.015664581209421158,
                      -0.018667073920369148,
                      -0.039073146879673004,
                      0.019658846780657768,
                      -0.002095630392432213,
                      -0.028150055557489395,
                      -0.012845500372350216,
                      0.022213002666831017,
                      -0.002409805078059435,
                      -0.001168390386737883,
                      -0.005818176083266735,
                      0.018245909363031387,
                      0.010617407038807869,
                      0.01501245703548193,
                      0.01690090261399746,
                      -0.004459582734853029,
                      -0.03497019410133362,
                      -0.01194203644990921,
                      -0.0059676216915249825,
                      -0.02008001133799553,
                      -0.027348484843969345,
                      -0.016588425263762474
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 8,
                  "total_tokens": 8
                }
              },
              "start_time": 1694832910.037893,
              "end_time": 1694832910.619005,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        },
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.022036384791135788,
              -0.0164797380566597,
              0.0005999039276503026,
              -0.007227716967463493,
              0.003868594765663147,
              0.01381010189652443,
              -0.006500869523733854,
              -0.011711074970662594,
              -0.01653408259153366,
              -0.015121144242584705,
              0.0027137903962284327,
              0.027362070977687836,
              -0.025758931413292885,
              0.00944222416728735,
              3.94045164284762e-05,
              0.0016226699808612466,
              0.016561252996325493,
              0.015487965196371078,
              0.004052004776895046,
              0.0009892258094623685,
              -0.005791004281491041,
              -0.014170128852128983,
              -0.019210509955883026,
              0.020990267395973206,
              -0.006324252113699913,
              -0.02627519704401493,
              0.05154503509402275,
              -0.033502914011478424,
              0.00033030801569111645,
              -0.013558762148022652,
              0.027144696563482285,
              0.030269460752606392,
              -0.010019626468420029,
              -0.008131181821227074,
              -0.013069668784737587,
              0.0003339167742524296,
              0.01770247146487236,
              -0.002297721104696393,
              0.017009589821100235,
              -0.01224092673510313,
              0.015800440683960915,
              0.01892520673573017,
              0.013878031633794308,
              -0.001744094304740429,
              -0.0005765530513599515,
              0.003309873165562749,
              0.01252623088657856,
              -0.004153899382799864,
              0.0005892898770980537,
              -0.023788969963788986,
              0.018653487786650658,
              0.01949581503868103,
              -0.02384331449866295,
              -0.002343573607504368,
              -0.03442675620317459,
              -0.0030534386169165373,
              -0.0077575682662427425,
              0.02582686021924019,
              0.009068611077964306,
              -0.02248472161591053,
              0.010753266513347626,
              0.0006746265571564436,
              -0.015664581209421158,
              0.019156167283654213,
              0.0048841433599591255,
              -0.009299571625888348,
              -0.022946642711758614,
              0.029101070016622543,
              0.010603821836411953,
              0.008885201066732407,
              0.007628501858562231,
              0.020623447373509407,
              0.019455058500170708,
              -0.011086122132837772,
              0.03654616326093674,
              -0.002844554837793112,
              -0.01502604316920042,
              -0.005852140951901674,
              0.008036079816520214,
              0.0004491849394980818,
              0.006405767984688282,
              0.02306891605257988,
              -0.02109895646572113,
              0.020922338590025902,
              0.013062875717878342,
              -0.007594536989927292,
              0.013103633187711239,
              0.01690090261399746,
              -0.019577331840991974,
              -0.0003455921832937747,
              0.006201978772878647,
              0.002939656376838684,
              0.03314967826008797,
              0.004282965790480375,
              0.000431353400927037,
              0.03024228848516941,
              -0.004602235276252031,
              0.029101070016622543,
              0.0016498418990522623,
              -0.035106055438518524,
              0.009027853608131409,
              -0.0075266072526574135,
              -0.007940978743135929,
              -0.01224092673510313,
              -0.019224096089601517,
              0.0021533705294132233,
              0.00983621645718813,
              -0.010372860357165337,
              0.028150055557489395,
              -0.008076838217675686,
              -0.007716810330748558,
              0.03203563392162323,
              -0.0034202588722109795,
              -0.020215868949890137,
              0.005529474932700396,
              0.021533705294132233,
              0.0010299836285412312,
              0.0005413145408965647,
              0.017457924783229828,
              0.020732134580612183,
              0.03328553959727287,
              -0.002212808933109045,
              0.0011565026361495256,
              -0.01827308163046837,
              0.022348862141370773,
              -0.008137974888086319,
              -0.021615220233798027,
              -0.019998494535684586,
              0.008701791055500507,
              -0.029481476172804832,
              0.006055930163711309,
              -0.0036444268189370632,
              0.0013798214495182037,
              -0.023816142231225967,
              -0.002878519706428051,
              0.023191189393401146,
              -0.00796135701239109,
              -0.010107934474945068,
              -0.026424642652273178,
              -0.01768888533115387,
              0.016819385811686516,
              0.022593408823013306,
              -0.023177603259682655,
              -0.03518756851553917,
              -0.026601258665323257,
              0.02546004019677639,
              0.0007625105208717287,
              0.02842177450656891,
              0.010114727541804314,
              0.002683222061023116,
              0.0042456043884158134,
              -0.031138960272073746,
              -0.018055707216262817,
              0.009591669775545597,
              0.0022875317372381687,
              0.013979925774037838,
              -0.00032075541093945503,
              0.0026458606589585543,
              -0.000471262086648494,
              -0.00011494974023662508,
              0.0030279650818556547,
              0.014550535008311272,
              0.011554837226867676,
              -0.009598462842404842,
              0.03760586678981781,
              0.04051325470209122,
              -0.004279569257050753,
              -0.0026611448265612125,
              0.012397164478898048,
              0.003573100548237562,
              -0.008688204921782017,
              0.01570533961057663,
              -0.026981664821505547,
              0.004629407078027725,
              0.015433620661497116,
              0.00746547058224678,
              0.017430752515792847,
              0.007805119268596172,
              0.0007680298294872046,
              -0.013361766003072262,
              -0.0051320865750312805,
              0.006398974917829037,
              0.03399200737476349,
              0.005003020167350769,
              -0.015257003717124462,
              -0.002204317832365632,
              0.003770096693187952,
              -0.035350602120161057,
              0.0028004006016999483,
              -0.044643379747867584,
              0.018205150961875916,
              0.019061066210269928,
              0.011711074970662594,
              -0.004673561546951532,
              -0.6364738345146179,
              -0.014455433934926987,
              0.016629183664917946,
              -0.00835534930229187,
              0.021329917013645172,
              -0.013083254918456078,
              -0.010352482087910175,
              -0.00566193787381053,
              -0.01171786803752184,
              0.03263341262936592,
              0.005447959527373314,
              -0.012607746757566929,
              -0.005906485021114349,
              -0.01502604316920042,
              -0.0013458565808832645,
              -0.022511892020702362,
              -0.00936070829629898,
              -0.00402143644168973,
              0.01869424618780613,
              -0.003973885904997587,
              -0.02446826733648777,
              0.02124840021133423,
              -0.04380105063319206,
              0.005852140951901674,
              -0.01752585545182228,
              0.007132615428417921,
              0.002350366674363613,
              -0.009313157759606838,
              0.007295646704733372,
              -0.007268474902957678,
              -0.01323269959539175,
              0.03336705267429352,
              0.03956224024295807,
              -0.015284175984561443,
              0.031709570437669754,
              0.011860520578920841,
              -0.002382633276283741,
              0.015759684145450592,
              -0.031193304806947708,
              0.0256638303399086,
              -0.00866782572120428,
              -0.019618088379502296,
              0.02329987660050392,
              -0.007010342087596655,
              0.03217149153351784,
              0.007248095702379942,
              0.01232923474162817,
              0.021425018087029457,
              0.0018884448800235987,
              0.007431505713611841,
              -0.027606617659330368,
              0.008722169324755669,
              0.0014333160361275077,
              -0.008464036509394646,
              0.014401090331375599,
              -0.0018341010436415672,
              0.022837955504655838,
              -0.014061441645026207,
              0.0002553731028456241,
              -0.022606994956731796,
              0.011643145233392715,
              0.034345243126153946,
              -0.010902712121605873,
              -0.02567741461098194,
              -0.011004606261849403,
              0.015229832381010056,
              0.0035323428455740213,
              0.012770777568221092,
              0.011887691915035248,
              -0.04477923735976219,
              0.010481548495590687,
              0.024984532967209816,
              0.0006393880466930568,
              -0.0006792966742068529,
              0.02008001133799553,
              0.017743229866027832,
              -0.00228923000395298,
              -0.015121144242584705,
              -0.0004844234499614686,
              0.022973814979195595,
              -0.009252021089196205,
              -0.005587215535342693,
              0.012607746757566929,
              -0.0028190813027322292,
              -0.018748588860034943,
              0.017797574400901794,
              -0.006918637081980705,
              -0.02309608832001686,
              0.005352857988327742,
              0.026207266375422478,
              -0.0012125446228310466,
              0.02782399207353592,
              -0.021601635962724686,
              -0.03942637890577316,
              0.02248472161591053,
              0.012369993142783642,
              0.003501774510368705,
              -0.016642769798636436,
              0.03247038275003433,
              -0.007241302635520697,
              0.002798702334985137,
              -0.017607370391488075,
              0.020243041217327118,
              0.023761799558997154,
              0.00856593158096075,
              0.016438979655504227,
              0.02270209603011608,
              0.020637033507227898,
              0.002071854891255498,
              -0.017199791967868805,
              0.004792438354343176,
              -0.016357464715838432,
              0.00825345516204834,
              0.008776513859629631,
              -0.006633332464843988,
              -0.0329594761133194,
              0.014183714985847473,
              0.0027817199006676674,
              0.012220547534525394,
              -0.021234814077615738,
              0.0019427885999903083,
              -0.009598462842404842,
              0.012444715946912766,
              -0.016642769798636436,
              0.019210509955883026,
              0.017675301060080528,
              -0.0031536349561065435,
              0.004310137592256069,
              -0.020650619640946388,
              -0.005298514384776354,
              -0.021832596510648727,
              -0.006426146719604731,
              0.0053800297901034355,
              -0.012152617797255516,
              0.04393691197037697,
              0.03899163007736206,
              0.03260624036192894,
              -0.0015937999123707414,
              -0.02050117403268814,
              0.005006416700780392,
              -0.012315649539232254,
              -0.010495133697986603,
              0.01567816734313965,
              -0.005740057211369276,
              -0.052034128457307816,
              -0.016167260706424713,
              0.008891994133591652,
              0.003070421051234007,
              0.001690599718131125,
              -0.011554837226867676,
              0.0015428526094183326,
              -0.02624802477657795,
              -0.018585557118058205,
              -0.005899691954255104,
              -0.013090047053992748,
              -0.028557633981108665,
              -0.009408259764313698,
              -0.04668127000331879,
              -0.019536573439836502,
              -0.031709570437669754,
              0.010325309820473194,
              0.00373273529112339,
              -0.011989586986601353,
              -0.012770777568221092,
              0.0011378219351172447,
              -0.03497019410133362,
              0.005515889264643192,
              0.022213002666831017,
              -0.027525102719664574,
              -0.033910490572452545,
              0.028938040137290955,
              -0.01846328377723694,
              -0.01033210288733244,
              0.005774022080004215,
              -0.026682773604989052,
              0.013776137493550777,
              -0.03325836732983589,
              -0.016017816960811615,
              0.0003791324852500111,
              -0.016044987365603447,
              0.005030191969126463,
              0.030704211443662643,
              -0.010012833401560783,
              -0.009224848821759224,
              0.012220547534525394,
              0.0020344937220215797,
              0.017172621563076973,
              -0.00310948072001338,
              -0.02488943189382553,
              0.00974111445248127,
              0.00874934159219265,
              0.026587672531604767,
              -0.003061929950490594,
              0.02090875245630741,
              0.012573782354593277,
              0.006110273767262697,
              0.016357464715838432,
              -0.009122954681515694,
              0.0008754436275921762,
              -0.005172844510525465,
              0.025297008454799652,
              0.010732888244092464,
              0.00034325712476857007,
              -0.012940602377057076,
              -0.008294212631881237,
              0.017457924783229828,
              -8.363840606762096e-05,
              -0.016438979655504227,
              0.008219489827752113,
              -0.00042880602995865047,
              0.01461846474558115,
              -0.029508648440241814,
              -0.02325911819934845,
              -0.0005476829828694463,
              -0.008708584122359753,
              0.021044611930847168,
              -0.009068611077964306,
              0.02289229817688465,
              -0.034263726323843,
              0.0033591222018003464,
              0.003783682594075799,
              -0.01793343387544155,
              0.004975848365575075,
              0.0009900749428197742,
              -0.011765418574213982,
              0.006297080311924219,
              -0.008097216486930847,
              -0.012702848762273788,
              0.012410750612616539,
              -0.022362448275089264,
              0.0005455601494759321,
              0.028285915032029152,
              0.02999774180352688,
              0.020025666803121567,
              -0.0012032042723149061,
              -0.00856593158096075,
              0.014469020068645477,
              -0.010196243412792683,
              0.025595899671316147,
              -0.017159035429358482,
              0.0026017064228653908,
              0.01154804416000843,
              0.013110426254570484,
              -0.0015776666114106774,
              0.03545928746461868,
              0.015814026817679405,
              0.03244321048259735,
              0.010651372373104095,
              -0.016072159633040428,
              0.02287871204316616,
              -0.01431957446038723,
              -0.008878407999873161,
              -0.030731383711099625,
              0.0011514079524204135,
              0.0240606889128685,
              -0.00924522802233696,
              0.0040757800452411175,
              0.012899843975901604,
              -0.0057094888761639595,
              0.033883318305015564,
              0.016357464715838432,
              0.008362142369151115,
              0.022797197103500366,
              0.007805119268596172,
              0.019183339551091194,
              -0.014455433934926987,
              -0.014591293409466743,
              -0.0058079869486391544,
              -0.009184091351926327,
              -0.004938486963510513,
              -0.017254136502742767,
              -0.015080386772751808,
              0.0010206432780250907,
              -0.010617407038807869,
              0.01441467646509409,
              0.03024228848516941,
              -0.02490301802754402,
              0.021438604220747948,
              -0.0067929672077298164,
              0.034127864986658096,
              -0.0023045141715556383,
              -0.029916226863861084,
              0.03160088136792183,
              -0.006130652967840433,
              -0.027959851548075676,
              -0.0171318631619215,
              0.0032283575274050236,
              -0.02169673703610897,
              -0.004269379656761885,
              -0.018830103799700737,
              0.0016956944018602371,
              0.009007474407553673,
              -0.0019852446857839823,
              0.025120392441749573,
              0.00111829221714288,
              0.013293836265802383,
              0.007302439771592617,
              -0.01986263506114483,
              0.004432410933077335,
              0.004486754536628723,
              0.0019190132152289152,
              0.00785946287214756,
              -0.006490679923444986,
              -0.028965210542082787,
              0.02029738575220108,
              0.001125934300944209,
              -0.029209759086370468,
              -0.016642769798636436,
              -0.016466151922941208,
              -0.015977058559656143,
              0.038257990032434464,
              0.009462603367865086,
              0.01868066005408764,
              -0.019346369430422783,
              -5.301698547555134e-05,
              0.0292369294911623,
              0.007893427275121212,
              -0.006996755953878164,
              0.008511587977409363,
              0.004408635664731264,
              -0.028313087299466133,
              -0.012268098071217537,
              -0.01671069860458374,
              0.008436865173280239,
              0.04317609965801239,
              0.04755076766014099,
              -0.004174278117716312,
              0.019020307809114456,
              -0.012757192365825176,
              0.03635596111416817,
              -0.025758931413292885,
              -0.0389372855424881,
              -0.021194057539105415,
              -0.01649332419037819,
              0.0015802140114828944,
              -0.02664201706647873,
              0.006531437858939171,
              -0.022810783237218857,
              0.005254359915852547,
              0.01025058701634407,
              -0.0003099291061516851,
              -0.02626161091029644,
              0.033747460693120956,
              -0.016547666862607002,
              -0.0009994152933359146,
              0.013701414689421654,
              0.006076308898627758,
              0.010467962361872196,
              0.001453694887459278,
              -0.019427886232733727,
              -0.012254512868821621,
              0.010807610116899014,
              -0.0035323428455740213,
              -0.011588801629841328,
              -0.016561252996325493,
              -0.022579822689294815,
              -0.01753944158554077,
              0.00787304900586605,
              -0.02051476016640663,
              0.03939921036362648,
              0.001176881487481296,
              0.006545023526996374,
              0.011004606261849403,
              0.020025666803121567,
              0.0029600353445857763,
              0.02662843093276024,
              -0.002523587318137288,
              -0.01540644932538271,
              7.790684321662411e-05,
              -0.030378147959709167,
              0.004031626041978598,
              0.016574839130043983,
              0.011317082680761814,
              -0.002888709306716919,
              0.0012337726075202227,
              -0.004809420555830002,
              -0.04111103713512421,
              -0.005013209767639637,
              0.014469020068645477,
              -0.014699980616569519,
              0.012471887283027172,
              0.010196243412792683,
              -0.01827308163046837,
              -0.02385690063238144,
              -0.017403582111001015,
              -0.014047855511307716,
              0.01872141659259796,
              -0.007275267504155636,
              0.00365801271982491,
              0.004639596678316593,
              -0.017362823709845543,
              -0.0244275089353323,
              -0.015596652403473854,
              0.04627368971705437,
              -0.008212696760892868,
              -0.04032305255532265,
              -0.04727904871106148,
              -0.016765043139457703,
              0.01221375446766615,
              0.011486907489597797,
              0.03260624036192894,
              -0.020786479115486145,
              -0.001669371617026627,
              -0.0023147035390138626,
              -0.007574158255010843,
              -0.03445392847061157,
              0.0223081037402153,
              -0.04051325470209122,
              0.0038006650283932686,
              -0.01102498546242714,
              -0.00746547058224678,
              -0.027565861120820045,
              -0.019332783296704292,
              0.0354321151971817,
              0.002744358731433749,
              0.005644955672323704,
              0.02961733564734459,
              -0.028965210542082787,
              0.01846328377723694,
              0.019984908401966095,
              0.0072005451656877995,
              0.005634766072034836,
              0.012886258773505688,
              -0.028313087299466133,
              -0.011955621652305126,
              -0.009285985492169857,
              -0.010420411825180054,
              -0.03325836732983589,
              0.03165522590279579,
              -0.006307269912213087,
              0.0036274443846195936,
              -0.021126126870512962,
              0.01733565144240856,
              -0.01072609517723322,
              0.008925958536565304,
              -0.023394977673888206,
              -0.00310948072001338,
              -0.01711827702820301,
              0.0313563346862793,
              -0.0038855771999806166,
              -0.001333968946710229,
              0.024970946833491325,
              -0.012268098071217537,
              -0.0019478832837194204,
              0.0025490608531981707,
              -0.02308250218629837,
              0.016588425263762474,
              0.019006721675395966,
              -0.0026017064228653908,
              0.021520119160413742,
              0.005498906597495079,
              -0.01688731648027897,
              -0.007037513889372349,
              0.025609485805034637,
              -0.024509025737643242,
              0.01668352633714676,
              -0.0017101294361054897,
              -0.0278511643409729,
              -0.03143785148859024,
              -0.016832971945405006,
              -0.002584723988547921,
              0.008932751603424549,
              -0.01461846474558115,
              -0.0012813233770430088,
              0.012179790064692497,
              -0.0006381143466569483,
              0.009992454200983047,
              -0.013225906528532505,
              0.01889803446829319,
              -0.022960228845477104,
              -0.014808667823672295,
              0.008531966246664524,
              0.001946185017004609,
              0.03206280618906021,
              -0.017634542658925056,
              0.02999774180352688,
              -0.030079258605837822,
              -0.0076692597940564156,
              0.0005748548428528011,
              -0.02983471192419529,
              -0.0004895181627944112,
              0.004462979268282652,
              0.03673636540770531,
              0.02324553392827511,
              0.03184542804956436,
              -0.000315448414767161,
              0.01082798931747675,
              0.01501245703548193,
              -0.0043169306591153145,
              -0.006378596182912588,
              -0.028394602239131927,
              -0.003245339961722493,
              -0.022117899730801582,
              0.017172621563076973,
              -0.001829006359912455,
              -0.013633484952151775,
              -0.008260248228907585,
              0.0026662396267056465,
              0.003905955934897065,
              0.002542268019169569,
              0.0028343654703348875,
              0.006945808883756399,
              0.0010180958779528737,
              -0.005757039412856102,
              -0.03635596111416817,
              0.02325911819934845,
              -0.004198053851723671,
              -0.0004954620380885899,
              -0.042279426008462906,
              -0.025595899671316147,
              0.013484039343893528,
              0.011065742932260036,
              0.014903769828379154,
              0.012397164478898048,
              0.013721792958676815,
              -0.015243417583405972,
              0.0006584932561963797,
              -0.008531966246664524,
              -0.003461016807705164,
              -0.001688052318058908,
              -0.014890183694660664,
              -0.029454305768013,
              -0.016846558079123497,
              0.013966340571641922,
              0.028557633981108665,
              -0.021139713004231453,
              -0.003255529562011361,
              -0.010603821836411953,
              0.01628953404724598,
              0.008321384899318218,
              0.01709110476076603,
              -0.008606689050793648,
              0.004378067329525948,
              -0.0013467057142406702,
              -0.009768286719918251,
              -0.03638312965631485,
              -0.028693493455648422,
              -0.0037633036263287067,
              0.036301616579294205,
              0.0034525254741311073,
              -0.02328629046678543,
              0.026519743725657463,
              0.011969207786023617,
              0.0026203871238976717,
              -0.01569175347685814,
              0.004496944136917591,
              0.05230584740638733,
              0.02051476016640663,
              0.011887691915035248,
              0.029481476172804832,
              -0.0005655144923366606,
              0.0016761645674705505,
              0.01870783045887947,
              -0.0005846197018399835,
              -0.014346746727824211,
              -0.004975848365575075,
              -0.007594536989927292,
              0.006755605805665255,
              -0.008321384899318218,
              0.02429164946079254,
              0.02844894677400589,
              -0.018802933394908905,
              -0.01164993830025196,
              0.0006028758361935616,
              -0.00596422515809536,
              0.012471887283027172,
              0.013572348281741142,
              -0.01690090261399746,
              -0.00231130700558424,
              0.012397164478898048,
              -0.026981664821505547,
              0.016602011397480965,
              -0.006602764129638672,
              -0.005070949904620647,
              0.012132239528000355,
              0.006830328144133091,
              -0.012927016243338585,
              0.014428261667490005,
              0.020392486825585365,
              0.0012550007086247206,
              0.001064797630533576,
              0.02070496417582035,
              -0.015841199085116386,
              -0.027389243245124817,
              0.02861197665333748,
              0.030514007434248924,
              -0.014958113431930542,
              0.015134730376303196,
              0.008973509073257446,
              0.014958113431930542,
              0.01589554361999035,
              0.012763985432684422,
              -0.00374971772544086,
              0.027008837088942528,
              -0.035731006413698196,
              -0.008593102917075157,
              -0.013253078795969486,
              -0.024128619581460953,
              0.025568727403879166,
              -0.0280685406178236,
              0.003950110170990229,
              -0.0007968999561853707,
              -0.017362823709845543,
              0.0157460980117321,
              0.03200846165418625,
              0.013171562924981117,
              -0.0250796340405941,
              0.005325686186552048,
              -0.006157824769616127,
              0.01313080545514822,
              -0.009904146194458008,
              0.0034915851429104805,
              0.02168315090239048,
              -0.03043249249458313,
              -0.022158658131957054,
              0.010026419535279274,
              -0.0067623988725245,
              0.01692807301878929,
              -0.015460792928934097,
              -0.0007009492837823927,
              0.006840517744421959,
              0.028476117178797722,
              -0.04290438070893288,
              -0.027511516585946083,
              -0.005994793493300676,
              0.0066061606630682945,
              -0.01849045604467392,
              -0.005111707840114832,
              -0.006235943641513586,
              -0.01806929148733616,
              0.01493094116449356,
              -0.011466528289020061,
              0.005991396959871054,
              -0.011018192395567894,
              -0.003729338990524411,
              -0.009095782414078712,
              -0.0018952378304675221,
              -0.03757869452238083,
              -0.016955245286226273,
              0.0071801659651100636,
              0.016411809250712395,
              -0.006619746331125498,
              -0.009720736183226109,
              -0.010046797804534435,
              0.022593408823013306,
              -0.0049724518321454525,
              0.013667449355125427,
              -0.03423655405640602,
              -0.003525549778714776,
              0.0023265911731868982,
              0.007397541310638189,
              -0.011969207786023617,
              -0.042279426008462906,
              -0.004378067329525948,
              -0.018639901652932167,
              0.006735226605087519,
              -0.018599143251776695,
              -0.011344254948198795,
              0.006898257881402969,
              0.005896295420825481,
              -0.0025303801521658897,
              -0.013762551359832287,
              -0.00305683515034616,
              0.0001466856338083744,
              0.013803308829665184,
              -0.008348556235432625,
              0.017648128792643547,
              0.0006079705781303346,
              -0.012302063405513763,
              -0.020976681262254715,
              -0.029345616698265076,
              0.001504642190411687,
              -0.014849426224827766,
              -0.023394977673888206,
              -0.014672808349132538,
              0.031193304806947708,
              0.010685336776077747,
              -0.01571892574429512,
              -0.01691448874771595,
              -0.020840823650360107,
              -0.03855688124895096,
              -0.00805645901709795,
              -0.007723603397607803,
              0.017267722636461258,
              -0.0014180318685248494,
              0.017403582111001015,
              0.027593031525611877,
              0.02425089292228222,
              0.007105443626642227,
              -0.003943317569792271,
              0.0024964152835309505,
              0.000519237422849983,
              -0.0002857291838154197,
              -0.02801419608294964,
              -0.019061066210269928,
              -0.0009433733066543937,
              -0.047414910048246384,
              -0.000902615487575531,
              0.04279569163918495,
              -0.018667073920369148,
              0.017607370391488075,
              0.05630011111497879,
              -0.008993888273835182,
              -0.016425393521785736,
              -0.001525870175100863,
              0.015161902643740177,
              0.010196243412792683,
              0.02980753965675831,
              0.015977058559656143,
              -0.026207266375422478,
              -0.00731602543964982,
              0.026709945872426033,
              0.00895992387086153,
              -0.0329594761133194,
              -0.0018120239255949855,
              0.01653408259153366,
              -0.003362518735229969,
              0.003048344049602747,
              -0.019210509955883026,
              0.01650691032409668,
              -0.0030636282172054052,
              -0.008858028799295425,
              0.0046531823463737965,
              0.0147271528840065,
              -0.02967168018221855,
              -0.0028530461713671684,
              0.029101070016622543,
              -0.019400713965296745,
              -0.021397845819592476,
              0.022606994956731796,
              -0.01400709804147482,
              -0.0023690471425652504,
              0.016221605241298676,
              -0.014740738086402416,
              0.006843914277851582,
              -0.004707525949925184,
              0.0009951696265488863,
              -0.01847686991095543,
              -0.0004294428799767047,
              -0.029345616698265076,
              0.028122883290052414,
              -0.023150430992245674,
              0.030731383711099625,
              0.011106501333415508,
              -0.02942713350057602,
              0.006497472990304232,
              0.0005608443170785904,
              -0.020243041217327118,
              -0.014686394482851028,
              -0.014876597560942173,
              0.009299571625888348,
              -0.009007474407553673,
              0.014088613912463188,
              0.0007591140456497669,
              -0.01351121161133051,
              -0.015610238537192345,
              -0.00285983900539577,
              0.01832742430269718,
              -0.006847310811281204,
              -0.00011325149534968659,
              0.23715606331825256,
              -0.03877425566315651,
              0.010936676524579525,
              0.0009178996551781893,
              -0.019183339551091194,
              0.0034678096417337656,
              -0.008531966246664524,
              0.010950262658298016,
              -0.009170505218207836,
              -0.0037667001597583294,
              0.02168315090239048,
              0.002808891935274005,
              -0.010705715976655483,
              0.0050437781028449535,
              0.016411809250712395,
              -0.01331421546638012,
              -0.013090047053992748,
              -0.021139713004231453,
              -0.015542308799922466,
              0.005030191969126463,
              0.010006040334701538,
              0.020637033507227898,
              -0.013769344426691532,
              -0.02226734533905983,
              0.03228018060326576,
              -0.002003925386816263,
              0.0007862859056331217,
              -0.004143709782510996,
              0.0354321151971817,
              0.011446149088442326,
              -0.009041438810527325,
              0.005003020167350769,
              -0.000519237422849983,
              -0.0013653864152729511,
              -0.0029005969408899546,
              0.003051740350201726,
              0.006072912365198135,
              0.009428638033568859,
              0.019061066210269928,
              -0.0006410862552002072,
              0.04154578596353531,
              0.021166885271668434,
              -0.008708584122359753,
              -0.019781120121479034,
              0.010637786239385605,
              0.009795458056032658,
              -0.020759306848049164,
              -0.012302063405513763,
              -0.008008908480405807,
              0.029698852449655533,
              -0.032089974731206894,
              0.022905884310603142,
              0.02465846948325634,
              0.037306975573301315,
              -0.0007421316695399582,
              -0.01732206530869007,
              0.00954411830753088,
              0.0015487964265048504,
              0.03279644623398781,
              -0.012994945980608463,
              -0.017362823709845543,
              0.009292778559029102,
              -0.005828365683555603,
              0.023367807269096375,
              -0.0037395283579826355,
              0.02627519704401493,
              -0.038692738860845566,
              0.04312175512313843,
              0.02268850989639759,
              0.014102199114859104,
              0.008919165469706059,
              -0.00975470058619976,
              -0.013558762148022652,
              0.004568270407617092,
              -0.02624802477657795,
              -0.02490301802754402,
              0.016941659152507782,
              0.01709110476076603,
              0.015555894933640957,
              0.03448110073804855,
              0.002479432849213481,
              -0.019047480076551437,
              -0.027538688853383064,
              -0.02209072932600975,
              -0.018585557118058205,
              -0.04298589378595352,
              -0.0010792326647788286,
              -0.020025666803121567,
              0.0010299836285412312,
              -0.03521474078297615,
              0.01729489490389824,
              -0.032334521412849426,
              -0.006168013904243708,
              -0.013490832410752773,
              0.02327270433306694,
              0.010732888244092464,
              0.004731301683932543,
              0.018177980557084084,
              0.010848368518054485,
              -0.010026419535279274,
              -0.0349973663687706,
              0.056626174598932266,
              0.017580198124051094,
              -0.0033455363009124994,
              -0.014292402192950249,
              0.004659975413233042,
              0.0033828974701464176,
              0.0019954340532422066,
              0.012383579276502132,
              -0.01074647344648838,
              0.0038482157979160547,
              -0.01532493345439434,
              0.013354972936213017,
              -0.016221605241298676,
              0.0037157528568059206,
              0.0009289382142014802,
              0.011955621652305126,
              0.012369993142783642,
              0.03024228848516941,
              -0.018001362681388855,
              0.003722545923665166,
              -0.019414300099015236,
              -0.0002935835509561002,
              0.030079258605837822,
              0.0014358634361997247,
              -0.01493094116449356,
              0.003742924891412258,
              0.007234510034322739,
              -0.02310967445373535,
              0.0024318823125213385,
              -0.004102952312678099,
              -0.016004230827093124,
              0.01123556774109602,
              -0.02107178419828415,
              -0.03575817868113518,
              0.013993511907756329,
              0.009965282864868641,
              -0.004975848365575075,
              -0.017226964235305786,
              0.012118653394281864,
              0.00012758040975313634,
              0.004038419108837843,
              0.005233981180936098,
              -0.01830025389790535,
              -0.011758625507354736,
              -0.009584876708686352,
              0.013205528259277344,
              -0.015868371352553368,
              -0.006881275679916143,
              -0.008776513859629631,
              -0.01811004988849163,
              -0.028394602239131927,
              -0.0058079869486391544,
              -0.03524191305041313,
              0.020039252936840057,
              0.010603821836411953,
              -0.008389314636588097,
              -0.022376032546162605,
              0.017362823709845543,
              0.009109368547797203,
              -0.020840823650360107,
              -0.005729867611080408,
              0.0032860978972166777,
              -0.023924829438328743,
              -0.001446052803657949,
              -0.004181071184575558,
              -0.17389994859695435,
              -0.011317082680761814,
              0.016778629273176193,
              -0.040241535753011703,
              0.008898787200450897,
              -0.0012066008057445288,
              0.01709110476076603,
              -0.02171032316982746,
              -0.038502536714076996,
              0.012981359846889973,
              0.02085440792143345,
              0.00885123573243618,
              -0.027008837088942528,
              -0.017063932493329048,
              -0.017036762088537216,
              0.004619217477738857,
              0.0025337766855955124,
              0.021737493574619293,
              0.0030670245178043842,
              0.0015411543427035213,
              0.042034879326820374,
              -0.0326605848968029,
              0.017661714926362038,
              -0.014075027778744698,
              0.00945581030100584,
              -0.0028615372721105814,
              -0.002742660464718938,
              0.009985661134123802,
              -0.0026662396267056465,
              -0.03722545877099037,
              0.008599895983934402,
              0.006239340174943209,
              0.025691000744700432,
              -0.009184091351926327,
              0.0023147035390138626,
              0.017199791967868805,
              0.016438979655504227,
              -0.006888068746775389,
              -0.009347123093903065,
              0.0025949133560061455,
              0.031111789867281914,
              0.01929202675819397,
              0.016547666862607002,
              -0.01400709804147482,
              -0.019061066210269928,
              0.020786479115486145,
              0.020134354010224342,
              -0.014387504197657108,
              0.021723909303545952,
              -0.019726775586605072,
              0.00318420329131186,
              -0.00817193929105997,
              -0.0016048384131863713,
              0.013151183724403381,
              0.023965587839484215,
              0.009190884418785572,
              -0.0033795011695474386,
              0.01666994020342827,
              -0.014795082621276379,
              0.004391652997583151,
              -0.00019243202405050397,
              -0.024617712944746017,
              0.011330668814480305,
              -0.017145449295639992,
              -0.02725338377058506,
              -0.01262133289128542,
              -0.017756815999746323,
              0.0047245086170732975,
              0.0006156126619316638,
              0.011004606261849403,
              -0.015447206795215607,
              -0.013844067230820656,
              -0.0068608964793384075,
              0.00015676894690841436,
              0.010474755428731441,
              0.032905131578445435,
              -0.009238434955477715,
              -0.005444562993943691,
              0.019821878522634506,
              0.009978868998587132,
              -0.013185149058699608,
              0.032714929431676865,
              -0.018599143251776695,
              0.018965963274240494,
              -0.006385388784110546,
              0.0009892258094623685,
              0.0009255417389795184,
              0.018558386713266373,
              -0.010624200105667114,
              -0.01869424618780613,
              0.005186430178582668,
              -0.03024228848516941,
              0.013205528259277344,
              0.0012210358399897814,
              0.0019852446857839823,
              0.011371427215635777,
              0.02310967445373535,
              0.0039161453023552895,
              0.004201449919492006,
              -0.015474379062652588,
              0.0053902193903923035,
              0.003841422963887453,
              0.009319950826466084,
              0.029399961233139038,
              0.011588801629841328,
              0.027959851548075676,
              -0.024413922801613808,
              0.013816894963383675,
              0.029726022854447365,
              0.0038244405295699835,
              -0.02429164946079254,
              -0.016642769798636436,
              0.015053214505314827,
              0.020650619640946388,
              0.013796515762805939,
              0.015352105721831322,
              -0.0035459287464618683,
              -0.017947018146514893,
              0.011208395473659039,
              -0.014061441645026207,
              0.03415503725409508,
              0.0034763009753078222,
              -0.01793343387544155,
              0.02168315090239048,
              -0.02207714319229126,
              -0.04717036336660385,
              -0.09417769312858582,
              -0.038094960153102875,
              -0.019658846780657768,
              0.0047143190167844296,
              -0.003844819264486432,
              0.016574839130043983,
              0.02688656374812126,
              0.018830103799700737,
              0.00039420436951331794,
              0.03282361850142479,
              -0.019047480076551437,
              -0.013558762148022652,
              0.002316401805728674,
              -0.02687297761440277,
              0.008104009553790092,
              0.0024607523810118437,
              0.0016260665142908692,
              0.004710922483354807,
              -0.017960604280233383,
              0.01969960518181324,
              0.0030789123848080635,
              -0.030785726383328438,
              0.032714929431676865,
              -0.03184542804956436,
              0.031138960272073746,
              -0.026207266375422478,
              -0.031763914972543716,
              0.008307798765599728,
              -0.0024641486816108227,
              -0.016778629273176193,
              -0.012336027808487415,
              -0.02926410175859928,
              9.860416321316734e-05,
              -0.014944527298212051,
              0.0032657189294695854,
              -0.008756134659051895,
              -0.016194432973861694,
              -0.027348484843969345,
              -0.003950110170990229,
              -0.029970571398735046,
              0.0020327954553067684,
              0.0007306685438379645,
              0.0004997076466679573,
              -0.01592271402478218,
              -0.012132239528000355,
              -0.04002416133880615,
              -0.010963848792016506,
              0.014387504197657108,
              0.011317082680761814,
              -0.007295646704733372,
              -0.014876597560942173,
              -0.008919165469706059,
              -0.03399200737476349,
              -0.007064685691148043,
              0.01313080545514822,
              0.012852293439209461,
              0.0032708137296140194,
              0.004452789667993784,
              -0.011147258803248405,
              -0.02171032316982746,
              0.007954563945531845,
              -0.007397541310638189,
              -0.01892520673573017,
              0.0072888536378741264,
              0.0010936676990240812,
              -0.004371274262666702,
              -0.00934033002704382,
              -0.025568727403879166,
              0.012600953690707684,
              -0.0306226946413517,
              0.0029464494436979294,
              0.014469020068645477,
              0.0035798936150968075,
              0.019604502245783806,
              -0.021207643672823906,
              -0.010705715976655483,
              -0.015515136532485485,
              0.021574463695287704,
              0.005845348350703716,
              -0.01609933190047741,
              -0.03228018060326576,
              -0.022729268297553062,
              0.01729489490389824,
              -0.025025291368365288,
              -0.006803156342357397,
              -0.00766246672719717,
              -0.024644885212183,
              -0.00037127811810933053,
              -0.0038040615618228912,
              -0.029698852449655533,
              0.012668883427977562,
              0.03045966476202011,
              -0.0013450074475258589,
              -0.013191942125558853,
              -0.006235943641513586,
              0.0068608964793384075,
              -0.025758931413292885,
              0.004374670796096325,
              0.006096688099205494,
              0.045458536595106125,
              -0.01364027801901102,
              -0.00641595758497715,
              -0.052822113037109375,
              0.008803685195744038,
              0.0031909963581711054,
              -0.022946642711758614,
              0.009218056686222553,
              0.023775383830070496,
              0.037497177720069885,
              0.009421844966709614,
              -0.013470453210175037,
              0.030731383711099625,
              -0.01073968131095171,
              0.02168315090239048,
              0.013049289584159851,
              0.0001267312909476459,
              -0.012994945980608463,
              -0.024821501225233078,
              0.02385690063238144,
              -0.021968455985188484,
              0.015814026817679405,
              0.024726400151848793,
              -0.003197789192199707,
              -0.011378219351172447,
              0.012859086506068707,
              0.01253981702029705,
              -0.0019173149485141039,
              -0.0010130011942237616,
              -0.015433620661497116,
              0.014686394482851028,
              -0.01733565144240856,
              -0.02268850989639759,
              -0.002095630392432213,
              -0.019590916112065315,
              0.006246133241802454,
              0.00995169673115015,
              -0.010889125987887383,
              -0.002598309889435768,
              0.0007387351361103356,
              0.008830857463181019,
              0.02268850989639759,
              0.009462603367865086,
              -0.01194203644990921,
              -0.026112165302038193,
              0.02882935293018818,
              -0.025976305827498436,
              0.0027494532987475395,
              -0.0033964836038649082,
              0.004632803611457348,
              -0.002212808933109045,
              -0.003817647462710738,
              -0.006483886856585741,
              0.024155789986252785,
              0.02108537033200264,
              0.009435431100428104,
              -0.022973814979195595,
              -0.01631670631468296,
              0.005447959527373314,
              0.013551969081163406,
              0.026152923703193665,
              -0.026913736015558243,
              -0.009809044189751148,
              0.017607370391488075,
              0.007825498469173908,
              0.002411503344774246,
              -0.00325383129529655,
              0.0016421998152509332,
              0.012859086506068707,
              0.019237682223320007,
              -0.0017186206532642245,
              -0.009197677485644817,
              -0.034535445272922516,
              -0.0014018985675647855,
              -0.0013008532114326954,
              0.012071102857589722,
              0.011575215496122837,
              0.03564948961138725,
              0.006229150574654341,
              0.00017884607950691134,
              0.00716658029705286,
              -0.01463205087929964,
              0.03662768006324768,
              0.015596652403473854,
              -0.028965210542082787,
              -0.01342969574034214,
              -0.017770402133464813,
              0.004829799756407738,
              -0.0027358673978596926,
              0.006796363741159439,
              -0.0024556575808674097,
              -0.01402068417519331,
              -0.0037870791275054216,
              0.005712885409593582,
              0.007988529279828072,
              0.002396219177171588,
              0.0053800297901034355,
              -0.004089366178959608,
              0.009809044189751148,
              0.010984227992594242,
              0.00021461529831867665,
              0.012607746757566929,
              -0.013592727482318878,
              0.001277926960028708,
              0.0013322706799954176,
              -0.004595442209392786,
              -0.005634766072034836,
              -0.014536949805915356,
              0.020990267395973206,
              -0.019020307809114456,
              -0.028313087299466133,
              0.00040587977855466306,
              0.0034151640720665455,
              0.010298137553036213,
              0.020243041217327118,
              -0.015257003717124462,
              0.0015292667085304856,
              -0.02861197665333748,
              0.03423655405640602,
              0.013735379092395306,
              -0.017457924783229828,
              -0.008498001843690872,
              0.007995322346687317,
              0.007085064426064491,
              -0.00015952858666423708,
              0.028557633981108665,
              0.0013874635333195329,
              0.01592271402478218,
              0.021642392501235008,
              0.04121972247958183,
              -0.010481548495590687,
              0.00894633773714304,
              0.008287419565021992,
              -0.013253078795969486,
              -0.017145449295639992,
              -0.005529474932700396,
              -0.019020307809114456,
              -0.00023456964117940515,
              0.0033217607997357845,
              0.0027137903962284327,
              0.022172244265675545,
              -0.0034253536723554134,
              0.0892324149608612,
              0.02586761862039566,
              0.004996227100491524,
              0.0027341691311448812,
              -0.0010401731124147773,
              0.025813274085521698,
              0.019210509955883026,
              0.014346746727824211,
              -0.0023418753407895565,
              0.00481961015611887,
              0.02449543960392475,
              0.01633029244840145,
              0.006545023526996374,
              -0.012492266483604908,
              -0.014795082621276379,
              0.006239340174943209,
              0.011595594696700573,
              -0.0030924982856959105,
              0.01787908934056759,
              -0.013103633187711239,
              0.046056315302848816,
              -0.026492571458220482,
              0.007458677981048822,
              0.011955621652305126,
              -0.03461695834994316,
              -0.006049137096852064,
              0.010760059580206871,
              0.015664581209421158,
              -0.018667073920369148,
              -0.039073146879673004,
              0.019658846780657768,
              -0.002095630392432213,
              -0.028150055557489395,
              -0.012845500372350216,
              0.022213002666831017,
              -0.002409805078059435,
              -0.001168390386737883,
              -0.005818176083266735,
              0.018245909363031387,
              0.010617407038807869,
              0.01501245703548193,
              0.01690090261399746,
              -0.004459582734853029,
              -0.03497019410133362,
              -0.01194203644990921,
              -0.0059676216915249825,
              -0.02008001133799553,
              -0.027348484843969345,
              -0.016588425263762474
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "What is the purpose of BERT?"
          },
          "output": {
            "jsonAnswer": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            },
            "existingAnswer": true
          },
          "start_time": 1694832910.628216,
          "end_time": 1694832912.441883,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        },
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.022036384791135788,
              -0.0164797380566597,
              0.0005999039276503026,
              -0.007227716967463493,
              0.003868594765663147,
              0.01381010189652443,
              -0.006500869523733854,
              -0.011711074970662594,
              -0.01653408259153366,
              -0.015121144242584705,
              0.0027137903962284327,
              0.027362070977687836,
              -0.025758931413292885,
              0.00944222416728735,
              3.94045164284762e-05,
              0.0016226699808612466,
              0.016561252996325493,
              0.015487965196371078,
              0.004052004776895046,
              0.0009892258094623685,
              -0.005791004281491041,
              -0.014170128852128983,
              -0.019210509955883026,
              0.020990267395973206,
              -0.006324252113699913,
              -0.02627519704401493,
              0.05154503509402275,
              -0.033502914011478424,
              0.00033030801569111645,
              -0.013558762148022652,
              0.027144696563482285,
              0.030269460752606392,
              -0.010019626468420029,
              -0.008131181821227074,
              -0.013069668784737587,
              0.0003339167742524296,
              0.01770247146487236,
              -0.002297721104696393,
              0.017009589821100235,
              -0.01224092673510313,
              0.015800440683960915,
              0.01892520673573017,
              0.013878031633794308,
              -0.001744094304740429,
              -0.0005765530513599515,
              0.003309873165562749,
              0.01252623088657856,
              -0.004153899382799864,
              0.0005892898770980537,
              -0.023788969963788986,
              0.018653487786650658,
              0.01949581503868103,
              -0.02384331449866295,
              -0.002343573607504368,
              -0.03442675620317459,
              -0.0030534386169165373,
              -0.0077575682662427425,
              0.02582686021924019,
              0.009068611077964306,
              -0.02248472161591053,
              0.010753266513347626,
              0.0006746265571564436,
              -0.015664581209421158,
              0.019156167283654213,
              0.0048841433599591255,
              -0.009299571625888348,
              -0.022946642711758614,
              0.029101070016622543,
              0.010603821836411953,
              0.008885201066732407,
              0.007628501858562231,
              0.020623447373509407,
              0.019455058500170708,
              -0.011086122132837772,
              0.03654616326093674,
              -0.002844554837793112,
              -0.01502604316920042,
              -0.005852140951901674,
              0.008036079816520214,
              0.0004491849394980818,
              0.006405767984688282,
              0.02306891605257988,
              -0.02109895646572113,
              0.020922338590025902,
              0.013062875717878342,
              -0.007594536989927292,
              0.013103633187711239,
              0.01690090261399746,
              -0.019577331840991974,
              -0.0003455921832937747,
              0.006201978772878647,
              0.002939656376838684,
              0.03314967826008797,
              0.004282965790480375,
              0.000431353400927037,
              0.03024228848516941,
              -0.004602235276252031,
              0.029101070016622543,
              0.0016498418990522623,
              -0.035106055438518524,
              0.009027853608131409,
              -0.0075266072526574135,
              -0.007940978743135929,
              -0.01224092673510313,
              -0.019224096089601517,
              0.0021533705294132233,
              0.00983621645718813,
              -0.010372860357165337,
              0.028150055557489395,
              -0.008076838217675686,
              -0.007716810330748558,
              0.03203563392162323,
              -0.0034202588722109795,
              -0.020215868949890137,
              0.005529474932700396,
              0.021533705294132233,
              0.0010299836285412312,
              0.0005413145408965647,
              0.017457924783229828,
              0.020732134580612183,
              0.03328553959727287,
              -0.002212808933109045,
              0.0011565026361495256,
              -0.01827308163046837,
              0.022348862141370773,
              -0.008137974888086319,
              -0.021615220233798027,
              -0.019998494535684586,
              0.008701791055500507,
              -0.029481476172804832,
              0.006055930163711309,
              -0.0036444268189370632,
              0.0013798214495182037,
              -0.023816142231225967,
              -0.002878519706428051,
              0.023191189393401146,
              -0.00796135701239109,
              -0.010107934474945068,
              -0.026424642652273178,
              -0.01768888533115387,
              0.016819385811686516,
              0.022593408823013306,
              -0.023177603259682655,
              -0.03518756851553917,
              -0.026601258665323257,
              0.02546004019677639,
              0.0007625105208717287,
              0.02842177450656891,
              0.010114727541804314,
              0.002683222061023116,
              0.0042456043884158134,
              -0.031138960272073746,
              -0.018055707216262817,
              0.009591669775545597,
              0.0022875317372381687,
              0.013979925774037838,
              -0.00032075541093945503,
              0.0026458606589585543,
              -0.000471262086648494,
              -0.00011494974023662508,
              0.0030279650818556547,
              0.014550535008311272,
              0.011554837226867676,
              -0.009598462842404842,
              0.03760586678981781,
              0.04051325470209122,
              -0.004279569257050753,
              -0.0026611448265612125,
              0.012397164478898048,
              0.003573100548237562,
              -0.008688204921782017,
              0.01570533961057663,
              -0.026981664821505547,
              0.004629407078027725,
              0.015433620661497116,
              0.00746547058224678,
              0.017430752515792847,
              0.007805119268596172,
              0.0007680298294872046,
              -0.013361766003072262,
              -0.0051320865750312805,
              0.006398974917829037,
              0.03399200737476349,
              0.005003020167350769,
              -0.015257003717124462,
              -0.002204317832365632,
              0.003770096693187952,
              -0.035350602120161057,
              0.0028004006016999483,
              -0.044643379747867584,
              0.018205150961875916,
              0.019061066210269928,
              0.011711074970662594,
              -0.004673561546951532,
              -0.6364738345146179,
              -0.014455433934926987,
              0.016629183664917946,
              -0.00835534930229187,
              0.021329917013645172,
              -0.013083254918456078,
              -0.010352482087910175,
              -0.00566193787381053,
              -0.01171786803752184,
              0.03263341262936592,
              0.005447959527373314,
              -0.012607746757566929,
              -0.005906485021114349,
              -0.01502604316920042,
              -0.0013458565808832645,
              -0.022511892020702362,
              -0.00936070829629898,
              -0.00402143644168973,
              0.01869424618780613,
              -0.003973885904997587,
              -0.02446826733648777,
              0.02124840021133423,
              -0.04380105063319206,
              0.005852140951901674,
              -0.01752585545182228,
              0.007132615428417921,
              0.002350366674363613,
              -0.009313157759606838,
              0.007295646704733372,
              -0.007268474902957678,
              -0.01323269959539175,
              0.03336705267429352,
              0.03956224024295807,
              -0.015284175984561443,
              0.031709570437669754,
              0.011860520578920841,
              -0.002382633276283741,
              0.015759684145450592,
              -0.031193304806947708,
              0.0256638303399086,
              -0.00866782572120428,
              -0.019618088379502296,
              0.02329987660050392,
              -0.007010342087596655,
              0.03217149153351784,
              0.007248095702379942,
              0.01232923474162817,
              0.021425018087029457,
              0.0018884448800235987,
              0.007431505713611841,
              -0.027606617659330368,
              0.008722169324755669,
              0.0014333160361275077,
              -0.008464036509394646,
              0.014401090331375599,
              -0.0018341010436415672,
              0.022837955504655838,
              -0.014061441645026207,
              0.0002553731028456241,
              -0.022606994956731796,
              0.011643145233392715,
              0.034345243126153946,
              -0.010902712121605873,
              -0.02567741461098194,
              -0.011004606261849403,
              0.015229832381010056,
              0.0035323428455740213,
              0.012770777568221092,
              0.011887691915035248,
              -0.04477923735976219,
              0.010481548495590687,
              0.024984532967209816,
              0.0006393880466930568,
              -0.0006792966742068529,
              0.02008001133799553,
              0.017743229866027832,
              -0.00228923000395298,
              -0.015121144242584705,
              -0.0004844234499614686,
              0.022973814979195595,
              -0.009252021089196205,
              -0.005587215535342693,
              0.012607746757566929,
              -0.0028190813027322292,
              -0.018748588860034943,
              0.017797574400901794,
              -0.006918637081980705,
              -0.02309608832001686,
              0.005352857988327742,
              0.026207266375422478,
              -0.0012125446228310466,
              0.02782399207353592,
              -0.021601635962724686,
              -0.03942637890577316,
              0.02248472161591053,
              0.012369993142783642,
              0.003501774510368705,
              -0.016642769798636436,
              0.03247038275003433,
              -0.007241302635520697,
              0.002798702334985137,
              -0.017607370391488075,
              0.020243041217327118,
              0.023761799558997154,
              0.00856593158096075,
              0.016438979655504227,
              0.02270209603011608,
              0.020637033507227898,
              0.002071854891255498,
              -0.017199791967868805,
              0.004792438354343176,
              -0.016357464715838432,
              0.00825345516204834,
              0.008776513859629631,
              -0.006633332464843988,
              -0.0329594761133194,
              0.014183714985847473,
              0.0027817199006676674,
              0.012220547534525394,
              -0.021234814077615738,
              0.0019427885999903083,
              -0.009598462842404842,
              0.012444715946912766,
              -0.016642769798636436,
              0.019210509955883026,
              0.017675301060080528,
              -0.0031536349561065435,
              0.004310137592256069,
              -0.020650619640946388,
              -0.005298514384776354,
              -0.021832596510648727,
              -0.006426146719604731,
              0.0053800297901034355,
              -0.012152617797255516,
              0.04393691197037697,
              0.03899163007736206,
              0.03260624036192894,
              -0.0015937999123707414,
              -0.02050117403268814,
              0.005006416700780392,
              -0.012315649539232254,
              -0.010495133697986603,
              0.01567816734313965,
              -0.005740057211369276,
              -0.052034128457307816,
              -0.016167260706424713,
              0.008891994133591652,
              0.003070421051234007,
              0.001690599718131125,
              -0.011554837226867676,
              0.0015428526094183326,
              -0.02624802477657795,
              -0.018585557118058205,
              -0.005899691954255104,
              -0.013090047053992748,
              -0.028557633981108665,
              -0.009408259764313698,
              -0.04668127000331879,
              -0.019536573439836502,
              -0.031709570437669754,
              0.010325309820473194,
              0.00373273529112339,
              -0.011989586986601353,
              -0.012770777568221092,
              0.0011378219351172447,
              -0.03497019410133362,
              0.005515889264643192,
              0.022213002666831017,
              -0.027525102719664574,
              -0.033910490572452545,
              0.028938040137290955,
              -0.01846328377723694,
              -0.01033210288733244,
              0.005774022080004215,
              -0.026682773604989052,
              0.013776137493550777,
              -0.03325836732983589,
              -0.016017816960811615,
              0.0003791324852500111,
              -0.016044987365603447,
              0.005030191969126463,
              0.030704211443662643,
              -0.010012833401560783,
              -0.009224848821759224,
              0.012220547534525394,
              0.0020344937220215797,
              0.017172621563076973,
              -0.00310948072001338,
              -0.02488943189382553,
              0.00974111445248127,
              0.00874934159219265,
              0.026587672531604767,
              -0.003061929950490594,
              0.02090875245630741,
              0.012573782354593277,
              0.006110273767262697,
              0.016357464715838432,
              -0.009122954681515694,
              0.0008754436275921762,
              -0.005172844510525465,
              0.025297008454799652,
              0.010732888244092464,
              0.00034325712476857007,
              -0.012940602377057076,
              -0.008294212631881237,
              0.017457924783229828,
              -8.363840606762096e-05,
              -0.016438979655504227,
              0.008219489827752113,
              -0.00042880602995865047,
              0.01461846474558115,
              -0.029508648440241814,
              -0.02325911819934845,
              -0.0005476829828694463,
              -0.008708584122359753,
              0.021044611930847168,
              -0.009068611077964306,
              0.02289229817688465,
              -0.034263726323843,
              0.0033591222018003464,
              0.003783682594075799,
              -0.01793343387544155,
              0.004975848365575075,
              0.0009900749428197742,
              -0.011765418574213982,
              0.006297080311924219,
              -0.008097216486930847,
              -0.012702848762273788,
              0.012410750612616539,
              -0.022362448275089264,
              0.0005455601494759321,
              0.028285915032029152,
              0.02999774180352688,
              0.020025666803121567,
              -0.0012032042723149061,
              -0.00856593158096075,
              0.014469020068645477,
              -0.010196243412792683,
              0.025595899671316147,
              -0.017159035429358482,
              0.0026017064228653908,
              0.01154804416000843,
              0.013110426254570484,
              -0.0015776666114106774,
              0.03545928746461868,
              0.015814026817679405,
              0.03244321048259735,
              0.010651372373104095,
              -0.016072159633040428,
              0.02287871204316616,
              -0.01431957446038723,
              -0.008878407999873161,
              -0.030731383711099625,
              0.0011514079524204135,
              0.0240606889128685,
              -0.00924522802233696,
              0.0040757800452411175,
              0.012899843975901604,
              -0.0057094888761639595,
              0.033883318305015564,
              0.016357464715838432,
              0.008362142369151115,
              0.022797197103500366,
              0.007805119268596172,
              0.019183339551091194,
              -0.014455433934926987,
              -0.014591293409466743,
              -0.0058079869486391544,
              -0.009184091351926327,
              -0.004938486963510513,
              -0.017254136502742767,
              -0.015080386772751808,
              0.0010206432780250907,
              -0.010617407038807869,
              0.01441467646509409,
              0.03024228848516941,
              -0.02490301802754402,
              0.021438604220747948,
              -0.0067929672077298164,
              0.034127864986658096,
              -0.0023045141715556383,
              -0.029916226863861084,
              0.03160088136792183,
              -0.006130652967840433,
              -0.027959851548075676,
              -0.0171318631619215,
              0.0032283575274050236,
              -0.02169673703610897,
              -0.004269379656761885,
              -0.018830103799700737,
              0.0016956944018602371,
              0.009007474407553673,
              -0.0019852446857839823,
              0.025120392441749573,
              0.00111829221714288,
              0.013293836265802383,
              0.007302439771592617,
              -0.01986263506114483,
              0.004432410933077335,
              0.004486754536628723,
              0.0019190132152289152,
              0.00785946287214756,
              -0.006490679923444986,
              -0.028965210542082787,
              0.02029738575220108,
              0.001125934300944209,
              -0.029209759086370468,
              -0.016642769798636436,
              -0.016466151922941208,
              -0.015977058559656143,
              0.038257990032434464,
              0.009462603367865086,
              0.01868066005408764,
              -0.019346369430422783,
              -5.301698547555134e-05,
              0.0292369294911623,
              0.007893427275121212,
              -0.006996755953878164,
              0.008511587977409363,
              0.004408635664731264,
              -0.028313087299466133,
              -0.012268098071217537,
              -0.01671069860458374,
              0.008436865173280239,
              0.04317609965801239,
              0.04755076766014099,
              -0.004174278117716312,
              0.019020307809114456,
              -0.012757192365825176,
              0.03635596111416817,
              -0.025758931413292885,
              -0.0389372855424881,
              -0.021194057539105415,
              -0.01649332419037819,
              0.0015802140114828944,
              -0.02664201706647873,
              0.006531437858939171,
              -0.022810783237218857,
              0.005254359915852547,
              0.01025058701634407,
              -0.0003099291061516851,
              -0.02626161091029644,
              0.033747460693120956,
              -0.016547666862607002,
              -0.0009994152933359146,
              0.013701414689421654,
              0.006076308898627758,
              0.010467962361872196,
              0.001453694887459278,
              -0.019427886232733727,
              -0.012254512868821621,
              0.010807610116899014,
              -0.0035323428455740213,
              -0.011588801629841328,
              -0.016561252996325493,
              -0.022579822689294815,
              -0.01753944158554077,
              0.00787304900586605,
              -0.02051476016640663,
              0.03939921036362648,
              0.001176881487481296,
              0.006545023526996374,
              0.011004606261849403,
              0.020025666803121567,
              0.0029600353445857763,
              0.02662843093276024,
              -0.002523587318137288,
              -0.01540644932538271,
              7.790684321662411e-05,
              -0.030378147959709167,
              0.004031626041978598,
              0.016574839130043983,
              0.011317082680761814,
              -0.002888709306716919,
              0.0012337726075202227,
              -0.004809420555830002,
              -0.04111103713512421,
              -0.005013209767639637,
              0.014469020068645477,
              -0.014699980616569519,
              0.012471887283027172,
              0.010196243412792683,
              -0.01827308163046837,
              -0.02385690063238144,
              -0.017403582111001015,
              -0.014047855511307716,
              0.01872141659259796,
              -0.007275267504155636,
              0.00365801271982491,
              0.004639596678316593,
              -0.017362823709845543,
              -0.0244275089353323,
              -0.015596652403473854,
              0.04627368971705437,
              -0.008212696760892868,
              -0.04032305255532265,
              -0.04727904871106148,
              -0.016765043139457703,
              0.01221375446766615,
              0.011486907489597797,
              0.03260624036192894,
              -0.020786479115486145,
              -0.001669371617026627,
              -0.0023147035390138626,
              -0.007574158255010843,
              -0.03445392847061157,
              0.0223081037402153,
              -0.04051325470209122,
              0.0038006650283932686,
              -0.01102498546242714,
              -0.00746547058224678,
              -0.027565861120820045,
              -0.019332783296704292,
              0.0354321151971817,
              0.002744358731433749,
              0.005644955672323704,
              0.02961733564734459,
              -0.028965210542082787,
              0.01846328377723694,
              0.019984908401966095,
              0.0072005451656877995,
              0.005634766072034836,
              0.012886258773505688,
              -0.028313087299466133,
              -0.011955621652305126,
              -0.009285985492169857,
              -0.010420411825180054,
              -0.03325836732983589,
              0.03165522590279579,
              -0.006307269912213087,
              0.0036274443846195936,
              -0.021126126870512962,
              0.01733565144240856,
              -0.01072609517723322,
              0.008925958536565304,
              -0.023394977673888206,
              -0.00310948072001338,
              -0.01711827702820301,
              0.0313563346862793,
              -0.0038855771999806166,
              -0.001333968946710229,
              0.024970946833491325,
              -0.012268098071217537,
              -0.0019478832837194204,
              0.0025490608531981707,
              -0.02308250218629837,
              0.016588425263762474,
              0.019006721675395966,
              -0.0026017064228653908,
              0.021520119160413742,
              0.005498906597495079,
              -0.01688731648027897,
              -0.007037513889372349,
              0.025609485805034637,
              -0.024509025737643242,
              0.01668352633714676,
              -0.0017101294361054897,
              -0.0278511643409729,
              -0.03143785148859024,
              -0.016832971945405006,
              -0.002584723988547921,
              0.008932751603424549,
              -0.01461846474558115,
              -0.0012813233770430088,
              0.012179790064692497,
              -0.0006381143466569483,
              0.009992454200983047,
              -0.013225906528532505,
              0.01889803446829319,
              -0.022960228845477104,
              -0.014808667823672295,
              0.008531966246664524,
              0.001946185017004609,
              0.03206280618906021,
              -0.017634542658925056,
              0.02999774180352688,
              -0.030079258605837822,
              -0.0076692597940564156,
              0.0005748548428528011,
              -0.02983471192419529,
              -0.0004895181627944112,
              0.004462979268282652,
              0.03673636540770531,
              0.02324553392827511,
              0.03184542804956436,
              -0.000315448414767161,
              0.01082798931747675,
              0.01501245703548193,
              -0.0043169306591153145,
              -0.006378596182912588,
              -0.028394602239131927,
              -0.003245339961722493,
              -0.022117899730801582,
              0.017172621563076973,
              -0.001829006359912455,
              -0.013633484952151775,
              -0.008260248228907585,
              0.0026662396267056465,
              0.003905955934897065,
              0.002542268019169569,
              0.0028343654703348875,
              0.006945808883756399,
              0.0010180958779528737,
              -0.005757039412856102,
              -0.03635596111416817,
              0.02325911819934845,
              -0.004198053851723671,
              -0.0004954620380885899,
              -0.042279426008462906,
              -0.025595899671316147,
              0.013484039343893528,
              0.011065742932260036,
              0.014903769828379154,
              0.012397164478898048,
              0.013721792958676815,
              -0.015243417583405972,
              0.0006584932561963797,
              -0.008531966246664524,
              -0.003461016807705164,
              -0.001688052318058908,
              -0.014890183694660664,
              -0.029454305768013,
              -0.016846558079123497,
              0.013966340571641922,
              0.028557633981108665,
              -0.021139713004231453,
              -0.003255529562011361,
              -0.010603821836411953,
              0.01628953404724598,
              0.008321384899318218,
              0.01709110476076603,
              -0.008606689050793648,
              0.004378067329525948,
              -0.0013467057142406702,
              -0.009768286719918251,
              -0.03638312965631485,
              -0.028693493455648422,
              -0.0037633036263287067,
              0.036301616579294205,
              0.0034525254741311073,
              -0.02328629046678543,
              0.026519743725657463,
              0.011969207786023617,
              0.0026203871238976717,
              -0.01569175347685814,
              0.004496944136917591,
              0.05230584740638733,
              0.02051476016640663,
              0.011887691915035248,
              0.029481476172804832,
              -0.0005655144923366606,
              0.0016761645674705505,
              0.01870783045887947,
              -0.0005846197018399835,
              -0.014346746727824211,
              -0.004975848365575075,
              -0.007594536989927292,
              0.006755605805665255,
              -0.008321384899318218,
              0.02429164946079254,
              0.02844894677400589,
              -0.018802933394908905,
              -0.01164993830025196,
              0.0006028758361935616,
              -0.00596422515809536,
              0.012471887283027172,
              0.013572348281741142,
              -0.01690090261399746,
              -0.00231130700558424,
              0.012397164478898048,
              -0.026981664821505547,
              0.016602011397480965,
              -0.006602764129638672,
              -0.005070949904620647,
              0.012132239528000355,
              0.006830328144133091,
              -0.012927016243338585,
              0.014428261667490005,
              0.020392486825585365,
              0.0012550007086247206,
              0.001064797630533576,
              0.02070496417582035,
              -0.015841199085116386,
              -0.027389243245124817,
              0.02861197665333748,
              0.030514007434248924,
              -0.014958113431930542,
              0.015134730376303196,
              0.008973509073257446,
              0.014958113431930542,
              0.01589554361999035,
              0.012763985432684422,
              -0.00374971772544086,
              0.027008837088942528,
              -0.035731006413698196,
              -0.008593102917075157,
              -0.013253078795969486,
              -0.024128619581460953,
              0.025568727403879166,
              -0.0280685406178236,
              0.003950110170990229,
              -0.0007968999561853707,
              -0.017362823709845543,
              0.0157460980117321,
              0.03200846165418625,
              0.013171562924981117,
              -0.0250796340405941,
              0.005325686186552048,
              -0.006157824769616127,
              0.01313080545514822,
              -0.009904146194458008,
              0.0034915851429104805,
              0.02168315090239048,
              -0.03043249249458313,
              -0.022158658131957054,
              0.010026419535279274,
              -0.0067623988725245,
              0.01692807301878929,
              -0.015460792928934097,
              -0.0007009492837823927,
              0.006840517744421959,
              0.028476117178797722,
              -0.04290438070893288,
              -0.027511516585946083,
              -0.005994793493300676,
              0.0066061606630682945,
              -0.01849045604467392,
              -0.005111707840114832,
              -0.006235943641513586,
              -0.01806929148733616,
              0.01493094116449356,
              -0.011466528289020061,
              0.005991396959871054,
              -0.011018192395567894,
              -0.003729338990524411,
              -0.009095782414078712,
              -0.0018952378304675221,
              -0.03757869452238083,
              -0.016955245286226273,
              0.0071801659651100636,
              0.016411809250712395,
              -0.006619746331125498,
              -0.009720736183226109,
              -0.010046797804534435,
              0.022593408823013306,
              -0.0049724518321454525,
              0.013667449355125427,
              -0.03423655405640602,
              -0.003525549778714776,
              0.0023265911731868982,
              0.007397541310638189,
              -0.011969207786023617,
              -0.042279426008462906,
              -0.004378067329525948,
              -0.018639901652932167,
              0.006735226605087519,
              -0.018599143251776695,
              -0.011344254948198795,
              0.006898257881402969,
              0.005896295420825481,
              -0.0025303801521658897,
              -0.013762551359832287,
              -0.00305683515034616,
              0.0001466856338083744,
              0.013803308829665184,
              -0.008348556235432625,
              0.017648128792643547,
              0.0006079705781303346,
              -0.012302063405513763,
              -0.020976681262254715,
              -0.029345616698265076,
              0.001504642190411687,
              -0.014849426224827766,
              -0.023394977673888206,
              -0.014672808349132538,
              0.031193304806947708,
              0.010685336776077747,
              -0.01571892574429512,
              -0.01691448874771595,
              -0.020840823650360107,
              -0.03855688124895096,
              -0.00805645901709795,
              -0.007723603397607803,
              0.017267722636461258,
              -0.0014180318685248494,
              0.017403582111001015,
              0.027593031525611877,
              0.02425089292228222,
              0.007105443626642227,
              -0.003943317569792271,
              0.0024964152835309505,
              0.000519237422849983,
              -0.0002857291838154197,
              -0.02801419608294964,
              -0.019061066210269928,
              -0.0009433733066543937,
              -0.047414910048246384,
              -0.000902615487575531,
              0.04279569163918495,
              -0.018667073920369148,
              0.017607370391488075,
              0.05630011111497879,
              -0.008993888273835182,
              -0.016425393521785736,
              -0.001525870175100863,
              0.015161902643740177,
              0.010196243412792683,
              0.02980753965675831,
              0.015977058559656143,
              -0.026207266375422478,
              -0.00731602543964982,
              0.026709945872426033,
              0.00895992387086153,
              -0.0329594761133194,
              -0.0018120239255949855,
              0.01653408259153366,
              -0.003362518735229969,
              0.003048344049602747,
              -0.019210509955883026,
              0.01650691032409668,
              -0.0030636282172054052,
              -0.008858028799295425,
              0.0046531823463737965,
              0.0147271528840065,
              -0.02967168018221855,
              -0.0028530461713671684,
              0.029101070016622543,
              -0.019400713965296745,
              -0.021397845819592476,
              0.022606994956731796,
              -0.01400709804147482,
              -0.0023690471425652504,
              0.016221605241298676,
              -0.014740738086402416,
              0.006843914277851582,
              -0.004707525949925184,
              0.0009951696265488863,
              -0.01847686991095543,
              -0.0004294428799767047,
              -0.029345616698265076,
              0.028122883290052414,
              -0.023150430992245674,
              0.030731383711099625,
              0.011106501333415508,
              -0.02942713350057602,
              0.006497472990304232,
              0.0005608443170785904,
              -0.020243041217327118,
              -0.014686394482851028,
              -0.014876597560942173,
              0.009299571625888348,
              -0.009007474407553673,
              0.014088613912463188,
              0.0007591140456497669,
              -0.01351121161133051,
              -0.015610238537192345,
              -0.00285983900539577,
              0.01832742430269718,
              -0.006847310811281204,
              -0.00011325149534968659,
              0.23715606331825256,
              -0.03877425566315651,
              0.010936676524579525,
              0.0009178996551781893,
              -0.019183339551091194,
              0.0034678096417337656,
              -0.008531966246664524,
              0.010950262658298016,
              -0.009170505218207836,
              -0.0037667001597583294,
              0.02168315090239048,
              0.002808891935274005,
              -0.010705715976655483,
              0.0050437781028449535,
              0.016411809250712395,
              -0.01331421546638012,
              -0.013090047053992748,
              -0.021139713004231453,
              -0.015542308799922466,
              0.005030191969126463,
              0.010006040334701538,
              0.020637033507227898,
              -0.013769344426691532,
              -0.02226734533905983,
              0.03228018060326576,
              -0.002003925386816263,
              0.0007862859056331217,
              -0.004143709782510996,
              0.0354321151971817,
              0.011446149088442326,
              -0.009041438810527325,
              0.005003020167350769,
              -0.000519237422849983,
              -0.0013653864152729511,
              -0.0029005969408899546,
              0.003051740350201726,
              0.006072912365198135,
              0.009428638033568859,
              0.019061066210269928,
              -0.0006410862552002072,
              0.04154578596353531,
              0.021166885271668434,
              -0.008708584122359753,
              -0.019781120121479034,
              0.010637786239385605,
              0.009795458056032658,
              -0.020759306848049164,
              -0.012302063405513763,
              -0.008008908480405807,
              0.029698852449655533,
              -0.032089974731206894,
              0.022905884310603142,
              0.02465846948325634,
              0.037306975573301315,
              -0.0007421316695399582,
              -0.01732206530869007,
              0.00954411830753088,
              0.0015487964265048504,
              0.03279644623398781,
              -0.012994945980608463,
              -0.017362823709845543,
              0.009292778559029102,
              -0.005828365683555603,
              0.023367807269096375,
              -0.0037395283579826355,
              0.02627519704401493,
              -0.038692738860845566,
              0.04312175512313843,
              0.02268850989639759,
              0.014102199114859104,
              0.008919165469706059,
              -0.00975470058619976,
              -0.013558762148022652,
              0.004568270407617092,
              -0.02624802477657795,
              -0.02490301802754402,
              0.016941659152507782,
              0.01709110476076603,
              0.015555894933640957,
              0.03448110073804855,
              0.002479432849213481,
              -0.019047480076551437,
              -0.027538688853383064,
              -0.02209072932600975,
              -0.018585557118058205,
              -0.04298589378595352,
              -0.0010792326647788286,
              -0.020025666803121567,
              0.0010299836285412312,
              -0.03521474078297615,
              0.01729489490389824,
              -0.032334521412849426,
              -0.006168013904243708,
              -0.013490832410752773,
              0.02327270433306694,
              0.010732888244092464,
              0.004731301683932543,
              0.018177980557084084,
              0.010848368518054485,
              -0.010026419535279274,
              -0.0349973663687706,
              0.056626174598932266,
              0.017580198124051094,
              -0.0033455363009124994,
              -0.014292402192950249,
              0.004659975413233042,
              0.0033828974701464176,
              0.0019954340532422066,
              0.012383579276502132,
              -0.01074647344648838,
              0.0038482157979160547,
              -0.01532493345439434,
              0.013354972936213017,
              -0.016221605241298676,
              0.0037157528568059206,
              0.0009289382142014802,
              0.011955621652305126,
              0.012369993142783642,
              0.03024228848516941,
              -0.018001362681388855,
              0.003722545923665166,
              -0.019414300099015236,
              -0.0002935835509561002,
              0.030079258605837822,
              0.0014358634361997247,
              -0.01493094116449356,
              0.003742924891412258,
              0.007234510034322739,
              -0.02310967445373535,
              0.0024318823125213385,
              -0.004102952312678099,
              -0.016004230827093124,
              0.01123556774109602,
              -0.02107178419828415,
              -0.03575817868113518,
              0.013993511907756329,
              0.009965282864868641,
              -0.004975848365575075,
              -0.017226964235305786,
              0.012118653394281864,
              0.00012758040975313634,
              0.004038419108837843,
              0.005233981180936098,
              -0.01830025389790535,
              -0.011758625507354736,
              -0.009584876708686352,
              0.013205528259277344,
              -0.015868371352553368,
              -0.006881275679916143,
              -0.008776513859629631,
              -0.01811004988849163,
              -0.028394602239131927,
              -0.0058079869486391544,
              -0.03524191305041313,
              0.020039252936840057,
              0.010603821836411953,
              -0.008389314636588097,
              -0.022376032546162605,
              0.017362823709845543,
              0.009109368547797203,
              -0.020840823650360107,
              -0.005729867611080408,
              0.0032860978972166777,
              -0.023924829438328743,
              -0.001446052803657949,
              -0.004181071184575558,
              -0.17389994859695435,
              -0.011317082680761814,
              0.016778629273176193,
              -0.040241535753011703,
              0.008898787200450897,
              -0.0012066008057445288,
              0.01709110476076603,
              -0.02171032316982746,
              -0.038502536714076996,
              0.012981359846889973,
              0.02085440792143345,
              0.00885123573243618,
              -0.027008837088942528,
              -0.017063932493329048,
              -0.017036762088537216,
              0.004619217477738857,
              0.0025337766855955124,
              0.021737493574619293,
              0.0030670245178043842,
              0.0015411543427035213,
              0.042034879326820374,
              -0.0326605848968029,
              0.017661714926362038,
              -0.014075027778744698,
              0.00945581030100584,
              -0.0028615372721105814,
              -0.002742660464718938,
              0.009985661134123802,
              -0.0026662396267056465,
              -0.03722545877099037,
              0.008599895983934402,
              0.006239340174943209,
              0.025691000744700432,
              -0.009184091351926327,
              0.0023147035390138626,
              0.017199791967868805,
              0.016438979655504227,
              -0.006888068746775389,
              -0.009347123093903065,
              0.0025949133560061455,
              0.031111789867281914,
              0.01929202675819397,
              0.016547666862607002,
              -0.01400709804147482,
              -0.019061066210269928,
              0.020786479115486145,
              0.020134354010224342,
              -0.014387504197657108,
              0.021723909303545952,
              -0.019726775586605072,
              0.00318420329131186,
              -0.00817193929105997,
              -0.0016048384131863713,
              0.013151183724403381,
              0.023965587839484215,
              0.009190884418785572,
              -0.0033795011695474386,
              0.01666994020342827,
              -0.014795082621276379,
              0.004391652997583151,
              -0.00019243202405050397,
              -0.024617712944746017,
              0.011330668814480305,
              -0.017145449295639992,
              -0.02725338377058506,
              -0.01262133289128542,
              -0.017756815999746323,
              0.0047245086170732975,
              0.0006156126619316638,
              0.011004606261849403,
              -0.015447206795215607,
              -0.013844067230820656,
              -0.0068608964793384075,
              0.00015676894690841436,
              0.010474755428731441,
              0.032905131578445435,
              -0.009238434955477715,
              -0.005444562993943691,
              0.019821878522634506,
              0.009978868998587132,
              -0.013185149058699608,
              0.032714929431676865,
              -0.018599143251776695,
              0.018965963274240494,
              -0.006385388784110546,
              0.0009892258094623685,
              0.0009255417389795184,
              0.018558386713266373,
              -0.010624200105667114,
              -0.01869424618780613,
              0.005186430178582668,
              -0.03024228848516941,
              0.013205528259277344,
              0.0012210358399897814,
              0.0019852446857839823,
              0.011371427215635777,
              0.02310967445373535,
              0.0039161453023552895,
              0.004201449919492006,
              -0.015474379062652588,
              0.0053902193903923035,
              0.003841422963887453,
              0.009319950826466084,
              0.029399961233139038,
              0.011588801629841328,
              0.027959851548075676,
              -0.024413922801613808,
              0.013816894963383675,
              0.029726022854447365,
              0.0038244405295699835,
              -0.02429164946079254,
              -0.016642769798636436,
              0.015053214505314827,
              0.020650619640946388,
              0.013796515762805939,
              0.015352105721831322,
              -0.0035459287464618683,
              -0.017947018146514893,
              0.011208395473659039,
              -0.014061441645026207,
              0.03415503725409508,
              0.0034763009753078222,
              -0.01793343387544155,
              0.02168315090239048,
              -0.02207714319229126,
              -0.04717036336660385,
              -0.09417769312858582,
              -0.038094960153102875,
              -0.019658846780657768,
              0.0047143190167844296,
              -0.003844819264486432,
              0.016574839130043983,
              0.02688656374812126,
              0.018830103799700737,
              0.00039420436951331794,
              0.03282361850142479,
              -0.019047480076551437,
              -0.013558762148022652,
              0.002316401805728674,
              -0.02687297761440277,
              0.008104009553790092,
              0.0024607523810118437,
              0.0016260665142908692,
              0.004710922483354807,
              -0.017960604280233383,
              0.01969960518181324,
              0.0030789123848080635,
              -0.030785726383328438,
              0.032714929431676865,
              -0.03184542804956436,
              0.031138960272073746,
              -0.026207266375422478,
              -0.031763914972543716,
              0.008307798765599728,
              -0.0024641486816108227,
              -0.016778629273176193,
              -0.012336027808487415,
              -0.02926410175859928,
              9.860416321316734e-05,
              -0.014944527298212051,
              0.0032657189294695854,
              -0.008756134659051895,
              -0.016194432973861694,
              -0.027348484843969345,
              -0.003950110170990229,
              -0.029970571398735046,
              0.0020327954553067684,
              0.0007306685438379645,
              0.0004997076466679573,
              -0.01592271402478218,
              -0.012132239528000355,
              -0.04002416133880615,
              -0.010963848792016506,
              0.014387504197657108,
              0.011317082680761814,
              -0.007295646704733372,
              -0.014876597560942173,
              -0.008919165469706059,
              -0.03399200737476349,
              -0.007064685691148043,
              0.01313080545514822,
              0.012852293439209461,
              0.0032708137296140194,
              0.004452789667993784,
              -0.011147258803248405,
              -0.02171032316982746,
              0.007954563945531845,
              -0.007397541310638189,
              -0.01892520673573017,
              0.0072888536378741264,
              0.0010936676990240812,
              -0.004371274262666702,
              -0.00934033002704382,
              -0.025568727403879166,
              0.012600953690707684,
              -0.0306226946413517,
              0.0029464494436979294,
              0.014469020068645477,
              0.0035798936150968075,
              0.019604502245783806,
              -0.021207643672823906,
              -0.010705715976655483,
              -0.015515136532485485,
              0.021574463695287704,
              0.005845348350703716,
              -0.01609933190047741,
              -0.03228018060326576,
              -0.022729268297553062,
              0.01729489490389824,
              -0.025025291368365288,
              -0.006803156342357397,
              -0.00766246672719717,
              -0.024644885212183,
              -0.00037127811810933053,
              -0.0038040615618228912,
              -0.029698852449655533,
              0.012668883427977562,
              0.03045966476202011,
              -0.0013450074475258589,
              -0.013191942125558853,
              -0.006235943641513586,
              0.0068608964793384075,
              -0.025758931413292885,
              0.004374670796096325,
              0.006096688099205494,
              0.045458536595106125,
              -0.01364027801901102,
              -0.00641595758497715,
              -0.052822113037109375,
              0.008803685195744038,
              0.0031909963581711054,
              -0.022946642711758614,
              0.009218056686222553,
              0.023775383830070496,
              0.037497177720069885,
              0.009421844966709614,
              -0.013470453210175037,
              0.030731383711099625,
              -0.01073968131095171,
              0.02168315090239048,
              0.013049289584159851,
              0.0001267312909476459,
              -0.012994945980608463,
              -0.024821501225233078,
              0.02385690063238144,
              -0.021968455985188484,
              0.015814026817679405,
              0.024726400151848793,
              -0.003197789192199707,
              -0.011378219351172447,
              0.012859086506068707,
              0.01253981702029705,
              -0.0019173149485141039,
              -0.0010130011942237616,
              -0.015433620661497116,
              0.014686394482851028,
              -0.01733565144240856,
              -0.02268850989639759,
              -0.002095630392432213,
              -0.019590916112065315,
              0.006246133241802454,
              0.00995169673115015,
              -0.010889125987887383,
              -0.002598309889435768,
              0.0007387351361103356,
              0.008830857463181019,
              0.02268850989639759,
              0.009462603367865086,
              -0.01194203644990921,
              -0.026112165302038193,
              0.02882935293018818,
              -0.025976305827498436,
              0.0027494532987475395,
              -0.0033964836038649082,
              0.004632803611457348,
              -0.002212808933109045,
              -0.003817647462710738,
              -0.006483886856585741,
              0.024155789986252785,
              0.02108537033200264,
              0.009435431100428104,
              -0.022973814979195595,
              -0.01631670631468296,
              0.005447959527373314,
              0.013551969081163406,
              0.026152923703193665,
              -0.026913736015558243,
              -0.009809044189751148,
              0.017607370391488075,
              0.007825498469173908,
              0.002411503344774246,
              -0.00325383129529655,
              0.0016421998152509332,
              0.012859086506068707,
              0.019237682223320007,
              -0.0017186206532642245,
              -0.009197677485644817,
              -0.034535445272922516,
              -0.0014018985675647855,
              -0.0013008532114326954,
              0.012071102857589722,
              0.011575215496122837,
              0.03564948961138725,
              0.006229150574654341,
              0.00017884607950691134,
              0.00716658029705286,
              -0.01463205087929964,
              0.03662768006324768,
              0.015596652403473854,
              -0.028965210542082787,
              -0.01342969574034214,
              -0.017770402133464813,
              0.004829799756407738,
              -0.0027358673978596926,
              0.006796363741159439,
              -0.0024556575808674097,
              -0.01402068417519331,
              -0.0037870791275054216,
              0.005712885409593582,
              0.007988529279828072,
              0.002396219177171588,
              0.0053800297901034355,
              -0.004089366178959608,
              0.009809044189751148,
              0.010984227992594242,
              0.00021461529831867665,
              0.012607746757566929,
              -0.013592727482318878,
              0.001277926960028708,
              0.0013322706799954176,
              -0.004595442209392786,
              -0.005634766072034836,
              -0.014536949805915356,
              0.020990267395973206,
              -0.019020307809114456,
              -0.028313087299466133,
              0.00040587977855466306,
              0.0034151640720665455,
              0.010298137553036213,
              0.020243041217327118,
              -0.015257003717124462,
              0.0015292667085304856,
              -0.02861197665333748,
              0.03423655405640602,
              0.013735379092395306,
              -0.017457924783229828,
              -0.008498001843690872,
              0.007995322346687317,
              0.007085064426064491,
              -0.00015952858666423708,
              0.028557633981108665,
              0.0013874635333195329,
              0.01592271402478218,
              0.021642392501235008,
              0.04121972247958183,
              -0.010481548495590687,
              0.00894633773714304,
              0.008287419565021992,
              -0.013253078795969486,
              -0.017145449295639992,
              -0.005529474932700396,
              -0.019020307809114456,
              -0.00023456964117940515,
              0.0033217607997357845,
              0.0027137903962284327,
              0.022172244265675545,
              -0.0034253536723554134,
              0.0892324149608612,
              0.02586761862039566,
              0.004996227100491524,
              0.0027341691311448812,
              -0.0010401731124147773,
              0.025813274085521698,
              0.019210509955883026,
              0.014346746727824211,
              -0.0023418753407895565,
              0.00481961015611887,
              0.02449543960392475,
              0.01633029244840145,
              0.006545023526996374,
              -0.012492266483604908,
              -0.014795082621276379,
              0.006239340174943209,
              0.011595594696700573,
              -0.0030924982856959105,
              0.01787908934056759,
              -0.013103633187711239,
              0.046056315302848816,
              -0.026492571458220482,
              0.007458677981048822,
              0.011955621652305126,
              -0.03461695834994316,
              -0.006049137096852064,
              0.010760059580206871,
              0.015664581209421158,
              -0.018667073920369148,
              -0.039073146879673004,
              0.019658846780657768,
              -0.002095630392432213,
              -0.028150055557489395,
              -0.012845500372350216,
              0.022213002666831017,
              -0.002409805078059435,
              -0.001168390386737883,
              -0.005818176083266735,
              0.018245909363031387,
              0.010617407038807869,
              0.01501245703548193,
              0.01690090261399746,
              -0.004459582734853029,
              -0.03497019410133362,
              -0.01194203644990921,
              -0.0059676216915249825,
              -0.02008001133799553,
              -0.027348484843969345,
              -0.016588425263762474
            ],
            "existingAnswer": true,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            },
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "",
            "question": "What is the purpose of BERT?",
            "retrievedDocs": ""
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
                  ],
                  "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1694832912.457556,
          "end_time": 1694832912.476555,
          "error": null,
          "children": null,
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "name": "",
      "description": "",
      "tags": null,
      "system_metrics": {
        "duration": 2.472668,
        "total_tokens": 8
      },
      "result": {
        "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
        "context": [
          "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
                ],
                "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
                "error": ""
              }
            }
          ]
        }
      },
      "upload_metrics": false
    }
  ],
  "node_runs": [
    {
      "node": "parse_postBody",
      "flow_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_parse_postBody_0",
      "status": "Completed",
      "inputs": {
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        }
      },
      "output": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "start_time": "2023-09-15T21:55:10.022265Z",
      "end_time": "2023-09-15T21:55:10.022265Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1694832910.022265,
          "end_time": 1694832910.022265,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.0
      },
      "result": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      }
    },
    {
      "node": "create_llm",
      "flow_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_create_llm_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        }
      },
      "output": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
      "metrics": null,
      "error": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "start_time": "2023-09-15T21:55:10.037893Z",
      "end_time": "2023-09-15T21:55:10.037893Z",
      "index": 0,
      "api_calls": [
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1694832910.037893,
          "end_time": 1694832910.037893,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.0
      },
      "result": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')"
    },
    {
      "node": "embed_the_question",
      "flow_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_embed_the_question_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "question": "What is the purpose of BERT?"
      },
      "output": [
        -0.022036384791135788,
        -0.0164797380566597,
        0.0005999039276503026,
        -0.007227716967463493,
        0.003868594765663147,
        0.01381010189652443,
        -0.006500869523733854,
        -0.011711074970662594,
        -0.01653408259153366,
        -0.015121144242584705,
        0.0027137903962284327,
        0.027362070977687836,
        -0.025758931413292885,
        0.00944222416728735,
        3.94045164284762e-05,
        0.0016226699808612466,
        0.016561252996325493,
        0.015487965196371078,
        0.004052004776895046,
        0.0009892258094623685,
        -0.005791004281491041,
        -0.014170128852128983,
        -0.019210509955883026,
        0.020990267395973206,
        -0.006324252113699913,
        -0.02627519704401493,
        0.05154503509402275,
        -0.033502914011478424,
        0.00033030801569111645,
        -0.013558762148022652,
        0.027144696563482285,
        0.030269460752606392,
        -0.010019626468420029,
        -0.008131181821227074,
        -0.013069668784737587,
        0.0003339167742524296,
        0.01770247146487236,
        -0.002297721104696393,
        0.017009589821100235,
        -0.01224092673510313,
        0.015800440683960915,
        0.01892520673573017,
        0.013878031633794308,
        -0.001744094304740429,
        -0.0005765530513599515,
        0.003309873165562749,
        0.01252623088657856,
        -0.004153899382799864,
        0.0005892898770980537,
        -0.023788969963788986,
        0.018653487786650658,
        0.01949581503868103,
        -0.02384331449866295,
        -0.002343573607504368,
        -0.03442675620317459,
        -0.0030534386169165373,
        -0.0077575682662427425,
        0.02582686021924019,
        0.009068611077964306,
        -0.02248472161591053,
        0.010753266513347626,
        0.0006746265571564436,
        -0.015664581209421158,
        0.019156167283654213,
        0.0048841433599591255,
        -0.009299571625888348,
        -0.022946642711758614,
        0.029101070016622543,
        0.010603821836411953,
        0.008885201066732407,
        0.007628501858562231,
        0.020623447373509407,
        0.019455058500170708,
        -0.011086122132837772,
        0.03654616326093674,
        -0.002844554837793112,
        -0.01502604316920042,
        -0.005852140951901674,
        0.008036079816520214,
        0.0004491849394980818,
        0.006405767984688282,
        0.02306891605257988,
        -0.02109895646572113,
        0.020922338590025902,
        0.013062875717878342,
        -0.007594536989927292,
        0.013103633187711239,
        0.01690090261399746,
        -0.019577331840991974,
        -0.0003455921832937747,
        0.006201978772878647,
        0.002939656376838684,
        0.03314967826008797,
        0.004282965790480375,
        0.000431353400927037,
        0.03024228848516941,
        -0.004602235276252031,
        0.029101070016622543,
        0.0016498418990522623,
        -0.035106055438518524,
        0.009027853608131409,
        -0.0075266072526574135,
        -0.007940978743135929,
        -0.01224092673510313,
        -0.019224096089601517,
        0.0021533705294132233,
        0.00983621645718813,
        -0.010372860357165337,
        0.028150055557489395,
        -0.008076838217675686,
        -0.007716810330748558,
        0.03203563392162323,
        -0.0034202588722109795,
        -0.020215868949890137,
        0.005529474932700396,
        0.021533705294132233,
        0.0010299836285412312,
        0.0005413145408965647,
        0.017457924783229828,
        0.020732134580612183,
        0.03328553959727287,
        -0.002212808933109045,
        0.0011565026361495256,
        -0.01827308163046837,
        0.022348862141370773,
        -0.008137974888086319,
        -0.021615220233798027,
        -0.019998494535684586,
        0.008701791055500507,
        -0.029481476172804832,
        0.006055930163711309,
        -0.0036444268189370632,
        0.0013798214495182037,
        -0.023816142231225967,
        -0.002878519706428051,
        0.023191189393401146,
        -0.00796135701239109,
        -0.010107934474945068,
        -0.026424642652273178,
        -0.01768888533115387,
        0.016819385811686516,
        0.022593408823013306,
        -0.023177603259682655,
        -0.03518756851553917,
        -0.026601258665323257,
        0.02546004019677639,
        0.0007625105208717287,
        0.02842177450656891,
        0.010114727541804314,
        0.002683222061023116,
        0.0042456043884158134,
        -0.031138960272073746,
        -0.018055707216262817,
        0.009591669775545597,
        0.0022875317372381687,
        0.013979925774037838,
        -0.00032075541093945503,
        0.0026458606589585543,
        -0.000471262086648494,
        -0.00011494974023662508,
        0.0030279650818556547,
        0.014550535008311272,
        0.011554837226867676,
        -0.009598462842404842,
        0.03760586678981781,
        0.04051325470209122,
        -0.004279569257050753,
        -0.0026611448265612125,
        0.012397164478898048,
        0.003573100548237562,
        -0.008688204921782017,
        0.01570533961057663,
        -0.026981664821505547,
        0.004629407078027725,
        0.015433620661497116,
        0.00746547058224678,
        0.017430752515792847,
        0.007805119268596172,
        0.0007680298294872046,
        -0.013361766003072262,
        -0.0051320865750312805,
        0.006398974917829037,
        0.03399200737476349,
        0.005003020167350769,
        -0.015257003717124462,
        -0.002204317832365632,
        0.003770096693187952,
        -0.035350602120161057,
        0.0028004006016999483,
        -0.044643379747867584,
        0.018205150961875916,
        0.019061066210269928,
        0.011711074970662594,
        -0.004673561546951532,
        -0.6364738345146179,
        -0.014455433934926987,
        0.016629183664917946,
        -0.00835534930229187,
        0.021329917013645172,
        -0.013083254918456078,
        -0.010352482087910175,
        -0.00566193787381053,
        -0.01171786803752184,
        0.03263341262936592,
        0.005447959527373314,
        -0.012607746757566929,
        -0.005906485021114349,
        -0.01502604316920042,
        -0.0013458565808832645,
        -0.022511892020702362,
        -0.00936070829629898,
        -0.00402143644168973,
        0.01869424618780613,
        -0.003973885904997587,
        -0.02446826733648777,
        0.02124840021133423,
        -0.04380105063319206,
        0.005852140951901674,
        -0.01752585545182228,
        0.007132615428417921,
        0.002350366674363613,
        -0.009313157759606838,
        0.007295646704733372,
        -0.007268474902957678,
        -0.01323269959539175,
        0.03336705267429352,
        0.03956224024295807,
        -0.015284175984561443,
        0.031709570437669754,
        0.011860520578920841,
        -0.002382633276283741,
        0.015759684145450592,
        -0.031193304806947708,
        0.0256638303399086,
        -0.00866782572120428,
        -0.019618088379502296,
        0.02329987660050392,
        -0.007010342087596655,
        0.03217149153351784,
        0.007248095702379942,
        0.01232923474162817,
        0.021425018087029457,
        0.0018884448800235987,
        0.007431505713611841,
        -0.027606617659330368,
        0.008722169324755669,
        0.0014333160361275077,
        -0.008464036509394646,
        0.014401090331375599,
        -0.0018341010436415672,
        0.022837955504655838,
        -0.014061441645026207,
        0.0002553731028456241,
        -0.022606994956731796,
        0.011643145233392715,
        0.034345243126153946,
        -0.010902712121605873,
        -0.02567741461098194,
        -0.011004606261849403,
        0.015229832381010056,
        0.0035323428455740213,
        0.012770777568221092,
        0.011887691915035248,
        -0.04477923735976219,
        0.010481548495590687,
        0.024984532967209816,
        0.0006393880466930568,
        -0.0006792966742068529,
        0.02008001133799553,
        0.017743229866027832,
        -0.00228923000395298,
        -0.015121144242584705,
        -0.0004844234499614686,
        0.022973814979195595,
        -0.009252021089196205,
        -0.005587215535342693,
        0.012607746757566929,
        -0.0028190813027322292,
        -0.018748588860034943,
        0.017797574400901794,
        -0.006918637081980705,
        -0.02309608832001686,
        0.005352857988327742,
        0.026207266375422478,
        -0.0012125446228310466,
        0.02782399207353592,
        -0.021601635962724686,
        -0.03942637890577316,
        0.02248472161591053,
        0.012369993142783642,
        0.003501774510368705,
        -0.016642769798636436,
        0.03247038275003433,
        -0.007241302635520697,
        0.002798702334985137,
        -0.017607370391488075,
        0.020243041217327118,
        0.023761799558997154,
        0.00856593158096075,
        0.016438979655504227,
        0.02270209603011608,
        0.020637033507227898,
        0.002071854891255498,
        -0.017199791967868805,
        0.004792438354343176,
        -0.016357464715838432,
        0.00825345516204834,
        0.008776513859629631,
        -0.006633332464843988,
        -0.0329594761133194,
        0.014183714985847473,
        0.0027817199006676674,
        0.012220547534525394,
        -0.021234814077615738,
        0.0019427885999903083,
        -0.009598462842404842,
        0.012444715946912766,
        -0.016642769798636436,
        0.019210509955883026,
        0.017675301060080528,
        -0.0031536349561065435,
        0.004310137592256069,
        -0.020650619640946388,
        -0.005298514384776354,
        -0.021832596510648727,
        -0.006426146719604731,
        0.0053800297901034355,
        -0.012152617797255516,
        0.04393691197037697,
        0.03899163007736206,
        0.03260624036192894,
        -0.0015937999123707414,
        -0.02050117403268814,
        0.005006416700780392,
        -0.012315649539232254,
        -0.010495133697986603,
        0.01567816734313965,
        -0.005740057211369276,
        -0.052034128457307816,
        -0.016167260706424713,
        0.008891994133591652,
        0.003070421051234007,
        0.001690599718131125,
        -0.011554837226867676,
        0.0015428526094183326,
        -0.02624802477657795,
        -0.018585557118058205,
        -0.005899691954255104,
        -0.013090047053992748,
        -0.028557633981108665,
        -0.009408259764313698,
        -0.04668127000331879,
        -0.019536573439836502,
        -0.031709570437669754,
        0.010325309820473194,
        0.00373273529112339,
        -0.011989586986601353,
        -0.012770777568221092,
        0.0011378219351172447,
        -0.03497019410133362,
        0.005515889264643192,
        0.022213002666831017,
        -0.027525102719664574,
        -0.033910490572452545,
        0.028938040137290955,
        -0.01846328377723694,
        -0.01033210288733244,
        0.005774022080004215,
        -0.026682773604989052,
        0.013776137493550777,
        -0.03325836732983589,
        -0.016017816960811615,
        0.0003791324852500111,
        -0.016044987365603447,
        0.005030191969126463,
        0.030704211443662643,
        -0.010012833401560783,
        -0.009224848821759224,
        0.012220547534525394,
        0.0020344937220215797,
        0.017172621563076973,
        -0.00310948072001338,
        -0.02488943189382553,
        0.00974111445248127,
        0.00874934159219265,
        0.026587672531604767,
        -0.003061929950490594,
        0.02090875245630741,
        0.012573782354593277,
        0.006110273767262697,
        0.016357464715838432,
        -0.009122954681515694,
        0.0008754436275921762,
        -0.005172844510525465,
        0.025297008454799652,
        0.010732888244092464,
        0.00034325712476857007,
        -0.012940602377057076,
        -0.008294212631881237,
        0.017457924783229828,
        -8.363840606762096e-05,
        -0.016438979655504227,
        0.008219489827752113,
        -0.00042880602995865047,
        0.01461846474558115,
        -0.029508648440241814,
        -0.02325911819934845,
        -0.0005476829828694463,
        -0.008708584122359753,
        0.021044611930847168,
        -0.009068611077964306,
        0.02289229817688465,
        -0.034263726323843,
        0.0033591222018003464,
        0.003783682594075799,
        -0.01793343387544155,
        0.004975848365575075,
        0.0009900749428197742,
        -0.011765418574213982,
        0.006297080311924219,
        -0.008097216486930847,
        -0.012702848762273788,
        0.012410750612616539,
        -0.022362448275089264,
        0.0005455601494759321,
        0.028285915032029152,
        0.02999774180352688,
        0.020025666803121567,
        -0.0012032042723149061,
        -0.00856593158096075,
        0.014469020068645477,
        -0.010196243412792683,
        0.025595899671316147,
        -0.017159035429358482,
        0.0026017064228653908,
        0.01154804416000843,
        0.013110426254570484,
        -0.0015776666114106774,
        0.03545928746461868,
        0.015814026817679405,
        0.03244321048259735,
        0.010651372373104095,
        -0.016072159633040428,
        0.02287871204316616,
        -0.01431957446038723,
        -0.008878407999873161,
        -0.030731383711099625,
        0.0011514079524204135,
        0.0240606889128685,
        -0.00924522802233696,
        0.0040757800452411175,
        0.012899843975901604,
        -0.0057094888761639595,
        0.033883318305015564,
        0.016357464715838432,
        0.008362142369151115,
        0.022797197103500366,
        0.007805119268596172,
        0.019183339551091194,
        -0.014455433934926987,
        -0.014591293409466743,
        -0.0058079869486391544,
        -0.009184091351926327,
        -0.004938486963510513,
        -0.017254136502742767,
        -0.015080386772751808,
        0.0010206432780250907,
        -0.010617407038807869,
        0.01441467646509409,
        0.03024228848516941,
        -0.02490301802754402,
        0.021438604220747948,
        -0.0067929672077298164,
        0.034127864986658096,
        -0.0023045141715556383,
        -0.029916226863861084,
        0.03160088136792183,
        -0.006130652967840433,
        -0.027959851548075676,
        -0.0171318631619215,
        0.0032283575274050236,
        -0.02169673703610897,
        -0.004269379656761885,
        -0.018830103799700737,
        0.0016956944018602371,
        0.009007474407553673,
        -0.0019852446857839823,
        0.025120392441749573,
        0.00111829221714288,
        0.013293836265802383,
        0.007302439771592617,
        -0.01986263506114483,
        0.004432410933077335,
        0.004486754536628723,
        0.0019190132152289152,
        0.00785946287214756,
        -0.006490679923444986,
        -0.028965210542082787,
        0.02029738575220108,
        0.001125934300944209,
        -0.029209759086370468,
        -0.016642769798636436,
        -0.016466151922941208,
        -0.015977058559656143,
        0.038257990032434464,
        0.009462603367865086,
        0.01868066005408764,
        -0.019346369430422783,
        -5.301698547555134e-05,
        0.0292369294911623,
        0.007893427275121212,
        -0.006996755953878164,
        0.008511587977409363,
        0.004408635664731264,
        -0.028313087299466133,
        -0.012268098071217537,
        -0.01671069860458374,
        0.008436865173280239,
        0.04317609965801239,
        0.04755076766014099,
        -0.004174278117716312,
        0.019020307809114456,
        -0.012757192365825176,
        0.03635596111416817,
        -0.025758931413292885,
        -0.0389372855424881,
        -0.021194057539105415,
        -0.01649332419037819,
        0.0015802140114828944,
        -0.02664201706647873,
        0.006531437858939171,
        -0.022810783237218857,
        0.005254359915852547,
        0.01025058701634407,
        -0.0003099291061516851,
        -0.02626161091029644,
        0.033747460693120956,
        -0.016547666862607002,
        -0.0009994152933359146,
        0.013701414689421654,
        0.006076308898627758,
        0.010467962361872196,
        0.001453694887459278,
        -0.019427886232733727,
        -0.012254512868821621,
        0.010807610116899014,
        -0.0035323428455740213,
        -0.011588801629841328,
        -0.016561252996325493,
        -0.022579822689294815,
        -0.01753944158554077,
        0.00787304900586605,
        -0.02051476016640663,
        0.03939921036362648,
        0.001176881487481296,
        0.006545023526996374,
        0.011004606261849403,
        0.020025666803121567,
        0.0029600353445857763,
        0.02662843093276024,
        -0.002523587318137288,
        -0.01540644932538271,
        7.790684321662411e-05,
        -0.030378147959709167,
        0.004031626041978598,
        0.016574839130043983,
        0.011317082680761814,
        -0.002888709306716919,
        0.0012337726075202227,
        -0.004809420555830002,
        -0.04111103713512421,
        -0.005013209767639637,
        0.014469020068645477,
        -0.014699980616569519,
        0.012471887283027172,
        0.010196243412792683,
        -0.01827308163046837,
        -0.02385690063238144,
        -0.017403582111001015,
        -0.014047855511307716,
        0.01872141659259796,
        -0.007275267504155636,
        0.00365801271982491,
        0.004639596678316593,
        -0.017362823709845543,
        -0.0244275089353323,
        -0.015596652403473854,
        0.04627368971705437,
        -0.008212696760892868,
        -0.04032305255532265,
        -0.04727904871106148,
        -0.016765043139457703,
        0.01221375446766615,
        0.011486907489597797,
        0.03260624036192894,
        -0.020786479115486145,
        -0.001669371617026627,
        -0.0023147035390138626,
        -0.007574158255010843,
        -0.03445392847061157,
        0.0223081037402153,
        -0.04051325470209122,
        0.0038006650283932686,
        -0.01102498546242714,
        -0.00746547058224678,
        -0.027565861120820045,
        -0.019332783296704292,
        0.0354321151971817,
        0.002744358731433749,
        0.005644955672323704,
        0.02961733564734459,
        -0.028965210542082787,
        0.01846328377723694,
        0.019984908401966095,
        0.0072005451656877995,
        0.005634766072034836,
        0.012886258773505688,
        -0.028313087299466133,
        -0.011955621652305126,
        -0.009285985492169857,
        -0.010420411825180054,
        -0.03325836732983589,
        0.03165522590279579,
        -0.006307269912213087,
        0.0036274443846195936,
        -0.021126126870512962,
        0.01733565144240856,
        -0.01072609517723322,
        0.008925958536565304,
        -0.023394977673888206,
        -0.00310948072001338,
        -0.01711827702820301,
        0.0313563346862793,
        -0.0038855771999806166,
        -0.001333968946710229,
        0.024970946833491325,
        -0.012268098071217537,
        -0.0019478832837194204,
        0.0025490608531981707,
        -0.02308250218629837,
        0.016588425263762474,
        0.019006721675395966,
        -0.0026017064228653908,
        0.021520119160413742,
        0.005498906597495079,
        -0.01688731648027897,
        -0.007037513889372349,
        0.025609485805034637,
        -0.024509025737643242,
        0.01668352633714676,
        -0.0017101294361054897,
        -0.0278511643409729,
        -0.03143785148859024,
        -0.016832971945405006,
        -0.002584723988547921,
        0.008932751603424549,
        -0.01461846474558115,
        -0.0012813233770430088,
        0.012179790064692497,
        -0.0006381143466569483,
        0.009992454200983047,
        -0.013225906528532505,
        0.01889803446829319,
        -0.022960228845477104,
        -0.014808667823672295,
        0.008531966246664524,
        0.001946185017004609,
        0.03206280618906021,
        -0.017634542658925056,
        0.02999774180352688,
        -0.030079258605837822,
        -0.0076692597940564156,
        0.0005748548428528011,
        -0.02983471192419529,
        -0.0004895181627944112,
        0.004462979268282652,
        0.03673636540770531,
        0.02324553392827511,
        0.03184542804956436,
        -0.000315448414767161,
        0.01082798931747675,
        0.01501245703548193,
        -0.0043169306591153145,
        -0.006378596182912588,
        -0.028394602239131927,
        -0.003245339961722493,
        -0.022117899730801582,
        0.017172621563076973,
        -0.001829006359912455,
        -0.013633484952151775,
        -0.008260248228907585,
        0.0026662396267056465,
        0.003905955934897065,
        0.002542268019169569,
        0.0028343654703348875,
        0.006945808883756399,
        0.0010180958779528737,
        -0.005757039412856102,
        -0.03635596111416817,
        0.02325911819934845,
        -0.004198053851723671,
        -0.0004954620380885899,
        -0.042279426008462906,
        -0.025595899671316147,
        0.013484039343893528,
        0.011065742932260036,
        0.014903769828379154,
        0.012397164478898048,
        0.013721792958676815,
        -0.015243417583405972,
        0.0006584932561963797,
        -0.008531966246664524,
        -0.003461016807705164,
        -0.001688052318058908,
        -0.014890183694660664,
        -0.029454305768013,
        -0.016846558079123497,
        0.013966340571641922,
        0.028557633981108665,
        -0.021139713004231453,
        -0.003255529562011361,
        -0.010603821836411953,
        0.01628953404724598,
        0.008321384899318218,
        0.01709110476076603,
        -0.008606689050793648,
        0.004378067329525948,
        -0.0013467057142406702,
        -0.009768286719918251,
        -0.03638312965631485,
        -0.028693493455648422,
        -0.0037633036263287067,
        0.036301616579294205,
        0.0034525254741311073,
        -0.02328629046678543,
        0.026519743725657463,
        0.011969207786023617,
        0.0026203871238976717,
        -0.01569175347685814,
        0.004496944136917591,
        0.05230584740638733,
        0.02051476016640663,
        0.011887691915035248,
        0.029481476172804832,
        -0.0005655144923366606,
        0.0016761645674705505,
        0.01870783045887947,
        -0.0005846197018399835,
        -0.014346746727824211,
        -0.004975848365575075,
        -0.007594536989927292,
        0.006755605805665255,
        -0.008321384899318218,
        0.02429164946079254,
        0.02844894677400589,
        -0.018802933394908905,
        -0.01164993830025196,
        0.0006028758361935616,
        -0.00596422515809536,
        0.012471887283027172,
        0.013572348281741142,
        -0.01690090261399746,
        -0.00231130700558424,
        0.012397164478898048,
        -0.026981664821505547,
        0.016602011397480965,
        -0.006602764129638672,
        -0.005070949904620647,
        0.012132239528000355,
        0.006830328144133091,
        -0.012927016243338585,
        0.014428261667490005,
        0.020392486825585365,
        0.0012550007086247206,
        0.001064797630533576,
        0.02070496417582035,
        -0.015841199085116386,
        -0.027389243245124817,
        0.02861197665333748,
        0.030514007434248924,
        -0.014958113431930542,
        0.015134730376303196,
        0.008973509073257446,
        0.014958113431930542,
        0.01589554361999035,
        0.012763985432684422,
        -0.00374971772544086,
        0.027008837088942528,
        -0.035731006413698196,
        -0.008593102917075157,
        -0.013253078795969486,
        -0.024128619581460953,
        0.025568727403879166,
        -0.0280685406178236,
        0.003950110170990229,
        -0.0007968999561853707,
        -0.017362823709845543,
        0.0157460980117321,
        0.03200846165418625,
        0.013171562924981117,
        -0.0250796340405941,
        0.005325686186552048,
        -0.006157824769616127,
        0.01313080545514822,
        -0.009904146194458008,
        0.0034915851429104805,
        0.02168315090239048,
        -0.03043249249458313,
        -0.022158658131957054,
        0.010026419535279274,
        -0.0067623988725245,
        0.01692807301878929,
        -0.015460792928934097,
        -0.0007009492837823927,
        0.006840517744421959,
        0.028476117178797722,
        -0.04290438070893288,
        -0.027511516585946083,
        -0.005994793493300676,
        0.0066061606630682945,
        -0.01849045604467392,
        -0.005111707840114832,
        -0.006235943641513586,
        -0.01806929148733616,
        0.01493094116449356,
        -0.011466528289020061,
        0.005991396959871054,
        -0.011018192395567894,
        -0.003729338990524411,
        -0.009095782414078712,
        -0.0018952378304675221,
        -0.03757869452238083,
        -0.016955245286226273,
        0.0071801659651100636,
        0.016411809250712395,
        -0.006619746331125498,
        -0.009720736183226109,
        -0.010046797804534435,
        0.022593408823013306,
        -0.0049724518321454525,
        0.013667449355125427,
        -0.03423655405640602,
        -0.003525549778714776,
        0.0023265911731868982,
        0.007397541310638189,
        -0.011969207786023617,
        -0.042279426008462906,
        -0.004378067329525948,
        -0.018639901652932167,
        0.006735226605087519,
        -0.018599143251776695,
        -0.011344254948198795,
        0.006898257881402969,
        0.005896295420825481,
        -0.0025303801521658897,
        -0.013762551359832287,
        -0.00305683515034616,
        0.0001466856338083744,
        0.013803308829665184,
        -0.008348556235432625,
        0.017648128792643547,
        0.0006079705781303346,
        -0.012302063405513763,
        -0.020976681262254715,
        -0.029345616698265076,
        0.001504642190411687,
        -0.014849426224827766,
        -0.023394977673888206,
        -0.014672808349132538,
        0.031193304806947708,
        0.010685336776077747,
        -0.01571892574429512,
        -0.01691448874771595,
        -0.020840823650360107,
        -0.03855688124895096,
        -0.00805645901709795,
        -0.007723603397607803,
        0.017267722636461258,
        -0.0014180318685248494,
        0.017403582111001015,
        0.027593031525611877,
        0.02425089292228222,
        0.007105443626642227,
        -0.003943317569792271,
        0.0024964152835309505,
        0.000519237422849983,
        -0.0002857291838154197,
        -0.02801419608294964,
        -0.019061066210269928,
        -0.0009433733066543937,
        -0.047414910048246384,
        -0.000902615487575531,
        0.04279569163918495,
        -0.018667073920369148,
        0.017607370391488075,
        0.05630011111497879,
        -0.008993888273835182,
        -0.016425393521785736,
        -0.001525870175100863,
        0.015161902643740177,
        0.010196243412792683,
        0.02980753965675831,
        0.015977058559656143,
        -0.026207266375422478,
        -0.00731602543964982,
        0.026709945872426033,
        0.00895992387086153,
        -0.0329594761133194,
        -0.0018120239255949855,
        0.01653408259153366,
        -0.003362518735229969,
        0.003048344049602747,
        -0.019210509955883026,
        0.01650691032409668,
        -0.0030636282172054052,
        -0.008858028799295425,
        0.0046531823463737965,
        0.0147271528840065,
        -0.02967168018221855,
        -0.0028530461713671684,
        0.029101070016622543,
        -0.019400713965296745,
        -0.021397845819592476,
        0.022606994956731796,
        -0.01400709804147482,
        -0.0023690471425652504,
        0.016221605241298676,
        -0.014740738086402416,
        0.006843914277851582,
        -0.004707525949925184,
        0.0009951696265488863,
        -0.01847686991095543,
        -0.0004294428799767047,
        -0.029345616698265076,
        0.028122883290052414,
        -0.023150430992245674,
        0.030731383711099625,
        0.011106501333415508,
        -0.02942713350057602,
        0.006497472990304232,
        0.0005608443170785904,
        -0.020243041217327118,
        -0.014686394482851028,
        -0.014876597560942173,
        0.009299571625888348,
        -0.009007474407553673,
        0.014088613912463188,
        0.0007591140456497669,
        -0.01351121161133051,
        -0.015610238537192345,
        -0.00285983900539577,
        0.01832742430269718,
        -0.006847310811281204,
        -0.00011325149534968659,
        0.23715606331825256,
        -0.03877425566315651,
        0.010936676524579525,
        0.0009178996551781893,
        -0.019183339551091194,
        0.0034678096417337656,
        -0.008531966246664524,
        0.010950262658298016,
        -0.009170505218207836,
        -0.0037667001597583294,
        0.02168315090239048,
        0.002808891935274005,
        -0.010705715976655483,
        0.0050437781028449535,
        0.016411809250712395,
        -0.01331421546638012,
        -0.013090047053992748,
        -0.021139713004231453,
        -0.015542308799922466,
        0.005030191969126463,
        0.010006040334701538,
        0.020637033507227898,
        -0.013769344426691532,
        -0.02226734533905983,
        0.03228018060326576,
        -0.002003925386816263,
        0.0007862859056331217,
        -0.004143709782510996,
        0.0354321151971817,
        0.011446149088442326,
        -0.009041438810527325,
        0.005003020167350769,
        -0.000519237422849983,
        -0.0013653864152729511,
        -0.0029005969408899546,
        0.003051740350201726,
        0.006072912365198135,
        0.009428638033568859,
        0.019061066210269928,
        -0.0006410862552002072,
        0.04154578596353531,
        0.021166885271668434,
        -0.008708584122359753,
        -0.019781120121479034,
        0.010637786239385605,
        0.009795458056032658,
        -0.020759306848049164,
        -0.012302063405513763,
        -0.008008908480405807,
        0.029698852449655533,
        -0.032089974731206894,
        0.022905884310603142,
        0.02465846948325634,
        0.037306975573301315,
        -0.0007421316695399582,
        -0.01732206530869007,
        0.00954411830753088,
        0.0015487964265048504,
        0.03279644623398781,
        -0.012994945980608463,
        -0.017362823709845543,
        0.009292778559029102,
        -0.005828365683555603,
        0.023367807269096375,
        -0.0037395283579826355,
        0.02627519704401493,
        -0.038692738860845566,
        0.04312175512313843,
        0.02268850989639759,
        0.014102199114859104,
        0.008919165469706059,
        -0.00975470058619976,
        -0.013558762148022652,
        0.004568270407617092,
        -0.02624802477657795,
        -0.02490301802754402,
        0.016941659152507782,
        0.01709110476076603,
        0.015555894933640957,
        0.03448110073804855,
        0.002479432849213481,
        -0.019047480076551437,
        -0.027538688853383064,
        -0.02209072932600975,
        -0.018585557118058205,
        -0.04298589378595352,
        -0.0010792326647788286,
        -0.020025666803121567,
        0.0010299836285412312,
        -0.03521474078297615,
        0.01729489490389824,
        -0.032334521412849426,
        -0.006168013904243708,
        -0.013490832410752773,
        0.02327270433306694,
        0.010732888244092464,
        0.004731301683932543,
        0.018177980557084084,
        0.010848368518054485,
        -0.010026419535279274,
        -0.0349973663687706,
        0.056626174598932266,
        0.017580198124051094,
        -0.0033455363009124994,
        -0.014292402192950249,
        0.004659975413233042,
        0.0033828974701464176,
        0.0019954340532422066,
        0.012383579276502132,
        -0.01074647344648838,
        0.0038482157979160547,
        -0.01532493345439434,
        0.013354972936213017,
        -0.016221605241298676,
        0.0037157528568059206,
        0.0009289382142014802,
        0.011955621652305126,
        0.012369993142783642,
        0.03024228848516941,
        -0.018001362681388855,
        0.003722545923665166,
        -0.019414300099015236,
        -0.0002935835509561002,
        0.030079258605837822,
        0.0014358634361997247,
        -0.01493094116449356,
        0.003742924891412258,
        0.007234510034322739,
        -0.02310967445373535,
        0.0024318823125213385,
        -0.004102952312678099,
        -0.016004230827093124,
        0.01123556774109602,
        -0.02107178419828415,
        -0.03575817868113518,
        0.013993511907756329,
        0.009965282864868641,
        -0.004975848365575075,
        -0.017226964235305786,
        0.012118653394281864,
        0.00012758040975313634,
        0.004038419108837843,
        0.005233981180936098,
        -0.01830025389790535,
        -0.011758625507354736,
        -0.009584876708686352,
        0.013205528259277344,
        -0.015868371352553368,
        -0.006881275679916143,
        -0.008776513859629631,
        -0.01811004988849163,
        -0.028394602239131927,
        -0.0058079869486391544,
        -0.03524191305041313,
        0.020039252936840057,
        0.010603821836411953,
        -0.008389314636588097,
        -0.022376032546162605,
        0.017362823709845543,
        0.009109368547797203,
        -0.020840823650360107,
        -0.005729867611080408,
        0.0032860978972166777,
        -0.023924829438328743,
        -0.001446052803657949,
        -0.004181071184575558,
        -0.17389994859695435,
        -0.011317082680761814,
        0.016778629273176193,
        -0.040241535753011703,
        0.008898787200450897,
        -0.0012066008057445288,
        0.01709110476076603,
        -0.02171032316982746,
        -0.038502536714076996,
        0.012981359846889973,
        0.02085440792143345,
        0.00885123573243618,
        -0.027008837088942528,
        -0.017063932493329048,
        -0.017036762088537216,
        0.004619217477738857,
        0.0025337766855955124,
        0.021737493574619293,
        0.0030670245178043842,
        0.0015411543427035213,
        0.042034879326820374,
        -0.0326605848968029,
        0.017661714926362038,
        -0.014075027778744698,
        0.00945581030100584,
        -0.0028615372721105814,
        -0.002742660464718938,
        0.009985661134123802,
        -0.0026662396267056465,
        -0.03722545877099037,
        0.008599895983934402,
        0.006239340174943209,
        0.025691000744700432,
        -0.009184091351926327,
        0.0023147035390138626,
        0.017199791967868805,
        0.016438979655504227,
        -0.006888068746775389,
        -0.009347123093903065,
        0.0025949133560061455,
        0.031111789867281914,
        0.01929202675819397,
        0.016547666862607002,
        -0.01400709804147482,
        -0.019061066210269928,
        0.020786479115486145,
        0.020134354010224342,
        -0.014387504197657108,
        0.021723909303545952,
        -0.019726775586605072,
        0.00318420329131186,
        -0.00817193929105997,
        -0.0016048384131863713,
        0.013151183724403381,
        0.023965587839484215,
        0.009190884418785572,
        -0.0033795011695474386,
        0.01666994020342827,
        -0.014795082621276379,
        0.004391652997583151,
        -0.00019243202405050397,
        -0.024617712944746017,
        0.011330668814480305,
        -0.017145449295639992,
        -0.02725338377058506,
        -0.01262133289128542,
        -0.017756815999746323,
        0.0047245086170732975,
        0.0006156126619316638,
        0.011004606261849403,
        -0.015447206795215607,
        -0.013844067230820656,
        -0.0068608964793384075,
        0.00015676894690841436,
        0.010474755428731441,
        0.032905131578445435,
        -0.009238434955477715,
        -0.005444562993943691,
        0.019821878522634506,
        0.009978868998587132,
        -0.013185149058699608,
        0.032714929431676865,
        -0.018599143251776695,
        0.018965963274240494,
        -0.006385388784110546,
        0.0009892258094623685,
        0.0009255417389795184,
        0.018558386713266373,
        -0.010624200105667114,
        -0.01869424618780613,
        0.005186430178582668,
        -0.03024228848516941,
        0.013205528259277344,
        0.0012210358399897814,
        0.0019852446857839823,
        0.011371427215635777,
        0.02310967445373535,
        0.0039161453023552895,
        0.004201449919492006,
        -0.015474379062652588,
        0.0053902193903923035,
        0.003841422963887453,
        0.009319950826466084,
        0.029399961233139038,
        0.011588801629841328,
        0.027959851548075676,
        -0.024413922801613808,
        0.013816894963383675,
        0.029726022854447365,
        0.0038244405295699835,
        -0.02429164946079254,
        -0.016642769798636436,
        0.015053214505314827,
        0.020650619640946388,
        0.013796515762805939,
        0.015352105721831322,
        -0.0035459287464618683,
        -0.017947018146514893,
        0.011208395473659039,
        -0.014061441645026207,
        0.03415503725409508,
        0.0034763009753078222,
        -0.01793343387544155,
        0.02168315090239048,
        -0.02207714319229126,
        -0.04717036336660385,
        -0.09417769312858582,
        -0.038094960153102875,
        -0.019658846780657768,
        0.0047143190167844296,
        -0.003844819264486432,
        0.016574839130043983,
        0.02688656374812126,
        0.018830103799700737,
        0.00039420436951331794,
        0.03282361850142479,
        -0.019047480076551437,
        -0.013558762148022652,
        0.002316401805728674,
        -0.02687297761440277,
        0.008104009553790092,
        0.0024607523810118437,
        0.0016260665142908692,
        0.004710922483354807,
        -0.017960604280233383,
        0.01969960518181324,
        0.0030789123848080635,
        -0.030785726383328438,
        0.032714929431676865,
        -0.03184542804956436,
        0.031138960272073746,
        -0.026207266375422478,
        -0.031763914972543716,
        0.008307798765599728,
        -0.0024641486816108227,
        -0.016778629273176193,
        -0.012336027808487415,
        -0.02926410175859928,
        9.860416321316734e-05,
        -0.014944527298212051,
        0.0032657189294695854,
        -0.008756134659051895,
        -0.016194432973861694,
        -0.027348484843969345,
        -0.003950110170990229,
        -0.029970571398735046,
        0.0020327954553067684,
        0.0007306685438379645,
        0.0004997076466679573,
        -0.01592271402478218,
        -0.012132239528000355,
        -0.04002416133880615,
        -0.010963848792016506,
        0.014387504197657108,
        0.011317082680761814,
        -0.007295646704733372,
        -0.014876597560942173,
        -0.008919165469706059,
        -0.03399200737476349,
        -0.007064685691148043,
        0.01313080545514822,
        0.012852293439209461,
        0.0032708137296140194,
        0.004452789667993784,
        -0.011147258803248405,
        -0.02171032316982746,
        0.007954563945531845,
        -0.007397541310638189,
        -0.01892520673573017,
        0.0072888536378741264,
        0.0010936676990240812,
        -0.004371274262666702,
        -0.00934033002704382,
        -0.025568727403879166,
        0.012600953690707684,
        -0.0306226946413517,
        0.0029464494436979294,
        0.014469020068645477,
        0.0035798936150968075,
        0.019604502245783806,
        -0.021207643672823906,
        -0.010705715976655483,
        -0.015515136532485485,
        0.021574463695287704,
        0.005845348350703716,
        -0.01609933190047741,
        -0.03228018060326576,
        -0.022729268297553062,
        0.01729489490389824,
        -0.025025291368365288,
        -0.006803156342357397,
        -0.00766246672719717,
        -0.024644885212183,
        -0.00037127811810933053,
        -0.0038040615618228912,
        -0.029698852449655533,
        0.012668883427977562,
        0.03045966476202011,
        -0.0013450074475258589,
        -0.013191942125558853,
        -0.006235943641513586,
        0.0068608964793384075,
        -0.025758931413292885,
        0.004374670796096325,
        0.006096688099205494,
        0.045458536595106125,
        -0.01364027801901102,
        -0.00641595758497715,
        -0.052822113037109375,
        0.008803685195744038,
        0.0031909963581711054,
        -0.022946642711758614,
        0.009218056686222553,
        0.023775383830070496,
        0.037497177720069885,
        0.009421844966709614,
        -0.013470453210175037,
        0.030731383711099625,
        -0.01073968131095171,
        0.02168315090239048,
        0.013049289584159851,
        0.0001267312909476459,
        -0.012994945980608463,
        -0.024821501225233078,
        0.02385690063238144,
        -0.021968455985188484,
        0.015814026817679405,
        0.024726400151848793,
        -0.003197789192199707,
        -0.011378219351172447,
        0.012859086506068707,
        0.01253981702029705,
        -0.0019173149485141039,
        -0.0010130011942237616,
        -0.015433620661497116,
        0.014686394482851028,
        -0.01733565144240856,
        -0.02268850989639759,
        -0.002095630392432213,
        -0.019590916112065315,
        0.006246133241802454,
        0.00995169673115015,
        -0.010889125987887383,
        -0.002598309889435768,
        0.0007387351361103356,
        0.008830857463181019,
        0.02268850989639759,
        0.009462603367865086,
        -0.01194203644990921,
        -0.026112165302038193,
        0.02882935293018818,
        -0.025976305827498436,
        0.0027494532987475395,
        -0.0033964836038649082,
        0.004632803611457348,
        -0.002212808933109045,
        -0.003817647462710738,
        -0.006483886856585741,
        0.024155789986252785,
        0.02108537033200264,
        0.009435431100428104,
        -0.022973814979195595,
        -0.01631670631468296,
        0.005447959527373314,
        0.013551969081163406,
        0.026152923703193665,
        -0.026913736015558243,
        -0.009809044189751148,
        0.017607370391488075,
        0.007825498469173908,
        0.002411503344774246,
        -0.00325383129529655,
        0.0016421998152509332,
        0.012859086506068707,
        0.019237682223320007,
        -0.0017186206532642245,
        -0.009197677485644817,
        -0.034535445272922516,
        -0.0014018985675647855,
        -0.0013008532114326954,
        0.012071102857589722,
        0.011575215496122837,
        0.03564948961138725,
        0.006229150574654341,
        0.00017884607950691134,
        0.00716658029705286,
        -0.01463205087929964,
        0.03662768006324768,
        0.015596652403473854,
        -0.028965210542082787,
        -0.01342969574034214,
        -0.017770402133464813,
        0.004829799756407738,
        -0.0027358673978596926,
        0.006796363741159439,
        -0.0024556575808674097,
        -0.01402068417519331,
        -0.0037870791275054216,
        0.005712885409593582,
        0.007988529279828072,
        0.002396219177171588,
        0.0053800297901034355,
        -0.004089366178959608,
        0.009809044189751148,
        0.010984227992594242,
        0.00021461529831867665,
        0.012607746757566929,
        -0.013592727482318878,
        0.001277926960028708,
        0.0013322706799954176,
        -0.004595442209392786,
        -0.005634766072034836,
        -0.014536949805915356,
        0.020990267395973206,
        -0.019020307809114456,
        -0.028313087299466133,
        0.00040587977855466306,
        0.0034151640720665455,
        0.010298137553036213,
        0.020243041217327118,
        -0.015257003717124462,
        0.0015292667085304856,
        -0.02861197665333748,
        0.03423655405640602,
        0.013735379092395306,
        -0.017457924783229828,
        -0.008498001843690872,
        0.007995322346687317,
        0.007085064426064491,
        -0.00015952858666423708,
        0.028557633981108665,
        0.0013874635333195329,
        0.01592271402478218,
        0.021642392501235008,
        0.04121972247958183,
        -0.010481548495590687,
        0.00894633773714304,
        0.008287419565021992,
        -0.013253078795969486,
        -0.017145449295639992,
        -0.005529474932700396,
        -0.019020307809114456,
        -0.00023456964117940515,
        0.0033217607997357845,
        0.0027137903962284327,
        0.022172244265675545,
        -0.0034253536723554134,
        0.0892324149608612,
        0.02586761862039566,
        0.004996227100491524,
        0.0027341691311448812,
        -0.0010401731124147773,
        0.025813274085521698,
        0.019210509955883026,
        0.014346746727824211,
        -0.0023418753407895565,
        0.00481961015611887,
        0.02449543960392475,
        0.01633029244840145,
        0.006545023526996374,
        -0.012492266483604908,
        -0.014795082621276379,
        0.006239340174943209,
        0.011595594696700573,
        -0.0030924982856959105,
        0.01787908934056759,
        -0.013103633187711239,
        0.046056315302848816,
        -0.026492571458220482,
        0.007458677981048822,
        0.011955621652305126,
        -0.03461695834994316,
        -0.006049137096852064,
        0.010760059580206871,
        0.015664581209421158,
        -0.018667073920369148,
        -0.039073146879673004,
        0.019658846780657768,
        -0.002095630392432213,
        -0.028150055557489395,
        -0.012845500372350216,
        0.022213002666831017,
        -0.002409805078059435,
        -0.001168390386737883,
        -0.005818176083266735,
        0.018245909363031387,
        0.010617407038807869,
        0.01501245703548193,
        0.01690090261399746,
        -0.004459582734853029,
        -0.03497019410133362,
        -0.01194203644990921,
        -0.0059676216915249825,
        -0.02008001133799553,
        -0.027348484843969345,
        -0.016588425263762474
      ],
      "metrics": null,
      "error": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "start_time": "2023-09-15T21:55:10.037893Z",
      "end_time": "2023-09-15T21:55:10.628216Z",
      "index": 0,
      "api_calls": [
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "What is the purpose of BERT?"
          },
          "output": [
            -0.022036384791135788,
            -0.0164797380566597,
            0.0005999039276503026,
            -0.007227716967463493,
            0.003868594765663147,
            0.01381010189652443,
            -0.006500869523733854,
            -0.011711074970662594,
            -0.01653408259153366,
            -0.015121144242584705,
            0.0027137903962284327,
            0.027362070977687836,
            -0.025758931413292885,
            0.00944222416728735,
            3.94045164284762e-05,
            0.0016226699808612466,
            0.016561252996325493,
            0.015487965196371078,
            0.004052004776895046,
            0.0009892258094623685,
            -0.005791004281491041,
            -0.014170128852128983,
            -0.019210509955883026,
            0.020990267395973206,
            -0.006324252113699913,
            -0.02627519704401493,
            0.05154503509402275,
            -0.033502914011478424,
            0.00033030801569111645,
            -0.013558762148022652,
            0.027144696563482285,
            0.030269460752606392,
            -0.010019626468420029,
            -0.008131181821227074,
            -0.013069668784737587,
            0.0003339167742524296,
            0.01770247146487236,
            -0.002297721104696393,
            0.017009589821100235,
            -0.01224092673510313,
            0.015800440683960915,
            0.01892520673573017,
            0.013878031633794308,
            -0.001744094304740429,
            -0.0005765530513599515,
            0.003309873165562749,
            0.01252623088657856,
            -0.004153899382799864,
            0.0005892898770980537,
            -0.023788969963788986,
            0.018653487786650658,
            0.01949581503868103,
            -0.02384331449866295,
            -0.002343573607504368,
            -0.03442675620317459,
            -0.0030534386169165373,
            -0.0077575682662427425,
            0.02582686021924019,
            0.009068611077964306,
            -0.02248472161591053,
            0.010753266513347626,
            0.0006746265571564436,
            -0.015664581209421158,
            0.019156167283654213,
            0.0048841433599591255,
            -0.009299571625888348,
            -0.022946642711758614,
            0.029101070016622543,
            0.010603821836411953,
            0.008885201066732407,
            0.007628501858562231,
            0.020623447373509407,
            0.019455058500170708,
            -0.011086122132837772,
            0.03654616326093674,
            -0.002844554837793112,
            -0.01502604316920042,
            -0.005852140951901674,
            0.008036079816520214,
            0.0004491849394980818,
            0.006405767984688282,
            0.02306891605257988,
            -0.02109895646572113,
            0.020922338590025902,
            0.013062875717878342,
            -0.007594536989927292,
            0.013103633187711239,
            0.01690090261399746,
            -0.019577331840991974,
            -0.0003455921832937747,
            0.006201978772878647,
            0.002939656376838684,
            0.03314967826008797,
            0.004282965790480375,
            0.000431353400927037,
            0.03024228848516941,
            -0.004602235276252031,
            0.029101070016622543,
            0.0016498418990522623,
            -0.035106055438518524,
            0.009027853608131409,
            -0.0075266072526574135,
            -0.007940978743135929,
            -0.01224092673510313,
            -0.019224096089601517,
            0.0021533705294132233,
            0.00983621645718813,
            -0.010372860357165337,
            0.028150055557489395,
            -0.008076838217675686,
            -0.007716810330748558,
            0.03203563392162323,
            -0.0034202588722109795,
            -0.020215868949890137,
            0.005529474932700396,
            0.021533705294132233,
            0.0010299836285412312,
            0.0005413145408965647,
            0.017457924783229828,
            0.020732134580612183,
            0.03328553959727287,
            -0.002212808933109045,
            0.0011565026361495256,
            -0.01827308163046837,
            0.022348862141370773,
            -0.008137974888086319,
            -0.021615220233798027,
            -0.019998494535684586,
            0.008701791055500507,
            -0.029481476172804832,
            0.006055930163711309,
            -0.0036444268189370632,
            0.0013798214495182037,
            -0.023816142231225967,
            -0.002878519706428051,
            0.023191189393401146,
            -0.00796135701239109,
            -0.010107934474945068,
            -0.026424642652273178,
            -0.01768888533115387,
            0.016819385811686516,
            0.022593408823013306,
            -0.023177603259682655,
            -0.03518756851553917,
            -0.026601258665323257,
            0.02546004019677639,
            0.0007625105208717287,
            0.02842177450656891,
            0.010114727541804314,
            0.002683222061023116,
            0.0042456043884158134,
            -0.031138960272073746,
            -0.018055707216262817,
            0.009591669775545597,
            0.0022875317372381687,
            0.013979925774037838,
            -0.00032075541093945503,
            0.0026458606589585543,
            -0.000471262086648494,
            -0.00011494974023662508,
            0.0030279650818556547,
            0.014550535008311272,
            0.011554837226867676,
            -0.009598462842404842,
            0.03760586678981781,
            0.04051325470209122,
            -0.004279569257050753,
            -0.0026611448265612125,
            0.012397164478898048,
            0.003573100548237562,
            -0.008688204921782017,
            0.01570533961057663,
            -0.026981664821505547,
            0.004629407078027725,
            0.015433620661497116,
            0.00746547058224678,
            0.017430752515792847,
            0.007805119268596172,
            0.0007680298294872046,
            -0.013361766003072262,
            -0.0051320865750312805,
            0.006398974917829037,
            0.03399200737476349,
            0.005003020167350769,
            -0.015257003717124462,
            -0.002204317832365632,
            0.003770096693187952,
            -0.035350602120161057,
            0.0028004006016999483,
            -0.044643379747867584,
            0.018205150961875916,
            0.019061066210269928,
            0.011711074970662594,
            -0.004673561546951532,
            -0.6364738345146179,
            -0.014455433934926987,
            0.016629183664917946,
            -0.00835534930229187,
            0.021329917013645172,
            -0.013083254918456078,
            -0.010352482087910175,
            -0.00566193787381053,
            -0.01171786803752184,
            0.03263341262936592,
            0.005447959527373314,
            -0.012607746757566929,
            -0.005906485021114349,
            -0.01502604316920042,
            -0.0013458565808832645,
            -0.022511892020702362,
            -0.00936070829629898,
            -0.00402143644168973,
            0.01869424618780613,
            -0.003973885904997587,
            -0.02446826733648777,
            0.02124840021133423,
            -0.04380105063319206,
            0.005852140951901674,
            -0.01752585545182228,
            0.007132615428417921,
            0.002350366674363613,
            -0.009313157759606838,
            0.007295646704733372,
            -0.007268474902957678,
            -0.01323269959539175,
            0.03336705267429352,
            0.03956224024295807,
            -0.015284175984561443,
            0.031709570437669754,
            0.011860520578920841,
            -0.002382633276283741,
            0.015759684145450592,
            -0.031193304806947708,
            0.0256638303399086,
            -0.00866782572120428,
            -0.019618088379502296,
            0.02329987660050392,
            -0.007010342087596655,
            0.03217149153351784,
            0.007248095702379942,
            0.01232923474162817,
            0.021425018087029457,
            0.0018884448800235987,
            0.007431505713611841,
            -0.027606617659330368,
            0.008722169324755669,
            0.0014333160361275077,
            -0.008464036509394646,
            0.014401090331375599,
            -0.0018341010436415672,
            0.022837955504655838,
            -0.014061441645026207,
            0.0002553731028456241,
            -0.022606994956731796,
            0.011643145233392715,
            0.034345243126153946,
            -0.010902712121605873,
            -0.02567741461098194,
            -0.011004606261849403,
            0.015229832381010056,
            0.0035323428455740213,
            0.012770777568221092,
            0.011887691915035248,
            -0.04477923735976219,
            0.010481548495590687,
            0.024984532967209816,
            0.0006393880466930568,
            -0.0006792966742068529,
            0.02008001133799553,
            0.017743229866027832,
            -0.00228923000395298,
            -0.015121144242584705,
            -0.0004844234499614686,
            0.022973814979195595,
            -0.009252021089196205,
            -0.005587215535342693,
            0.012607746757566929,
            -0.0028190813027322292,
            -0.018748588860034943,
            0.017797574400901794,
            -0.006918637081980705,
            -0.02309608832001686,
            0.005352857988327742,
            0.026207266375422478,
            -0.0012125446228310466,
            0.02782399207353592,
            -0.021601635962724686,
            -0.03942637890577316,
            0.02248472161591053,
            0.012369993142783642,
            0.003501774510368705,
            -0.016642769798636436,
            0.03247038275003433,
            -0.007241302635520697,
            0.002798702334985137,
            -0.017607370391488075,
            0.020243041217327118,
            0.023761799558997154,
            0.00856593158096075,
            0.016438979655504227,
            0.02270209603011608,
            0.020637033507227898,
            0.002071854891255498,
            -0.017199791967868805,
            0.004792438354343176,
            -0.016357464715838432,
            0.00825345516204834,
            0.008776513859629631,
            -0.006633332464843988,
            -0.0329594761133194,
            0.014183714985847473,
            0.0027817199006676674,
            0.012220547534525394,
            -0.021234814077615738,
            0.0019427885999903083,
            -0.009598462842404842,
            0.012444715946912766,
            -0.016642769798636436,
            0.019210509955883026,
            0.017675301060080528,
            -0.0031536349561065435,
            0.004310137592256069,
            -0.020650619640946388,
            -0.005298514384776354,
            -0.021832596510648727,
            -0.006426146719604731,
            0.0053800297901034355,
            -0.012152617797255516,
            0.04393691197037697,
            0.03899163007736206,
            0.03260624036192894,
            -0.0015937999123707414,
            -0.02050117403268814,
            0.005006416700780392,
            -0.012315649539232254,
            -0.010495133697986603,
            0.01567816734313965,
            -0.005740057211369276,
            -0.052034128457307816,
            -0.016167260706424713,
            0.008891994133591652,
            0.003070421051234007,
            0.001690599718131125,
            -0.011554837226867676,
            0.0015428526094183326,
            -0.02624802477657795,
            -0.018585557118058205,
            -0.005899691954255104,
            -0.013090047053992748,
            -0.028557633981108665,
            -0.009408259764313698,
            -0.04668127000331879,
            -0.019536573439836502,
            -0.031709570437669754,
            0.010325309820473194,
            0.00373273529112339,
            -0.011989586986601353,
            -0.012770777568221092,
            0.0011378219351172447,
            -0.03497019410133362,
            0.005515889264643192,
            0.022213002666831017,
            -0.027525102719664574,
            -0.033910490572452545,
            0.028938040137290955,
            -0.01846328377723694,
            -0.01033210288733244,
            0.005774022080004215,
            -0.026682773604989052,
            0.013776137493550777,
            -0.03325836732983589,
            -0.016017816960811615,
            0.0003791324852500111,
            -0.016044987365603447,
            0.005030191969126463,
            0.030704211443662643,
            -0.010012833401560783,
            -0.009224848821759224,
            0.012220547534525394,
            0.0020344937220215797,
            0.017172621563076973,
            -0.00310948072001338,
            -0.02488943189382553,
            0.00974111445248127,
            0.00874934159219265,
            0.026587672531604767,
            -0.003061929950490594,
            0.02090875245630741,
            0.012573782354593277,
            0.006110273767262697,
            0.016357464715838432,
            -0.009122954681515694,
            0.0008754436275921762,
            -0.005172844510525465,
            0.025297008454799652,
            0.010732888244092464,
            0.00034325712476857007,
            -0.012940602377057076,
            -0.008294212631881237,
            0.017457924783229828,
            -8.363840606762096e-05,
            -0.016438979655504227,
            0.008219489827752113,
            -0.00042880602995865047,
            0.01461846474558115,
            -0.029508648440241814,
            -0.02325911819934845,
            -0.0005476829828694463,
            -0.008708584122359753,
            0.021044611930847168,
            -0.009068611077964306,
            0.02289229817688465,
            -0.034263726323843,
            0.0033591222018003464,
            0.003783682594075799,
            -0.01793343387544155,
            0.004975848365575075,
            0.0009900749428197742,
            -0.011765418574213982,
            0.006297080311924219,
            -0.008097216486930847,
            -0.012702848762273788,
            0.012410750612616539,
            -0.022362448275089264,
            0.0005455601494759321,
            0.028285915032029152,
            0.02999774180352688,
            0.020025666803121567,
            -0.0012032042723149061,
            -0.00856593158096075,
            0.014469020068645477,
            -0.010196243412792683,
            0.025595899671316147,
            -0.017159035429358482,
            0.0026017064228653908,
            0.01154804416000843,
            0.013110426254570484,
            -0.0015776666114106774,
            0.03545928746461868,
            0.015814026817679405,
            0.03244321048259735,
            0.010651372373104095,
            -0.016072159633040428,
            0.02287871204316616,
            -0.01431957446038723,
            -0.008878407999873161,
            -0.030731383711099625,
            0.0011514079524204135,
            0.0240606889128685,
            -0.00924522802233696,
            0.0040757800452411175,
            0.012899843975901604,
            -0.0057094888761639595,
            0.033883318305015564,
            0.016357464715838432,
            0.008362142369151115,
            0.022797197103500366,
            0.007805119268596172,
            0.019183339551091194,
            -0.014455433934926987,
            -0.014591293409466743,
            -0.0058079869486391544,
            -0.009184091351926327,
            -0.004938486963510513,
            -0.017254136502742767,
            -0.015080386772751808,
            0.0010206432780250907,
            -0.010617407038807869,
            0.01441467646509409,
            0.03024228848516941,
            -0.02490301802754402,
            0.021438604220747948,
            -0.0067929672077298164,
            0.034127864986658096,
            -0.0023045141715556383,
            -0.029916226863861084,
            0.03160088136792183,
            -0.006130652967840433,
            -0.027959851548075676,
            -0.0171318631619215,
            0.0032283575274050236,
            -0.02169673703610897,
            -0.004269379656761885,
            -0.018830103799700737,
            0.0016956944018602371,
            0.009007474407553673,
            -0.0019852446857839823,
            0.025120392441749573,
            0.00111829221714288,
            0.013293836265802383,
            0.007302439771592617,
            -0.01986263506114483,
            0.004432410933077335,
            0.004486754536628723,
            0.0019190132152289152,
            0.00785946287214756,
            -0.006490679923444986,
            -0.028965210542082787,
            0.02029738575220108,
            0.001125934300944209,
            -0.029209759086370468,
            -0.016642769798636436,
            -0.016466151922941208,
            -0.015977058559656143,
            0.038257990032434464,
            0.009462603367865086,
            0.01868066005408764,
            -0.019346369430422783,
            -5.301698547555134e-05,
            0.0292369294911623,
            0.007893427275121212,
            -0.006996755953878164,
            0.008511587977409363,
            0.004408635664731264,
            -0.028313087299466133,
            -0.012268098071217537,
            -0.01671069860458374,
            0.008436865173280239,
            0.04317609965801239,
            0.04755076766014099,
            -0.004174278117716312,
            0.019020307809114456,
            -0.012757192365825176,
            0.03635596111416817,
            -0.025758931413292885,
            -0.0389372855424881,
            -0.021194057539105415,
            -0.01649332419037819,
            0.0015802140114828944,
            -0.02664201706647873,
            0.006531437858939171,
            -0.022810783237218857,
            0.005254359915852547,
            0.01025058701634407,
            -0.0003099291061516851,
            -0.02626161091029644,
            0.033747460693120956,
            -0.016547666862607002,
            -0.0009994152933359146,
            0.013701414689421654,
            0.006076308898627758,
            0.010467962361872196,
            0.001453694887459278,
            -0.019427886232733727,
            -0.012254512868821621,
            0.010807610116899014,
            -0.0035323428455740213,
            -0.011588801629841328,
            -0.016561252996325493,
            -0.022579822689294815,
            -0.01753944158554077,
            0.00787304900586605,
            -0.02051476016640663,
            0.03939921036362648,
            0.001176881487481296,
            0.006545023526996374,
            0.011004606261849403,
            0.020025666803121567,
            0.0029600353445857763,
            0.02662843093276024,
            -0.002523587318137288,
            -0.01540644932538271,
            7.790684321662411e-05,
            -0.030378147959709167,
            0.004031626041978598,
            0.016574839130043983,
            0.011317082680761814,
            -0.002888709306716919,
            0.0012337726075202227,
            -0.004809420555830002,
            -0.04111103713512421,
            -0.005013209767639637,
            0.014469020068645477,
            -0.014699980616569519,
            0.012471887283027172,
            0.010196243412792683,
            -0.01827308163046837,
            -0.02385690063238144,
            -0.017403582111001015,
            -0.014047855511307716,
            0.01872141659259796,
            -0.007275267504155636,
            0.00365801271982491,
            0.004639596678316593,
            -0.017362823709845543,
            -0.0244275089353323,
            -0.015596652403473854,
            0.04627368971705437,
            -0.008212696760892868,
            -0.04032305255532265,
            -0.04727904871106148,
            -0.016765043139457703,
            0.01221375446766615,
            0.011486907489597797,
            0.03260624036192894,
            -0.020786479115486145,
            -0.001669371617026627,
            -0.0023147035390138626,
            -0.007574158255010843,
            -0.03445392847061157,
            0.0223081037402153,
            -0.04051325470209122,
            0.0038006650283932686,
            -0.01102498546242714,
            -0.00746547058224678,
            -0.027565861120820045,
            -0.019332783296704292,
            0.0354321151971817,
            0.002744358731433749,
            0.005644955672323704,
            0.02961733564734459,
            -0.028965210542082787,
            0.01846328377723694,
            0.019984908401966095,
            0.0072005451656877995,
            0.005634766072034836,
            0.012886258773505688,
            -0.028313087299466133,
            -0.011955621652305126,
            -0.009285985492169857,
            -0.010420411825180054,
            -0.03325836732983589,
            0.03165522590279579,
            -0.006307269912213087,
            0.0036274443846195936,
            -0.021126126870512962,
            0.01733565144240856,
            -0.01072609517723322,
            0.008925958536565304,
            -0.023394977673888206,
            -0.00310948072001338,
            -0.01711827702820301,
            0.0313563346862793,
            -0.0038855771999806166,
            -0.001333968946710229,
            0.024970946833491325,
            -0.012268098071217537,
            -0.0019478832837194204,
            0.0025490608531981707,
            -0.02308250218629837,
            0.016588425263762474,
            0.019006721675395966,
            -0.0026017064228653908,
            0.021520119160413742,
            0.005498906597495079,
            -0.01688731648027897,
            -0.007037513889372349,
            0.025609485805034637,
            -0.024509025737643242,
            0.01668352633714676,
            -0.0017101294361054897,
            -0.0278511643409729,
            -0.03143785148859024,
            -0.016832971945405006,
            -0.002584723988547921,
            0.008932751603424549,
            -0.01461846474558115,
            -0.0012813233770430088,
            0.012179790064692497,
            -0.0006381143466569483,
            0.009992454200983047,
            -0.013225906528532505,
            0.01889803446829319,
            -0.022960228845477104,
            -0.014808667823672295,
            0.008531966246664524,
            0.001946185017004609,
            0.03206280618906021,
            -0.017634542658925056,
            0.02999774180352688,
            -0.030079258605837822,
            -0.0076692597940564156,
            0.0005748548428528011,
            -0.02983471192419529,
            -0.0004895181627944112,
            0.004462979268282652,
            0.03673636540770531,
            0.02324553392827511,
            0.03184542804956436,
            -0.000315448414767161,
            0.01082798931747675,
            0.01501245703548193,
            -0.0043169306591153145,
            -0.006378596182912588,
            -0.028394602239131927,
            -0.003245339961722493,
            -0.022117899730801582,
            0.017172621563076973,
            -0.001829006359912455,
            -0.013633484952151775,
            -0.008260248228907585,
            0.0026662396267056465,
            0.003905955934897065,
            0.002542268019169569,
            0.0028343654703348875,
            0.006945808883756399,
            0.0010180958779528737,
            -0.005757039412856102,
            -0.03635596111416817,
            0.02325911819934845,
            -0.004198053851723671,
            -0.0004954620380885899,
            -0.042279426008462906,
            -0.025595899671316147,
            0.013484039343893528,
            0.011065742932260036,
            0.014903769828379154,
            0.012397164478898048,
            0.013721792958676815,
            -0.015243417583405972,
            0.0006584932561963797,
            -0.008531966246664524,
            -0.003461016807705164,
            -0.001688052318058908,
            -0.014890183694660664,
            -0.029454305768013,
            -0.016846558079123497,
            0.013966340571641922,
            0.028557633981108665,
            -0.021139713004231453,
            -0.003255529562011361,
            -0.010603821836411953,
            0.01628953404724598,
            0.008321384899318218,
            0.01709110476076603,
            -0.008606689050793648,
            0.004378067329525948,
            -0.0013467057142406702,
            -0.009768286719918251,
            -0.03638312965631485,
            -0.028693493455648422,
            -0.0037633036263287067,
            0.036301616579294205,
            0.0034525254741311073,
            -0.02328629046678543,
            0.026519743725657463,
            0.011969207786023617,
            0.0026203871238976717,
            -0.01569175347685814,
            0.004496944136917591,
            0.05230584740638733,
            0.02051476016640663,
            0.011887691915035248,
            0.029481476172804832,
            -0.0005655144923366606,
            0.0016761645674705505,
            0.01870783045887947,
            -0.0005846197018399835,
            -0.014346746727824211,
            -0.004975848365575075,
            -0.007594536989927292,
            0.006755605805665255,
            -0.008321384899318218,
            0.02429164946079254,
            0.02844894677400589,
            -0.018802933394908905,
            -0.01164993830025196,
            0.0006028758361935616,
            -0.00596422515809536,
            0.012471887283027172,
            0.013572348281741142,
            -0.01690090261399746,
            -0.00231130700558424,
            0.012397164478898048,
            -0.026981664821505547,
            0.016602011397480965,
            -0.006602764129638672,
            -0.005070949904620647,
            0.012132239528000355,
            0.006830328144133091,
            -0.012927016243338585,
            0.014428261667490005,
            0.020392486825585365,
            0.0012550007086247206,
            0.001064797630533576,
            0.02070496417582035,
            -0.015841199085116386,
            -0.027389243245124817,
            0.02861197665333748,
            0.030514007434248924,
            -0.014958113431930542,
            0.015134730376303196,
            0.008973509073257446,
            0.014958113431930542,
            0.01589554361999035,
            0.012763985432684422,
            -0.00374971772544086,
            0.027008837088942528,
            -0.035731006413698196,
            -0.008593102917075157,
            -0.013253078795969486,
            -0.024128619581460953,
            0.025568727403879166,
            -0.0280685406178236,
            0.003950110170990229,
            -0.0007968999561853707,
            -0.017362823709845543,
            0.0157460980117321,
            0.03200846165418625,
            0.013171562924981117,
            -0.0250796340405941,
            0.005325686186552048,
            -0.006157824769616127,
            0.01313080545514822,
            -0.009904146194458008,
            0.0034915851429104805,
            0.02168315090239048,
            -0.03043249249458313,
            -0.022158658131957054,
            0.010026419535279274,
            -0.0067623988725245,
            0.01692807301878929,
            -0.015460792928934097,
            -0.0007009492837823927,
            0.006840517744421959,
            0.028476117178797722,
            -0.04290438070893288,
            -0.027511516585946083,
            -0.005994793493300676,
            0.0066061606630682945,
            -0.01849045604467392,
            -0.005111707840114832,
            -0.006235943641513586,
            -0.01806929148733616,
            0.01493094116449356,
            -0.011466528289020061,
            0.005991396959871054,
            -0.011018192395567894,
            -0.003729338990524411,
            -0.009095782414078712,
            -0.0018952378304675221,
            -0.03757869452238083,
            -0.016955245286226273,
            0.0071801659651100636,
            0.016411809250712395,
            -0.006619746331125498,
            -0.009720736183226109,
            -0.010046797804534435,
            0.022593408823013306,
            -0.0049724518321454525,
            0.013667449355125427,
            -0.03423655405640602,
            -0.003525549778714776,
            0.0023265911731868982,
            0.007397541310638189,
            -0.011969207786023617,
            -0.042279426008462906,
            -0.004378067329525948,
            -0.018639901652932167,
            0.006735226605087519,
            -0.018599143251776695,
            -0.011344254948198795,
            0.006898257881402969,
            0.005896295420825481,
            -0.0025303801521658897,
            -0.013762551359832287,
            -0.00305683515034616,
            0.0001466856338083744,
            0.013803308829665184,
            -0.008348556235432625,
            0.017648128792643547,
            0.0006079705781303346,
            -0.012302063405513763,
            -0.020976681262254715,
            -0.029345616698265076,
            0.001504642190411687,
            -0.014849426224827766,
            -0.023394977673888206,
            -0.014672808349132538,
            0.031193304806947708,
            0.010685336776077747,
            -0.01571892574429512,
            -0.01691448874771595,
            -0.020840823650360107,
            -0.03855688124895096,
            -0.00805645901709795,
            -0.007723603397607803,
            0.017267722636461258,
            -0.0014180318685248494,
            0.017403582111001015,
            0.027593031525611877,
            0.02425089292228222,
            0.007105443626642227,
            -0.003943317569792271,
            0.0024964152835309505,
            0.000519237422849983,
            -0.0002857291838154197,
            -0.02801419608294964,
            -0.019061066210269928,
            -0.0009433733066543937,
            -0.047414910048246384,
            -0.000902615487575531,
            0.04279569163918495,
            -0.018667073920369148,
            0.017607370391488075,
            0.05630011111497879,
            -0.008993888273835182,
            -0.016425393521785736,
            -0.001525870175100863,
            0.015161902643740177,
            0.010196243412792683,
            0.02980753965675831,
            0.015977058559656143,
            -0.026207266375422478,
            -0.00731602543964982,
            0.026709945872426033,
            0.00895992387086153,
            -0.0329594761133194,
            -0.0018120239255949855,
            0.01653408259153366,
            -0.003362518735229969,
            0.003048344049602747,
            -0.019210509955883026,
            0.01650691032409668,
            -0.0030636282172054052,
            -0.008858028799295425,
            0.0046531823463737965,
            0.0147271528840065,
            -0.02967168018221855,
            -0.0028530461713671684,
            0.029101070016622543,
            -0.019400713965296745,
            -0.021397845819592476,
            0.022606994956731796,
            -0.01400709804147482,
            -0.0023690471425652504,
            0.016221605241298676,
            -0.014740738086402416,
            0.006843914277851582,
            -0.004707525949925184,
            0.0009951696265488863,
            -0.01847686991095543,
            -0.0004294428799767047,
            -0.029345616698265076,
            0.028122883290052414,
            -0.023150430992245674,
            0.030731383711099625,
            0.011106501333415508,
            -0.02942713350057602,
            0.006497472990304232,
            0.0005608443170785904,
            -0.020243041217327118,
            -0.014686394482851028,
            -0.014876597560942173,
            0.009299571625888348,
            -0.009007474407553673,
            0.014088613912463188,
            0.0007591140456497669,
            -0.01351121161133051,
            -0.015610238537192345,
            -0.00285983900539577,
            0.01832742430269718,
            -0.006847310811281204,
            -0.00011325149534968659,
            0.23715606331825256,
            -0.03877425566315651,
            0.010936676524579525,
            0.0009178996551781893,
            -0.019183339551091194,
            0.0034678096417337656,
            -0.008531966246664524,
            0.010950262658298016,
            -0.009170505218207836,
            -0.0037667001597583294,
            0.02168315090239048,
            0.002808891935274005,
            -0.010705715976655483,
            0.0050437781028449535,
            0.016411809250712395,
            -0.01331421546638012,
            -0.013090047053992748,
            -0.021139713004231453,
            -0.015542308799922466,
            0.005030191969126463,
            0.010006040334701538,
            0.020637033507227898,
            -0.013769344426691532,
            -0.02226734533905983,
            0.03228018060326576,
            -0.002003925386816263,
            0.0007862859056331217,
            -0.004143709782510996,
            0.0354321151971817,
            0.011446149088442326,
            -0.009041438810527325,
            0.005003020167350769,
            -0.000519237422849983,
            -0.0013653864152729511,
            -0.0029005969408899546,
            0.003051740350201726,
            0.006072912365198135,
            0.009428638033568859,
            0.019061066210269928,
            -0.0006410862552002072,
            0.04154578596353531,
            0.021166885271668434,
            -0.008708584122359753,
            -0.019781120121479034,
            0.010637786239385605,
            0.009795458056032658,
            -0.020759306848049164,
            -0.012302063405513763,
            -0.008008908480405807,
            0.029698852449655533,
            -0.032089974731206894,
            0.022905884310603142,
            0.02465846948325634,
            0.037306975573301315,
            -0.0007421316695399582,
            -0.01732206530869007,
            0.00954411830753088,
            0.0015487964265048504,
            0.03279644623398781,
            -0.012994945980608463,
            -0.017362823709845543,
            0.009292778559029102,
            -0.005828365683555603,
            0.023367807269096375,
            -0.0037395283579826355,
            0.02627519704401493,
            -0.038692738860845566,
            0.04312175512313843,
            0.02268850989639759,
            0.014102199114859104,
            0.008919165469706059,
            -0.00975470058619976,
            -0.013558762148022652,
            0.004568270407617092,
            -0.02624802477657795,
            -0.02490301802754402,
            0.016941659152507782,
            0.01709110476076603,
            0.015555894933640957,
            0.03448110073804855,
            0.002479432849213481,
            -0.019047480076551437,
            -0.027538688853383064,
            -0.02209072932600975,
            -0.018585557118058205,
            -0.04298589378595352,
            -0.0010792326647788286,
            -0.020025666803121567,
            0.0010299836285412312,
            -0.03521474078297615,
            0.01729489490389824,
            -0.032334521412849426,
            -0.006168013904243708,
            -0.013490832410752773,
            0.02327270433306694,
            0.010732888244092464,
            0.004731301683932543,
            0.018177980557084084,
            0.010848368518054485,
            -0.010026419535279274,
            -0.0349973663687706,
            0.056626174598932266,
            0.017580198124051094,
            -0.0033455363009124994,
            -0.014292402192950249,
            0.004659975413233042,
            0.0033828974701464176,
            0.0019954340532422066,
            0.012383579276502132,
            -0.01074647344648838,
            0.0038482157979160547,
            -0.01532493345439434,
            0.013354972936213017,
            -0.016221605241298676,
            0.0037157528568059206,
            0.0009289382142014802,
            0.011955621652305126,
            0.012369993142783642,
            0.03024228848516941,
            -0.018001362681388855,
            0.003722545923665166,
            -0.019414300099015236,
            -0.0002935835509561002,
            0.030079258605837822,
            0.0014358634361997247,
            -0.01493094116449356,
            0.003742924891412258,
            0.007234510034322739,
            -0.02310967445373535,
            0.0024318823125213385,
            -0.004102952312678099,
            -0.016004230827093124,
            0.01123556774109602,
            -0.02107178419828415,
            -0.03575817868113518,
            0.013993511907756329,
            0.009965282864868641,
            -0.004975848365575075,
            -0.017226964235305786,
            0.012118653394281864,
            0.00012758040975313634,
            0.004038419108837843,
            0.005233981180936098,
            -0.01830025389790535,
            -0.011758625507354736,
            -0.009584876708686352,
            0.013205528259277344,
            -0.015868371352553368,
            -0.006881275679916143,
            -0.008776513859629631,
            -0.01811004988849163,
            -0.028394602239131927,
            -0.0058079869486391544,
            -0.03524191305041313,
            0.020039252936840057,
            0.010603821836411953,
            -0.008389314636588097,
            -0.022376032546162605,
            0.017362823709845543,
            0.009109368547797203,
            -0.020840823650360107,
            -0.005729867611080408,
            0.0032860978972166777,
            -0.023924829438328743,
            -0.001446052803657949,
            -0.004181071184575558,
            -0.17389994859695435,
            -0.011317082680761814,
            0.016778629273176193,
            -0.040241535753011703,
            0.008898787200450897,
            -0.0012066008057445288,
            0.01709110476076603,
            -0.02171032316982746,
            -0.038502536714076996,
            0.012981359846889973,
            0.02085440792143345,
            0.00885123573243618,
            -0.027008837088942528,
            -0.017063932493329048,
            -0.017036762088537216,
            0.004619217477738857,
            0.0025337766855955124,
            0.021737493574619293,
            0.0030670245178043842,
            0.0015411543427035213,
            0.042034879326820374,
            -0.0326605848968029,
            0.017661714926362038,
            -0.014075027778744698,
            0.00945581030100584,
            -0.0028615372721105814,
            -0.002742660464718938,
            0.009985661134123802,
            -0.0026662396267056465,
            -0.03722545877099037,
            0.008599895983934402,
            0.006239340174943209,
            0.025691000744700432,
            -0.009184091351926327,
            0.0023147035390138626,
            0.017199791967868805,
            0.016438979655504227,
            -0.006888068746775389,
            -0.009347123093903065,
            0.0025949133560061455,
            0.031111789867281914,
            0.01929202675819397,
            0.016547666862607002,
            -0.01400709804147482,
            -0.019061066210269928,
            0.020786479115486145,
            0.020134354010224342,
            -0.014387504197657108,
            0.021723909303545952,
            -0.019726775586605072,
            0.00318420329131186,
            -0.00817193929105997,
            -0.0016048384131863713,
            0.013151183724403381,
            0.023965587839484215,
            0.009190884418785572,
            -0.0033795011695474386,
            0.01666994020342827,
            -0.014795082621276379,
            0.004391652997583151,
            -0.00019243202405050397,
            -0.024617712944746017,
            0.011330668814480305,
            -0.017145449295639992,
            -0.02725338377058506,
            -0.01262133289128542,
            -0.017756815999746323,
            0.0047245086170732975,
            0.0006156126619316638,
            0.011004606261849403,
            -0.015447206795215607,
            -0.013844067230820656,
            -0.0068608964793384075,
            0.00015676894690841436,
            0.010474755428731441,
            0.032905131578445435,
            -0.009238434955477715,
            -0.005444562993943691,
            0.019821878522634506,
            0.009978868998587132,
            -0.013185149058699608,
            0.032714929431676865,
            -0.018599143251776695,
            0.018965963274240494,
            -0.006385388784110546,
            0.0009892258094623685,
            0.0009255417389795184,
            0.018558386713266373,
            -0.010624200105667114,
            -0.01869424618780613,
            0.005186430178582668,
            -0.03024228848516941,
            0.013205528259277344,
            0.0012210358399897814,
            0.0019852446857839823,
            0.011371427215635777,
            0.02310967445373535,
            0.0039161453023552895,
            0.004201449919492006,
            -0.015474379062652588,
            0.0053902193903923035,
            0.003841422963887453,
            0.009319950826466084,
            0.029399961233139038,
            0.011588801629841328,
            0.027959851548075676,
            -0.024413922801613808,
            0.013816894963383675,
            0.029726022854447365,
            0.0038244405295699835,
            -0.02429164946079254,
            -0.016642769798636436,
            0.015053214505314827,
            0.020650619640946388,
            0.013796515762805939,
            0.015352105721831322,
            -0.0035459287464618683,
            -0.017947018146514893,
            0.011208395473659039,
            -0.014061441645026207,
            0.03415503725409508,
            0.0034763009753078222,
            -0.01793343387544155,
            0.02168315090239048,
            -0.02207714319229126,
            -0.04717036336660385,
            -0.09417769312858582,
            -0.038094960153102875,
            -0.019658846780657768,
            0.0047143190167844296,
            -0.003844819264486432,
            0.016574839130043983,
            0.02688656374812126,
            0.018830103799700737,
            0.00039420436951331794,
            0.03282361850142479,
            -0.019047480076551437,
            -0.013558762148022652,
            0.002316401805728674,
            -0.02687297761440277,
            0.008104009553790092,
            0.0024607523810118437,
            0.0016260665142908692,
            0.004710922483354807,
            -0.017960604280233383,
            0.01969960518181324,
            0.0030789123848080635,
            -0.030785726383328438,
            0.032714929431676865,
            -0.03184542804956436,
            0.031138960272073746,
            -0.026207266375422478,
            -0.031763914972543716,
            0.008307798765599728,
            -0.0024641486816108227,
            -0.016778629273176193,
            -0.012336027808487415,
            -0.02926410175859928,
            9.860416321316734e-05,
            -0.014944527298212051,
            0.0032657189294695854,
            -0.008756134659051895,
            -0.016194432973861694,
            -0.027348484843969345,
            -0.003950110170990229,
            -0.029970571398735046,
            0.0020327954553067684,
            0.0007306685438379645,
            0.0004997076466679573,
            -0.01592271402478218,
            -0.012132239528000355,
            -0.04002416133880615,
            -0.010963848792016506,
            0.014387504197657108,
            0.011317082680761814,
            -0.007295646704733372,
            -0.014876597560942173,
            -0.008919165469706059,
            -0.03399200737476349,
            -0.007064685691148043,
            0.01313080545514822,
            0.012852293439209461,
            0.0032708137296140194,
            0.004452789667993784,
            -0.011147258803248405,
            -0.02171032316982746,
            0.007954563945531845,
            -0.007397541310638189,
            -0.01892520673573017,
            0.0072888536378741264,
            0.0010936676990240812,
            -0.004371274262666702,
            -0.00934033002704382,
            -0.025568727403879166,
            0.012600953690707684,
            -0.0306226946413517,
            0.0029464494436979294,
            0.014469020068645477,
            0.0035798936150968075,
            0.019604502245783806,
            -0.021207643672823906,
            -0.010705715976655483,
            -0.015515136532485485,
            0.021574463695287704,
            0.005845348350703716,
            -0.01609933190047741,
            -0.03228018060326576,
            -0.022729268297553062,
            0.01729489490389824,
            -0.025025291368365288,
            -0.006803156342357397,
            -0.00766246672719717,
            -0.024644885212183,
            -0.00037127811810933053,
            -0.0038040615618228912,
            -0.029698852449655533,
            0.012668883427977562,
            0.03045966476202011,
            -0.0013450074475258589,
            -0.013191942125558853,
            -0.006235943641513586,
            0.0068608964793384075,
            -0.025758931413292885,
            0.004374670796096325,
            0.006096688099205494,
            0.045458536595106125,
            -0.01364027801901102,
            -0.00641595758497715,
            -0.052822113037109375,
            0.008803685195744038,
            0.0031909963581711054,
            -0.022946642711758614,
            0.009218056686222553,
            0.023775383830070496,
            0.037497177720069885,
            0.009421844966709614,
            -0.013470453210175037,
            0.030731383711099625,
            -0.01073968131095171,
            0.02168315090239048,
            0.013049289584159851,
            0.0001267312909476459,
            -0.012994945980608463,
            -0.024821501225233078,
            0.02385690063238144,
            -0.021968455985188484,
            0.015814026817679405,
            0.024726400151848793,
            -0.003197789192199707,
            -0.011378219351172447,
            0.012859086506068707,
            0.01253981702029705,
            -0.0019173149485141039,
            -0.0010130011942237616,
            -0.015433620661497116,
            0.014686394482851028,
            -0.01733565144240856,
            -0.02268850989639759,
            -0.002095630392432213,
            -0.019590916112065315,
            0.006246133241802454,
            0.00995169673115015,
            -0.010889125987887383,
            -0.002598309889435768,
            0.0007387351361103356,
            0.008830857463181019,
            0.02268850989639759,
            0.009462603367865086,
            -0.01194203644990921,
            -0.026112165302038193,
            0.02882935293018818,
            -0.025976305827498436,
            0.0027494532987475395,
            -0.0033964836038649082,
            0.004632803611457348,
            -0.002212808933109045,
            -0.003817647462710738,
            -0.006483886856585741,
            0.024155789986252785,
            0.02108537033200264,
            0.009435431100428104,
            -0.022973814979195595,
            -0.01631670631468296,
            0.005447959527373314,
            0.013551969081163406,
            0.026152923703193665,
            -0.026913736015558243,
            -0.009809044189751148,
            0.017607370391488075,
            0.007825498469173908,
            0.002411503344774246,
            -0.00325383129529655,
            0.0016421998152509332,
            0.012859086506068707,
            0.019237682223320007,
            -0.0017186206532642245,
            -0.009197677485644817,
            -0.034535445272922516,
            -0.0014018985675647855,
            -0.0013008532114326954,
            0.012071102857589722,
            0.011575215496122837,
            0.03564948961138725,
            0.006229150574654341,
            0.00017884607950691134,
            0.00716658029705286,
            -0.01463205087929964,
            0.03662768006324768,
            0.015596652403473854,
            -0.028965210542082787,
            -0.01342969574034214,
            -0.017770402133464813,
            0.004829799756407738,
            -0.0027358673978596926,
            0.006796363741159439,
            -0.0024556575808674097,
            -0.01402068417519331,
            -0.0037870791275054216,
            0.005712885409593582,
            0.007988529279828072,
            0.002396219177171588,
            0.0053800297901034355,
            -0.004089366178959608,
            0.009809044189751148,
            0.010984227992594242,
            0.00021461529831867665,
            0.012607746757566929,
            -0.013592727482318878,
            0.001277926960028708,
            0.0013322706799954176,
            -0.004595442209392786,
            -0.005634766072034836,
            -0.014536949805915356,
            0.020990267395973206,
            -0.019020307809114456,
            -0.028313087299466133,
            0.00040587977855466306,
            0.0034151640720665455,
            0.010298137553036213,
            0.020243041217327118,
            -0.015257003717124462,
            0.0015292667085304856,
            -0.02861197665333748,
            0.03423655405640602,
            0.013735379092395306,
            -0.017457924783229828,
            -0.008498001843690872,
            0.007995322346687317,
            0.007085064426064491,
            -0.00015952858666423708,
            0.028557633981108665,
            0.0013874635333195329,
            0.01592271402478218,
            0.021642392501235008,
            0.04121972247958183,
            -0.010481548495590687,
            0.00894633773714304,
            0.008287419565021992,
            -0.013253078795969486,
            -0.017145449295639992,
            -0.005529474932700396,
            -0.019020307809114456,
            -0.00023456964117940515,
            0.0033217607997357845,
            0.0027137903962284327,
            0.022172244265675545,
            -0.0034253536723554134,
            0.0892324149608612,
            0.02586761862039566,
            0.004996227100491524,
            0.0027341691311448812,
            -0.0010401731124147773,
            0.025813274085521698,
            0.019210509955883026,
            0.014346746727824211,
            -0.0023418753407895565,
            0.00481961015611887,
            0.02449543960392475,
            0.01633029244840145,
            0.006545023526996374,
            -0.012492266483604908,
            -0.014795082621276379,
            0.006239340174943209,
            0.011595594696700573,
            -0.0030924982856959105,
            0.01787908934056759,
            -0.013103633187711239,
            0.046056315302848816,
            -0.026492571458220482,
            0.007458677981048822,
            0.011955621652305126,
            -0.03461695834994316,
            -0.006049137096852064,
            0.010760059580206871,
            0.015664581209421158,
            -0.018667073920369148,
            -0.039073146879673004,
            0.019658846780657768,
            -0.002095630392432213,
            -0.028150055557489395,
            -0.012845500372350216,
            0.022213002666831017,
            -0.002409805078059435,
            -0.001168390386737883,
            -0.005818176083266735,
            0.018245909363031387,
            0.010617407038807869,
            0.01501245703548193,
            0.01690090261399746,
            -0.004459582734853029,
            -0.03497019410133362,
            -0.01194203644990921,
            -0.0059676216915249825,
            -0.02008001133799553,
            -0.027348484843969345,
            -0.016588425263762474
          ],
          "start_time": 1694832910.037893,
          "end_time": 1694832910.628216,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "What is the purpose of BERT?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.022036384791135788,
                      -0.0164797380566597,
                      0.0005999039276503026,
                      -0.007227716967463493,
                      0.003868594765663147,
                      0.01381010189652443,
                      -0.006500869523733854,
                      -0.011711074970662594,
                      -0.01653408259153366,
                      -0.015121144242584705,
                      0.0027137903962284327,
                      0.027362070977687836,
                      -0.025758931413292885,
                      0.00944222416728735,
                      3.94045164284762e-05,
                      0.0016226699808612466,
                      0.016561252996325493,
                      0.015487965196371078,
                      0.004052004776895046,
                      0.0009892258094623685,
                      -0.005791004281491041,
                      -0.014170128852128983,
                      -0.019210509955883026,
                      0.020990267395973206,
                      -0.006324252113699913,
                      -0.02627519704401493,
                      0.05154503509402275,
                      -0.033502914011478424,
                      0.00033030801569111645,
                      -0.013558762148022652,
                      0.027144696563482285,
                      0.030269460752606392,
                      -0.010019626468420029,
                      -0.008131181821227074,
                      -0.013069668784737587,
                      0.0003339167742524296,
                      0.01770247146487236,
                      -0.002297721104696393,
                      0.017009589821100235,
                      -0.01224092673510313,
                      0.015800440683960915,
                      0.01892520673573017,
                      0.013878031633794308,
                      -0.001744094304740429,
                      -0.0005765530513599515,
                      0.003309873165562749,
                      0.01252623088657856,
                      -0.004153899382799864,
                      0.0005892898770980537,
                      -0.023788969963788986,
                      0.018653487786650658,
                      0.01949581503868103,
                      -0.02384331449866295,
                      -0.002343573607504368,
                      -0.03442675620317459,
                      -0.0030534386169165373,
                      -0.0077575682662427425,
                      0.02582686021924019,
                      0.009068611077964306,
                      -0.02248472161591053,
                      0.010753266513347626,
                      0.0006746265571564436,
                      -0.015664581209421158,
                      0.019156167283654213,
                      0.0048841433599591255,
                      -0.009299571625888348,
                      -0.022946642711758614,
                      0.029101070016622543,
                      0.010603821836411953,
                      0.008885201066732407,
                      0.007628501858562231,
                      0.020623447373509407,
                      0.019455058500170708,
                      -0.011086122132837772,
                      0.03654616326093674,
                      -0.002844554837793112,
                      -0.01502604316920042,
                      -0.005852140951901674,
                      0.008036079816520214,
                      0.0004491849394980818,
                      0.006405767984688282,
                      0.02306891605257988,
                      -0.02109895646572113,
                      0.020922338590025902,
                      0.013062875717878342,
                      -0.007594536989927292,
                      0.013103633187711239,
                      0.01690090261399746,
                      -0.019577331840991974,
                      -0.0003455921832937747,
                      0.006201978772878647,
                      0.002939656376838684,
                      0.03314967826008797,
                      0.004282965790480375,
                      0.000431353400927037,
                      0.03024228848516941,
                      -0.004602235276252031,
                      0.029101070016622543,
                      0.0016498418990522623,
                      -0.035106055438518524,
                      0.009027853608131409,
                      -0.0075266072526574135,
                      -0.007940978743135929,
                      -0.01224092673510313,
                      -0.019224096089601517,
                      0.0021533705294132233,
                      0.00983621645718813,
                      -0.010372860357165337,
                      0.028150055557489395,
                      -0.008076838217675686,
                      -0.007716810330748558,
                      0.03203563392162323,
                      -0.0034202588722109795,
                      -0.020215868949890137,
                      0.005529474932700396,
                      0.021533705294132233,
                      0.0010299836285412312,
                      0.0005413145408965647,
                      0.017457924783229828,
                      0.020732134580612183,
                      0.03328553959727287,
                      -0.002212808933109045,
                      0.0011565026361495256,
                      -0.01827308163046837,
                      0.022348862141370773,
                      -0.008137974888086319,
                      -0.021615220233798027,
                      -0.019998494535684586,
                      0.008701791055500507,
                      -0.029481476172804832,
                      0.006055930163711309,
                      -0.0036444268189370632,
                      0.0013798214495182037,
                      -0.023816142231225967,
                      -0.002878519706428051,
                      0.023191189393401146,
                      -0.00796135701239109,
                      -0.010107934474945068,
                      -0.026424642652273178,
                      -0.01768888533115387,
                      0.016819385811686516,
                      0.022593408823013306,
                      -0.023177603259682655,
                      -0.03518756851553917,
                      -0.026601258665323257,
                      0.02546004019677639,
                      0.0007625105208717287,
                      0.02842177450656891,
                      0.010114727541804314,
                      0.002683222061023116,
                      0.0042456043884158134,
                      -0.031138960272073746,
                      -0.018055707216262817,
                      0.009591669775545597,
                      0.0022875317372381687,
                      0.013979925774037838,
                      -0.00032075541093945503,
                      0.0026458606589585543,
                      -0.000471262086648494,
                      -0.00011494974023662508,
                      0.0030279650818556547,
                      0.014550535008311272,
                      0.011554837226867676,
                      -0.009598462842404842,
                      0.03760586678981781,
                      0.04051325470209122,
                      -0.004279569257050753,
                      -0.0026611448265612125,
                      0.012397164478898048,
                      0.003573100548237562,
                      -0.008688204921782017,
                      0.01570533961057663,
                      -0.026981664821505547,
                      0.004629407078027725,
                      0.015433620661497116,
                      0.00746547058224678,
                      0.017430752515792847,
                      0.007805119268596172,
                      0.0007680298294872046,
                      -0.013361766003072262,
                      -0.0051320865750312805,
                      0.006398974917829037,
                      0.03399200737476349,
                      0.005003020167350769,
                      -0.015257003717124462,
                      -0.002204317832365632,
                      0.003770096693187952,
                      -0.035350602120161057,
                      0.0028004006016999483,
                      -0.044643379747867584,
                      0.018205150961875916,
                      0.019061066210269928,
                      0.011711074970662594,
                      -0.004673561546951532,
                      -0.6364738345146179,
                      -0.014455433934926987,
                      0.016629183664917946,
                      -0.00835534930229187,
                      0.021329917013645172,
                      -0.013083254918456078,
                      -0.010352482087910175,
                      -0.00566193787381053,
                      -0.01171786803752184,
                      0.03263341262936592,
                      0.005447959527373314,
                      -0.012607746757566929,
                      -0.005906485021114349,
                      -0.01502604316920042,
                      -0.0013458565808832645,
                      -0.022511892020702362,
                      -0.00936070829629898,
                      -0.00402143644168973,
                      0.01869424618780613,
                      -0.003973885904997587,
                      -0.02446826733648777,
                      0.02124840021133423,
                      -0.04380105063319206,
                      0.005852140951901674,
                      -0.01752585545182228,
                      0.007132615428417921,
                      0.002350366674363613,
                      -0.009313157759606838,
                      0.007295646704733372,
                      -0.007268474902957678,
                      -0.01323269959539175,
                      0.03336705267429352,
                      0.03956224024295807,
                      -0.015284175984561443,
                      0.031709570437669754,
                      0.011860520578920841,
                      -0.002382633276283741,
                      0.015759684145450592,
                      -0.031193304806947708,
                      0.0256638303399086,
                      -0.00866782572120428,
                      -0.019618088379502296,
                      0.02329987660050392,
                      -0.007010342087596655,
                      0.03217149153351784,
                      0.007248095702379942,
                      0.01232923474162817,
                      0.021425018087029457,
                      0.0018884448800235987,
                      0.007431505713611841,
                      -0.027606617659330368,
                      0.008722169324755669,
                      0.0014333160361275077,
                      -0.008464036509394646,
                      0.014401090331375599,
                      -0.0018341010436415672,
                      0.022837955504655838,
                      -0.014061441645026207,
                      0.0002553731028456241,
                      -0.022606994956731796,
                      0.011643145233392715,
                      0.034345243126153946,
                      -0.010902712121605873,
                      -0.02567741461098194,
                      -0.011004606261849403,
                      0.015229832381010056,
                      0.0035323428455740213,
                      0.012770777568221092,
                      0.011887691915035248,
                      -0.04477923735976219,
                      0.010481548495590687,
                      0.024984532967209816,
                      0.0006393880466930568,
                      -0.0006792966742068529,
                      0.02008001133799553,
                      0.017743229866027832,
                      -0.00228923000395298,
                      -0.015121144242584705,
                      -0.0004844234499614686,
                      0.022973814979195595,
                      -0.009252021089196205,
                      -0.005587215535342693,
                      0.012607746757566929,
                      -0.0028190813027322292,
                      -0.018748588860034943,
                      0.017797574400901794,
                      -0.006918637081980705,
                      -0.02309608832001686,
                      0.005352857988327742,
                      0.026207266375422478,
                      -0.0012125446228310466,
                      0.02782399207353592,
                      -0.021601635962724686,
                      -0.03942637890577316,
                      0.02248472161591053,
                      0.012369993142783642,
                      0.003501774510368705,
                      -0.016642769798636436,
                      0.03247038275003433,
                      -0.007241302635520697,
                      0.002798702334985137,
                      -0.017607370391488075,
                      0.020243041217327118,
                      0.023761799558997154,
                      0.00856593158096075,
                      0.016438979655504227,
                      0.02270209603011608,
                      0.020637033507227898,
                      0.002071854891255498,
                      -0.017199791967868805,
                      0.004792438354343176,
                      -0.016357464715838432,
                      0.00825345516204834,
                      0.008776513859629631,
                      -0.006633332464843988,
                      -0.0329594761133194,
                      0.014183714985847473,
                      0.0027817199006676674,
                      0.012220547534525394,
                      -0.021234814077615738,
                      0.0019427885999903083,
                      -0.009598462842404842,
                      0.012444715946912766,
                      -0.016642769798636436,
                      0.019210509955883026,
                      0.017675301060080528,
                      -0.0031536349561065435,
                      0.004310137592256069,
                      -0.020650619640946388,
                      -0.005298514384776354,
                      -0.021832596510648727,
                      -0.006426146719604731,
                      0.0053800297901034355,
                      -0.012152617797255516,
                      0.04393691197037697,
                      0.03899163007736206,
                      0.03260624036192894,
                      -0.0015937999123707414,
                      -0.02050117403268814,
                      0.005006416700780392,
                      -0.012315649539232254,
                      -0.010495133697986603,
                      0.01567816734313965,
                      -0.005740057211369276,
                      -0.052034128457307816,
                      -0.016167260706424713,
                      0.008891994133591652,
                      0.003070421051234007,
                      0.001690599718131125,
                      -0.011554837226867676,
                      0.0015428526094183326,
                      -0.02624802477657795,
                      -0.018585557118058205,
                      -0.005899691954255104,
                      -0.013090047053992748,
                      -0.028557633981108665,
                      -0.009408259764313698,
                      -0.04668127000331879,
                      -0.019536573439836502,
                      -0.031709570437669754,
                      0.010325309820473194,
                      0.00373273529112339,
                      -0.011989586986601353,
                      -0.012770777568221092,
                      0.0011378219351172447,
                      -0.03497019410133362,
                      0.005515889264643192,
                      0.022213002666831017,
                      -0.027525102719664574,
                      -0.033910490572452545,
                      0.028938040137290955,
                      -0.01846328377723694,
                      -0.01033210288733244,
                      0.005774022080004215,
                      -0.026682773604989052,
                      0.013776137493550777,
                      -0.03325836732983589,
                      -0.016017816960811615,
                      0.0003791324852500111,
                      -0.016044987365603447,
                      0.005030191969126463,
                      0.030704211443662643,
                      -0.010012833401560783,
                      -0.009224848821759224,
                      0.012220547534525394,
                      0.0020344937220215797,
                      0.017172621563076973,
                      -0.00310948072001338,
                      -0.02488943189382553,
                      0.00974111445248127,
                      0.00874934159219265,
                      0.026587672531604767,
                      -0.003061929950490594,
                      0.02090875245630741,
                      0.012573782354593277,
                      0.006110273767262697,
                      0.016357464715838432,
                      -0.009122954681515694,
                      0.0008754436275921762,
                      -0.005172844510525465,
                      0.025297008454799652,
                      0.010732888244092464,
                      0.00034325712476857007,
                      -0.012940602377057076,
                      -0.008294212631881237,
                      0.017457924783229828,
                      -8.363840606762096e-05,
                      -0.016438979655504227,
                      0.008219489827752113,
                      -0.00042880602995865047,
                      0.01461846474558115,
                      -0.029508648440241814,
                      -0.02325911819934845,
                      -0.0005476829828694463,
                      -0.008708584122359753,
                      0.021044611930847168,
                      -0.009068611077964306,
                      0.02289229817688465,
                      -0.034263726323843,
                      0.0033591222018003464,
                      0.003783682594075799,
                      -0.01793343387544155,
                      0.004975848365575075,
                      0.0009900749428197742,
                      -0.011765418574213982,
                      0.006297080311924219,
                      -0.008097216486930847,
                      -0.012702848762273788,
                      0.012410750612616539,
                      -0.022362448275089264,
                      0.0005455601494759321,
                      0.028285915032029152,
                      0.02999774180352688,
                      0.020025666803121567,
                      -0.0012032042723149061,
                      -0.00856593158096075,
                      0.014469020068645477,
                      -0.010196243412792683,
                      0.025595899671316147,
                      -0.017159035429358482,
                      0.0026017064228653908,
                      0.01154804416000843,
                      0.013110426254570484,
                      -0.0015776666114106774,
                      0.03545928746461868,
                      0.015814026817679405,
                      0.03244321048259735,
                      0.010651372373104095,
                      -0.016072159633040428,
                      0.02287871204316616,
                      -0.01431957446038723,
                      -0.008878407999873161,
                      -0.030731383711099625,
                      0.0011514079524204135,
                      0.0240606889128685,
                      -0.00924522802233696,
                      0.0040757800452411175,
                      0.012899843975901604,
                      -0.0057094888761639595,
                      0.033883318305015564,
                      0.016357464715838432,
                      0.008362142369151115,
                      0.022797197103500366,
                      0.007805119268596172,
                      0.019183339551091194,
                      -0.014455433934926987,
                      -0.014591293409466743,
                      -0.0058079869486391544,
                      -0.009184091351926327,
                      -0.004938486963510513,
                      -0.017254136502742767,
                      -0.015080386772751808,
                      0.0010206432780250907,
                      -0.010617407038807869,
                      0.01441467646509409,
                      0.03024228848516941,
                      -0.02490301802754402,
                      0.021438604220747948,
                      -0.0067929672077298164,
                      0.034127864986658096,
                      -0.0023045141715556383,
                      -0.029916226863861084,
                      0.03160088136792183,
                      -0.006130652967840433,
                      -0.027959851548075676,
                      -0.0171318631619215,
                      0.0032283575274050236,
                      -0.02169673703610897,
                      -0.004269379656761885,
                      -0.018830103799700737,
                      0.0016956944018602371,
                      0.009007474407553673,
                      -0.0019852446857839823,
                      0.025120392441749573,
                      0.00111829221714288,
                      0.013293836265802383,
                      0.007302439771592617,
                      -0.01986263506114483,
                      0.004432410933077335,
                      0.004486754536628723,
                      0.0019190132152289152,
                      0.00785946287214756,
                      -0.006490679923444986,
                      -0.028965210542082787,
                      0.02029738575220108,
                      0.001125934300944209,
                      -0.029209759086370468,
                      -0.016642769798636436,
                      -0.016466151922941208,
                      -0.015977058559656143,
                      0.038257990032434464,
                      0.009462603367865086,
                      0.01868066005408764,
                      -0.019346369430422783,
                      -5.301698547555134e-05,
                      0.0292369294911623,
                      0.007893427275121212,
                      -0.006996755953878164,
                      0.008511587977409363,
                      0.004408635664731264,
                      -0.028313087299466133,
                      -0.012268098071217537,
                      -0.01671069860458374,
                      0.008436865173280239,
                      0.04317609965801239,
                      0.04755076766014099,
                      -0.004174278117716312,
                      0.019020307809114456,
                      -0.012757192365825176,
                      0.03635596111416817,
                      -0.025758931413292885,
                      -0.0389372855424881,
                      -0.021194057539105415,
                      -0.01649332419037819,
                      0.0015802140114828944,
                      -0.02664201706647873,
                      0.006531437858939171,
                      -0.022810783237218857,
                      0.005254359915852547,
                      0.01025058701634407,
                      -0.0003099291061516851,
                      -0.02626161091029644,
                      0.033747460693120956,
                      -0.016547666862607002,
                      -0.0009994152933359146,
                      0.013701414689421654,
                      0.006076308898627758,
                      0.010467962361872196,
                      0.001453694887459278,
                      -0.019427886232733727,
                      -0.012254512868821621,
                      0.010807610116899014,
                      -0.0035323428455740213,
                      -0.011588801629841328,
                      -0.016561252996325493,
                      -0.022579822689294815,
                      -0.01753944158554077,
                      0.00787304900586605,
                      -0.02051476016640663,
                      0.03939921036362648,
                      0.001176881487481296,
                      0.006545023526996374,
                      0.011004606261849403,
                      0.020025666803121567,
                      0.0029600353445857763,
                      0.02662843093276024,
                      -0.002523587318137288,
                      -0.01540644932538271,
                      7.790684321662411e-05,
                      -0.030378147959709167,
                      0.004031626041978598,
                      0.016574839130043983,
                      0.011317082680761814,
                      -0.002888709306716919,
                      0.0012337726075202227,
                      -0.004809420555830002,
                      -0.04111103713512421,
                      -0.005013209767639637,
                      0.014469020068645477,
                      -0.014699980616569519,
                      0.012471887283027172,
                      0.010196243412792683,
                      -0.01827308163046837,
                      -0.02385690063238144,
                      -0.017403582111001015,
                      -0.014047855511307716,
                      0.01872141659259796,
                      -0.007275267504155636,
                      0.00365801271982491,
                      0.004639596678316593,
                      -0.017362823709845543,
                      -0.0244275089353323,
                      -0.015596652403473854,
                      0.04627368971705437,
                      -0.008212696760892868,
                      -0.04032305255532265,
                      -0.04727904871106148,
                      -0.016765043139457703,
                      0.01221375446766615,
                      0.011486907489597797,
                      0.03260624036192894,
                      -0.020786479115486145,
                      -0.001669371617026627,
                      -0.0023147035390138626,
                      -0.007574158255010843,
                      -0.03445392847061157,
                      0.0223081037402153,
                      -0.04051325470209122,
                      0.0038006650283932686,
                      -0.01102498546242714,
                      -0.00746547058224678,
                      -0.027565861120820045,
                      -0.019332783296704292,
                      0.0354321151971817,
                      0.002744358731433749,
                      0.005644955672323704,
                      0.02961733564734459,
                      -0.028965210542082787,
                      0.01846328377723694,
                      0.019984908401966095,
                      0.0072005451656877995,
                      0.005634766072034836,
                      0.012886258773505688,
                      -0.028313087299466133,
                      -0.011955621652305126,
                      -0.009285985492169857,
                      -0.010420411825180054,
                      -0.03325836732983589,
                      0.03165522590279579,
                      -0.006307269912213087,
                      0.0036274443846195936,
                      -0.021126126870512962,
                      0.01733565144240856,
                      -0.01072609517723322,
                      0.008925958536565304,
                      -0.023394977673888206,
                      -0.00310948072001338,
                      -0.01711827702820301,
                      0.0313563346862793,
                      -0.0038855771999806166,
                      -0.001333968946710229,
                      0.024970946833491325,
                      -0.012268098071217537,
                      -0.0019478832837194204,
                      0.0025490608531981707,
                      -0.02308250218629837,
                      0.016588425263762474,
                      0.019006721675395966,
                      -0.0026017064228653908,
                      0.021520119160413742,
                      0.005498906597495079,
                      -0.01688731648027897,
                      -0.007037513889372349,
                      0.025609485805034637,
                      -0.024509025737643242,
                      0.01668352633714676,
                      -0.0017101294361054897,
                      -0.0278511643409729,
                      -0.03143785148859024,
                      -0.016832971945405006,
                      -0.002584723988547921,
                      0.008932751603424549,
                      -0.01461846474558115,
                      -0.0012813233770430088,
                      0.012179790064692497,
                      -0.0006381143466569483,
                      0.009992454200983047,
                      -0.013225906528532505,
                      0.01889803446829319,
                      -0.022960228845477104,
                      -0.014808667823672295,
                      0.008531966246664524,
                      0.001946185017004609,
                      0.03206280618906021,
                      -0.017634542658925056,
                      0.02999774180352688,
                      -0.030079258605837822,
                      -0.0076692597940564156,
                      0.0005748548428528011,
                      -0.02983471192419529,
                      -0.0004895181627944112,
                      0.004462979268282652,
                      0.03673636540770531,
                      0.02324553392827511,
                      0.03184542804956436,
                      -0.000315448414767161,
                      0.01082798931747675,
                      0.01501245703548193,
                      -0.0043169306591153145,
                      -0.006378596182912588,
                      -0.028394602239131927,
                      -0.003245339961722493,
                      -0.022117899730801582,
                      0.017172621563076973,
                      -0.001829006359912455,
                      -0.013633484952151775,
                      -0.008260248228907585,
                      0.0026662396267056465,
                      0.003905955934897065,
                      0.002542268019169569,
                      0.0028343654703348875,
                      0.006945808883756399,
                      0.0010180958779528737,
                      -0.005757039412856102,
                      -0.03635596111416817,
                      0.02325911819934845,
                      -0.004198053851723671,
                      -0.0004954620380885899,
                      -0.042279426008462906,
                      -0.025595899671316147,
                      0.013484039343893528,
                      0.011065742932260036,
                      0.014903769828379154,
                      0.012397164478898048,
                      0.013721792958676815,
                      -0.015243417583405972,
                      0.0006584932561963797,
                      -0.008531966246664524,
                      -0.003461016807705164,
                      -0.001688052318058908,
                      -0.014890183694660664,
                      -0.029454305768013,
                      -0.016846558079123497,
                      0.013966340571641922,
                      0.028557633981108665,
                      -0.021139713004231453,
                      -0.003255529562011361,
                      -0.010603821836411953,
                      0.01628953404724598,
                      0.008321384899318218,
                      0.01709110476076603,
                      -0.008606689050793648,
                      0.004378067329525948,
                      -0.0013467057142406702,
                      -0.009768286719918251,
                      -0.03638312965631485,
                      -0.028693493455648422,
                      -0.0037633036263287067,
                      0.036301616579294205,
                      0.0034525254741311073,
                      -0.02328629046678543,
                      0.026519743725657463,
                      0.011969207786023617,
                      0.0026203871238976717,
                      -0.01569175347685814,
                      0.004496944136917591,
                      0.05230584740638733,
                      0.02051476016640663,
                      0.011887691915035248,
                      0.029481476172804832,
                      -0.0005655144923366606,
                      0.0016761645674705505,
                      0.01870783045887947,
                      -0.0005846197018399835,
                      -0.014346746727824211,
                      -0.004975848365575075,
                      -0.007594536989927292,
                      0.006755605805665255,
                      -0.008321384899318218,
                      0.02429164946079254,
                      0.02844894677400589,
                      -0.018802933394908905,
                      -0.01164993830025196,
                      0.0006028758361935616,
                      -0.00596422515809536,
                      0.012471887283027172,
                      0.013572348281741142,
                      -0.01690090261399746,
                      -0.00231130700558424,
                      0.012397164478898048,
                      -0.026981664821505547,
                      0.016602011397480965,
                      -0.006602764129638672,
                      -0.005070949904620647,
                      0.012132239528000355,
                      0.006830328144133091,
                      -0.012927016243338585,
                      0.014428261667490005,
                      0.020392486825585365,
                      0.0012550007086247206,
                      0.001064797630533576,
                      0.02070496417582035,
                      -0.015841199085116386,
                      -0.027389243245124817,
                      0.02861197665333748,
                      0.030514007434248924,
                      -0.014958113431930542,
                      0.015134730376303196,
                      0.008973509073257446,
                      0.014958113431930542,
                      0.01589554361999035,
                      0.012763985432684422,
                      -0.00374971772544086,
                      0.027008837088942528,
                      -0.035731006413698196,
                      -0.008593102917075157,
                      -0.013253078795969486,
                      -0.024128619581460953,
                      0.025568727403879166,
                      -0.0280685406178236,
                      0.003950110170990229,
                      -0.0007968999561853707,
                      -0.017362823709845543,
                      0.0157460980117321,
                      0.03200846165418625,
                      0.013171562924981117,
                      -0.0250796340405941,
                      0.005325686186552048,
                      -0.006157824769616127,
                      0.01313080545514822,
                      -0.009904146194458008,
                      0.0034915851429104805,
                      0.02168315090239048,
                      -0.03043249249458313,
                      -0.022158658131957054,
                      0.010026419535279274,
                      -0.0067623988725245,
                      0.01692807301878929,
                      -0.015460792928934097,
                      -0.0007009492837823927,
                      0.006840517744421959,
                      0.028476117178797722,
                      -0.04290438070893288,
                      -0.027511516585946083,
                      -0.005994793493300676,
                      0.0066061606630682945,
                      -0.01849045604467392,
                      -0.005111707840114832,
                      -0.006235943641513586,
                      -0.01806929148733616,
                      0.01493094116449356,
                      -0.011466528289020061,
                      0.005991396959871054,
                      -0.011018192395567894,
                      -0.003729338990524411,
                      -0.009095782414078712,
                      -0.0018952378304675221,
                      -0.03757869452238083,
                      -0.016955245286226273,
                      0.0071801659651100636,
                      0.016411809250712395,
                      -0.006619746331125498,
                      -0.009720736183226109,
                      -0.010046797804534435,
                      0.022593408823013306,
                      -0.0049724518321454525,
                      0.013667449355125427,
                      -0.03423655405640602,
                      -0.003525549778714776,
                      0.0023265911731868982,
                      0.007397541310638189,
                      -0.011969207786023617,
                      -0.042279426008462906,
                      -0.004378067329525948,
                      -0.018639901652932167,
                      0.006735226605087519,
                      -0.018599143251776695,
                      -0.011344254948198795,
                      0.006898257881402969,
                      0.005896295420825481,
                      -0.0025303801521658897,
                      -0.013762551359832287,
                      -0.00305683515034616,
                      0.0001466856338083744,
                      0.013803308829665184,
                      -0.008348556235432625,
                      0.017648128792643547,
                      0.0006079705781303346,
                      -0.012302063405513763,
                      -0.020976681262254715,
                      -0.029345616698265076,
                      0.001504642190411687,
                      -0.014849426224827766,
                      -0.023394977673888206,
                      -0.014672808349132538,
                      0.031193304806947708,
                      0.010685336776077747,
                      -0.01571892574429512,
                      -0.01691448874771595,
                      -0.020840823650360107,
                      -0.03855688124895096,
                      -0.00805645901709795,
                      -0.007723603397607803,
                      0.017267722636461258,
                      -0.0014180318685248494,
                      0.017403582111001015,
                      0.027593031525611877,
                      0.02425089292228222,
                      0.007105443626642227,
                      -0.003943317569792271,
                      0.0024964152835309505,
                      0.000519237422849983,
                      -0.0002857291838154197,
                      -0.02801419608294964,
                      -0.019061066210269928,
                      -0.0009433733066543937,
                      -0.047414910048246384,
                      -0.000902615487575531,
                      0.04279569163918495,
                      -0.018667073920369148,
                      0.017607370391488075,
                      0.05630011111497879,
                      -0.008993888273835182,
                      -0.016425393521785736,
                      -0.001525870175100863,
                      0.015161902643740177,
                      0.010196243412792683,
                      0.02980753965675831,
                      0.015977058559656143,
                      -0.026207266375422478,
                      -0.00731602543964982,
                      0.026709945872426033,
                      0.00895992387086153,
                      -0.0329594761133194,
                      -0.0018120239255949855,
                      0.01653408259153366,
                      -0.003362518735229969,
                      0.003048344049602747,
                      -0.019210509955883026,
                      0.01650691032409668,
                      -0.0030636282172054052,
                      -0.008858028799295425,
                      0.0046531823463737965,
                      0.0147271528840065,
                      -0.02967168018221855,
                      -0.0028530461713671684,
                      0.029101070016622543,
                      -0.019400713965296745,
                      -0.021397845819592476,
                      0.022606994956731796,
                      -0.01400709804147482,
                      -0.0023690471425652504,
                      0.016221605241298676,
                      -0.014740738086402416,
                      0.006843914277851582,
                      -0.004707525949925184,
                      0.0009951696265488863,
                      -0.01847686991095543,
                      -0.0004294428799767047,
                      -0.029345616698265076,
                      0.028122883290052414,
                      -0.023150430992245674,
                      0.030731383711099625,
                      0.011106501333415508,
                      -0.02942713350057602,
                      0.006497472990304232,
                      0.0005608443170785904,
                      -0.020243041217327118,
                      -0.014686394482851028,
                      -0.014876597560942173,
                      0.009299571625888348,
                      -0.009007474407553673,
                      0.014088613912463188,
                      0.0007591140456497669,
                      -0.01351121161133051,
                      -0.015610238537192345,
                      -0.00285983900539577,
                      0.01832742430269718,
                      -0.006847310811281204,
                      -0.00011325149534968659,
                      0.23715606331825256,
                      -0.03877425566315651,
                      0.010936676524579525,
                      0.0009178996551781893,
                      -0.019183339551091194,
                      0.0034678096417337656,
                      -0.008531966246664524,
                      0.010950262658298016,
                      -0.009170505218207836,
                      -0.0037667001597583294,
                      0.02168315090239048,
                      0.002808891935274005,
                      -0.010705715976655483,
                      0.0050437781028449535,
                      0.016411809250712395,
                      -0.01331421546638012,
                      -0.013090047053992748,
                      -0.021139713004231453,
                      -0.015542308799922466,
                      0.005030191969126463,
                      0.010006040334701538,
                      0.020637033507227898,
                      -0.013769344426691532,
                      -0.02226734533905983,
                      0.03228018060326576,
                      -0.002003925386816263,
                      0.0007862859056331217,
                      -0.004143709782510996,
                      0.0354321151971817,
                      0.011446149088442326,
                      -0.009041438810527325,
                      0.005003020167350769,
                      -0.000519237422849983,
                      -0.0013653864152729511,
                      -0.0029005969408899546,
                      0.003051740350201726,
                      0.006072912365198135,
                      0.009428638033568859,
                      0.019061066210269928,
                      -0.0006410862552002072,
                      0.04154578596353531,
                      0.021166885271668434,
                      -0.008708584122359753,
                      -0.019781120121479034,
                      0.010637786239385605,
                      0.009795458056032658,
                      -0.020759306848049164,
                      -0.012302063405513763,
                      -0.008008908480405807,
                      0.029698852449655533,
                      -0.032089974731206894,
                      0.022905884310603142,
                      0.02465846948325634,
                      0.037306975573301315,
                      -0.0007421316695399582,
                      -0.01732206530869007,
                      0.00954411830753088,
                      0.0015487964265048504,
                      0.03279644623398781,
                      -0.012994945980608463,
                      -0.017362823709845543,
                      0.009292778559029102,
                      -0.005828365683555603,
                      0.023367807269096375,
                      -0.0037395283579826355,
                      0.02627519704401493,
                      -0.038692738860845566,
                      0.04312175512313843,
                      0.02268850989639759,
                      0.014102199114859104,
                      0.008919165469706059,
                      -0.00975470058619976,
                      -0.013558762148022652,
                      0.004568270407617092,
                      -0.02624802477657795,
                      -0.02490301802754402,
                      0.016941659152507782,
                      0.01709110476076603,
                      0.015555894933640957,
                      0.03448110073804855,
                      0.002479432849213481,
                      -0.019047480076551437,
                      -0.027538688853383064,
                      -0.02209072932600975,
                      -0.018585557118058205,
                      -0.04298589378595352,
                      -0.0010792326647788286,
                      -0.020025666803121567,
                      0.0010299836285412312,
                      -0.03521474078297615,
                      0.01729489490389824,
                      -0.032334521412849426,
                      -0.006168013904243708,
                      -0.013490832410752773,
                      0.02327270433306694,
                      0.010732888244092464,
                      0.004731301683932543,
                      0.018177980557084084,
                      0.010848368518054485,
                      -0.010026419535279274,
                      -0.0349973663687706,
                      0.056626174598932266,
                      0.017580198124051094,
                      -0.0033455363009124994,
                      -0.014292402192950249,
                      0.004659975413233042,
                      0.0033828974701464176,
                      0.0019954340532422066,
                      0.012383579276502132,
                      -0.01074647344648838,
                      0.0038482157979160547,
                      -0.01532493345439434,
                      0.013354972936213017,
                      -0.016221605241298676,
                      0.0037157528568059206,
                      0.0009289382142014802,
                      0.011955621652305126,
                      0.012369993142783642,
                      0.03024228848516941,
                      -0.018001362681388855,
                      0.003722545923665166,
                      -0.019414300099015236,
                      -0.0002935835509561002,
                      0.030079258605837822,
                      0.0014358634361997247,
                      -0.01493094116449356,
                      0.003742924891412258,
                      0.007234510034322739,
                      -0.02310967445373535,
                      0.0024318823125213385,
                      -0.004102952312678099,
                      -0.016004230827093124,
                      0.01123556774109602,
                      -0.02107178419828415,
                      -0.03575817868113518,
                      0.013993511907756329,
                      0.009965282864868641,
                      -0.004975848365575075,
                      -0.017226964235305786,
                      0.012118653394281864,
                      0.00012758040975313634,
                      0.004038419108837843,
                      0.005233981180936098,
                      -0.01830025389790535,
                      -0.011758625507354736,
                      -0.009584876708686352,
                      0.013205528259277344,
                      -0.015868371352553368,
                      -0.006881275679916143,
                      -0.008776513859629631,
                      -0.01811004988849163,
                      -0.028394602239131927,
                      -0.0058079869486391544,
                      -0.03524191305041313,
                      0.020039252936840057,
                      0.010603821836411953,
                      -0.008389314636588097,
                      -0.022376032546162605,
                      0.017362823709845543,
                      0.009109368547797203,
                      -0.020840823650360107,
                      -0.005729867611080408,
                      0.0032860978972166777,
                      -0.023924829438328743,
                      -0.001446052803657949,
                      -0.004181071184575558,
                      -0.17389994859695435,
                      -0.011317082680761814,
                      0.016778629273176193,
                      -0.040241535753011703,
                      0.008898787200450897,
                      -0.0012066008057445288,
                      0.01709110476076603,
                      -0.02171032316982746,
                      -0.038502536714076996,
                      0.012981359846889973,
                      0.02085440792143345,
                      0.00885123573243618,
                      -0.027008837088942528,
                      -0.017063932493329048,
                      -0.017036762088537216,
                      0.004619217477738857,
                      0.0025337766855955124,
                      0.021737493574619293,
                      0.0030670245178043842,
                      0.0015411543427035213,
                      0.042034879326820374,
                      -0.0326605848968029,
                      0.017661714926362038,
                      -0.014075027778744698,
                      0.00945581030100584,
                      -0.0028615372721105814,
                      -0.002742660464718938,
                      0.009985661134123802,
                      -0.0026662396267056465,
                      -0.03722545877099037,
                      0.008599895983934402,
                      0.006239340174943209,
                      0.025691000744700432,
                      -0.009184091351926327,
                      0.0023147035390138626,
                      0.017199791967868805,
                      0.016438979655504227,
                      -0.006888068746775389,
                      -0.009347123093903065,
                      0.0025949133560061455,
                      0.031111789867281914,
                      0.01929202675819397,
                      0.016547666862607002,
                      -0.01400709804147482,
                      -0.019061066210269928,
                      0.020786479115486145,
                      0.020134354010224342,
                      -0.014387504197657108,
                      0.021723909303545952,
                      -0.019726775586605072,
                      0.00318420329131186,
                      -0.00817193929105997,
                      -0.0016048384131863713,
                      0.013151183724403381,
                      0.023965587839484215,
                      0.009190884418785572,
                      -0.0033795011695474386,
                      0.01666994020342827,
                      -0.014795082621276379,
                      0.004391652997583151,
                      -0.00019243202405050397,
                      -0.024617712944746017,
                      0.011330668814480305,
                      -0.017145449295639992,
                      -0.02725338377058506,
                      -0.01262133289128542,
                      -0.017756815999746323,
                      0.0047245086170732975,
                      0.0006156126619316638,
                      0.011004606261849403,
                      -0.015447206795215607,
                      -0.013844067230820656,
                      -0.0068608964793384075,
                      0.00015676894690841436,
                      0.010474755428731441,
                      0.032905131578445435,
                      -0.009238434955477715,
                      -0.005444562993943691,
                      0.019821878522634506,
                      0.009978868998587132,
                      -0.013185149058699608,
                      0.032714929431676865,
                      -0.018599143251776695,
                      0.018965963274240494,
                      -0.006385388784110546,
                      0.0009892258094623685,
                      0.0009255417389795184,
                      0.018558386713266373,
                      -0.010624200105667114,
                      -0.01869424618780613,
                      0.005186430178582668,
                      -0.03024228848516941,
                      0.013205528259277344,
                      0.0012210358399897814,
                      0.0019852446857839823,
                      0.011371427215635777,
                      0.02310967445373535,
                      0.0039161453023552895,
                      0.004201449919492006,
                      -0.015474379062652588,
                      0.0053902193903923035,
                      0.003841422963887453,
                      0.009319950826466084,
                      0.029399961233139038,
                      0.011588801629841328,
                      0.027959851548075676,
                      -0.024413922801613808,
                      0.013816894963383675,
                      0.029726022854447365,
                      0.0038244405295699835,
                      -0.02429164946079254,
                      -0.016642769798636436,
                      0.015053214505314827,
                      0.020650619640946388,
                      0.013796515762805939,
                      0.015352105721831322,
                      -0.0035459287464618683,
                      -0.017947018146514893,
                      0.011208395473659039,
                      -0.014061441645026207,
                      0.03415503725409508,
                      0.0034763009753078222,
                      -0.01793343387544155,
                      0.02168315090239048,
                      -0.02207714319229126,
                      -0.04717036336660385,
                      -0.09417769312858582,
                      -0.038094960153102875,
                      -0.019658846780657768,
                      0.0047143190167844296,
                      -0.003844819264486432,
                      0.016574839130043983,
                      0.02688656374812126,
                      0.018830103799700737,
                      0.00039420436951331794,
                      0.03282361850142479,
                      -0.019047480076551437,
                      -0.013558762148022652,
                      0.002316401805728674,
                      -0.02687297761440277,
                      0.008104009553790092,
                      0.0024607523810118437,
                      0.0016260665142908692,
                      0.004710922483354807,
                      -0.017960604280233383,
                      0.01969960518181324,
                      0.0030789123848080635,
                      -0.030785726383328438,
                      0.032714929431676865,
                      -0.03184542804956436,
                      0.031138960272073746,
                      -0.026207266375422478,
                      -0.031763914972543716,
                      0.008307798765599728,
                      -0.0024641486816108227,
                      -0.016778629273176193,
                      -0.012336027808487415,
                      -0.02926410175859928,
                      9.860416321316734e-05,
                      -0.014944527298212051,
                      0.0032657189294695854,
                      -0.008756134659051895,
                      -0.016194432973861694,
                      -0.027348484843969345,
                      -0.003950110170990229,
                      -0.029970571398735046,
                      0.0020327954553067684,
                      0.0007306685438379645,
                      0.0004997076466679573,
                      -0.01592271402478218,
                      -0.012132239528000355,
                      -0.04002416133880615,
                      -0.010963848792016506,
                      0.014387504197657108,
                      0.011317082680761814,
                      -0.007295646704733372,
                      -0.014876597560942173,
                      -0.008919165469706059,
                      -0.03399200737476349,
                      -0.007064685691148043,
                      0.01313080545514822,
                      0.012852293439209461,
                      0.0032708137296140194,
                      0.004452789667993784,
                      -0.011147258803248405,
                      -0.02171032316982746,
                      0.007954563945531845,
                      -0.007397541310638189,
                      -0.01892520673573017,
                      0.0072888536378741264,
                      0.0010936676990240812,
                      -0.004371274262666702,
                      -0.00934033002704382,
                      -0.025568727403879166,
                      0.012600953690707684,
                      -0.0306226946413517,
                      0.0029464494436979294,
                      0.014469020068645477,
                      0.0035798936150968075,
                      0.019604502245783806,
                      -0.021207643672823906,
                      -0.010705715976655483,
                      -0.015515136532485485,
                      0.021574463695287704,
                      0.005845348350703716,
                      -0.01609933190047741,
                      -0.03228018060326576,
                      -0.022729268297553062,
                      0.01729489490389824,
                      -0.025025291368365288,
                      -0.006803156342357397,
                      -0.00766246672719717,
                      -0.024644885212183,
                      -0.00037127811810933053,
                      -0.0038040615618228912,
                      -0.029698852449655533,
                      0.012668883427977562,
                      0.03045966476202011,
                      -0.0013450074475258589,
                      -0.013191942125558853,
                      -0.006235943641513586,
                      0.0068608964793384075,
                      -0.025758931413292885,
                      0.004374670796096325,
                      0.006096688099205494,
                      0.045458536595106125,
                      -0.01364027801901102,
                      -0.00641595758497715,
                      -0.052822113037109375,
                      0.008803685195744038,
                      0.0031909963581711054,
                      -0.022946642711758614,
                      0.009218056686222553,
                      0.023775383830070496,
                      0.037497177720069885,
                      0.009421844966709614,
                      -0.013470453210175037,
                      0.030731383711099625,
                      -0.01073968131095171,
                      0.02168315090239048,
                      0.013049289584159851,
                      0.0001267312909476459,
                      -0.012994945980608463,
                      -0.024821501225233078,
                      0.02385690063238144,
                      -0.021968455985188484,
                      0.015814026817679405,
                      0.024726400151848793,
                      -0.003197789192199707,
                      -0.011378219351172447,
                      0.012859086506068707,
                      0.01253981702029705,
                      -0.0019173149485141039,
                      -0.0010130011942237616,
                      -0.015433620661497116,
                      0.014686394482851028,
                      -0.01733565144240856,
                      -0.02268850989639759,
                      -0.002095630392432213,
                      -0.019590916112065315,
                      0.006246133241802454,
                      0.00995169673115015,
                      -0.010889125987887383,
                      -0.002598309889435768,
                      0.0007387351361103356,
                      0.008830857463181019,
                      0.02268850989639759,
                      0.009462603367865086,
                      -0.01194203644990921,
                      -0.026112165302038193,
                      0.02882935293018818,
                      -0.025976305827498436,
                      0.0027494532987475395,
                      -0.0033964836038649082,
                      0.004632803611457348,
                      -0.002212808933109045,
                      -0.003817647462710738,
                      -0.006483886856585741,
                      0.024155789986252785,
                      0.02108537033200264,
                      0.009435431100428104,
                      -0.022973814979195595,
                      -0.01631670631468296,
                      0.005447959527373314,
                      0.013551969081163406,
                      0.026152923703193665,
                      -0.026913736015558243,
                      -0.009809044189751148,
                      0.017607370391488075,
                      0.007825498469173908,
                      0.002411503344774246,
                      -0.00325383129529655,
                      0.0016421998152509332,
                      0.012859086506068707,
                      0.019237682223320007,
                      -0.0017186206532642245,
                      -0.009197677485644817,
                      -0.034535445272922516,
                      -0.0014018985675647855,
                      -0.0013008532114326954,
                      0.012071102857589722,
                      0.011575215496122837,
                      0.03564948961138725,
                      0.006229150574654341,
                      0.00017884607950691134,
                      0.00716658029705286,
                      -0.01463205087929964,
                      0.03662768006324768,
                      0.015596652403473854,
                      -0.028965210542082787,
                      -0.01342969574034214,
                      -0.017770402133464813,
                      0.004829799756407738,
                      -0.0027358673978596926,
                      0.006796363741159439,
                      -0.0024556575808674097,
                      -0.01402068417519331,
                      -0.0037870791275054216,
                      0.005712885409593582,
                      0.007988529279828072,
                      0.002396219177171588,
                      0.0053800297901034355,
                      -0.004089366178959608,
                      0.009809044189751148,
                      0.010984227992594242,
                      0.00021461529831867665,
                      0.012607746757566929,
                      -0.013592727482318878,
                      0.001277926960028708,
                      0.0013322706799954176,
                      -0.004595442209392786,
                      -0.005634766072034836,
                      -0.014536949805915356,
                      0.020990267395973206,
                      -0.019020307809114456,
                      -0.028313087299466133,
                      0.00040587977855466306,
                      0.0034151640720665455,
                      0.010298137553036213,
                      0.020243041217327118,
                      -0.015257003717124462,
                      0.0015292667085304856,
                      -0.02861197665333748,
                      0.03423655405640602,
                      0.013735379092395306,
                      -0.017457924783229828,
                      -0.008498001843690872,
                      0.007995322346687317,
                      0.007085064426064491,
                      -0.00015952858666423708,
                      0.028557633981108665,
                      0.0013874635333195329,
                      0.01592271402478218,
                      0.021642392501235008,
                      0.04121972247958183,
                      -0.010481548495590687,
                      0.00894633773714304,
                      0.008287419565021992,
                      -0.013253078795969486,
                      -0.017145449295639992,
                      -0.005529474932700396,
                      -0.019020307809114456,
                      -0.00023456964117940515,
                      0.0033217607997357845,
                      0.0027137903962284327,
                      0.022172244265675545,
                      -0.0034253536723554134,
                      0.0892324149608612,
                      0.02586761862039566,
                      0.004996227100491524,
                      0.0027341691311448812,
                      -0.0010401731124147773,
                      0.025813274085521698,
                      0.019210509955883026,
                      0.014346746727824211,
                      -0.0023418753407895565,
                      0.00481961015611887,
                      0.02449543960392475,
                      0.01633029244840145,
                      0.006545023526996374,
                      -0.012492266483604908,
                      -0.014795082621276379,
                      0.006239340174943209,
                      0.011595594696700573,
                      -0.0030924982856959105,
                      0.01787908934056759,
                      -0.013103633187711239,
                      0.046056315302848816,
                      -0.026492571458220482,
                      0.007458677981048822,
                      0.011955621652305126,
                      -0.03461695834994316,
                      -0.006049137096852064,
                      0.010760059580206871,
                      0.015664581209421158,
                      -0.018667073920369148,
                      -0.039073146879673004,
                      0.019658846780657768,
                      -0.002095630392432213,
                      -0.028150055557489395,
                      -0.012845500372350216,
                      0.022213002666831017,
                      -0.002409805078059435,
                      -0.001168390386737883,
                      -0.005818176083266735,
                      0.018245909363031387,
                      0.010617407038807869,
                      0.01501245703548193,
                      0.01690090261399746,
                      -0.004459582734853029,
                      -0.03497019410133362,
                      -0.01194203644990921,
                      -0.0059676216915249825,
                      -0.02008001133799553,
                      -0.027348484843969345,
                      -0.016588425263762474
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 8,
                  "total_tokens": 8
                }
              },
              "start_time": 1694832910.037893,
              "end_time": 1694832910.619005,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "prompt_tokens": 8,
        "total_tokens": 8,
        "duration": 0.590323
      },
      "result": [
        -0.022036384791135788,
        -0.0164797380566597,
        0.0005999039276503026,
        -0.007227716967463493,
        0.003868594765663147,
        0.01381010189652443,
        -0.006500869523733854,
        -0.011711074970662594,
        -0.01653408259153366,
        -0.015121144242584705,
        0.0027137903962284327,
        0.027362070977687836,
        -0.025758931413292885,
        0.00944222416728735,
        3.94045164284762e-05,
        0.0016226699808612466,
        0.016561252996325493,
        0.015487965196371078,
        0.004052004776895046,
        0.0009892258094623685,
        -0.005791004281491041,
        -0.014170128852128983,
        -0.019210509955883026,
        0.020990267395973206,
        -0.006324252113699913,
        -0.02627519704401493,
        0.05154503509402275,
        -0.033502914011478424,
        0.00033030801569111645,
        -0.013558762148022652,
        0.027144696563482285,
        0.030269460752606392,
        -0.010019626468420029,
        -0.008131181821227074,
        -0.013069668784737587,
        0.0003339167742524296,
        0.01770247146487236,
        -0.002297721104696393,
        0.017009589821100235,
        -0.01224092673510313,
        0.015800440683960915,
        0.01892520673573017,
        0.013878031633794308,
        -0.001744094304740429,
        -0.0005765530513599515,
        0.003309873165562749,
        0.01252623088657856,
        -0.004153899382799864,
        0.0005892898770980537,
        -0.023788969963788986,
        0.018653487786650658,
        0.01949581503868103,
        -0.02384331449866295,
        -0.002343573607504368,
        -0.03442675620317459,
        -0.0030534386169165373,
        -0.0077575682662427425,
        0.02582686021924019,
        0.009068611077964306,
        -0.02248472161591053,
        0.010753266513347626,
        0.0006746265571564436,
        -0.015664581209421158,
        0.019156167283654213,
        0.0048841433599591255,
        -0.009299571625888348,
        -0.022946642711758614,
        0.029101070016622543,
        0.010603821836411953,
        0.008885201066732407,
        0.007628501858562231,
        0.020623447373509407,
        0.019455058500170708,
        -0.011086122132837772,
        0.03654616326093674,
        -0.002844554837793112,
        -0.01502604316920042,
        -0.005852140951901674,
        0.008036079816520214,
        0.0004491849394980818,
        0.006405767984688282,
        0.02306891605257988,
        -0.02109895646572113,
        0.020922338590025902,
        0.013062875717878342,
        -0.007594536989927292,
        0.013103633187711239,
        0.01690090261399746,
        -0.019577331840991974,
        -0.0003455921832937747,
        0.006201978772878647,
        0.002939656376838684,
        0.03314967826008797,
        0.004282965790480375,
        0.000431353400927037,
        0.03024228848516941,
        -0.004602235276252031,
        0.029101070016622543,
        0.0016498418990522623,
        -0.035106055438518524,
        0.009027853608131409,
        -0.0075266072526574135,
        -0.007940978743135929,
        -0.01224092673510313,
        -0.019224096089601517,
        0.0021533705294132233,
        0.00983621645718813,
        -0.010372860357165337,
        0.028150055557489395,
        -0.008076838217675686,
        -0.007716810330748558,
        0.03203563392162323,
        -0.0034202588722109795,
        -0.020215868949890137,
        0.005529474932700396,
        0.021533705294132233,
        0.0010299836285412312,
        0.0005413145408965647,
        0.017457924783229828,
        0.020732134580612183,
        0.03328553959727287,
        -0.002212808933109045,
        0.0011565026361495256,
        -0.01827308163046837,
        0.022348862141370773,
        -0.008137974888086319,
        -0.021615220233798027,
        -0.019998494535684586,
        0.008701791055500507,
        -0.029481476172804832,
        0.006055930163711309,
        -0.0036444268189370632,
        0.0013798214495182037,
        -0.023816142231225967,
        -0.002878519706428051,
        0.023191189393401146,
        -0.00796135701239109,
        -0.010107934474945068,
        -0.026424642652273178,
        -0.01768888533115387,
        0.016819385811686516,
        0.022593408823013306,
        -0.023177603259682655,
        -0.03518756851553917,
        -0.026601258665323257,
        0.02546004019677639,
        0.0007625105208717287,
        0.02842177450656891,
        0.010114727541804314,
        0.002683222061023116,
        0.0042456043884158134,
        -0.031138960272073746,
        -0.018055707216262817,
        0.009591669775545597,
        0.0022875317372381687,
        0.013979925774037838,
        -0.00032075541093945503,
        0.0026458606589585543,
        -0.000471262086648494,
        -0.00011494974023662508,
        0.0030279650818556547,
        0.014550535008311272,
        0.011554837226867676,
        -0.009598462842404842,
        0.03760586678981781,
        0.04051325470209122,
        -0.004279569257050753,
        -0.0026611448265612125,
        0.012397164478898048,
        0.003573100548237562,
        -0.008688204921782017,
        0.01570533961057663,
        -0.026981664821505547,
        0.004629407078027725,
        0.015433620661497116,
        0.00746547058224678,
        0.017430752515792847,
        0.007805119268596172,
        0.0007680298294872046,
        -0.013361766003072262,
        -0.0051320865750312805,
        0.006398974917829037,
        0.03399200737476349,
        0.005003020167350769,
        -0.015257003717124462,
        -0.002204317832365632,
        0.003770096693187952,
        -0.035350602120161057,
        0.0028004006016999483,
        -0.044643379747867584,
        0.018205150961875916,
        0.019061066210269928,
        0.011711074970662594,
        -0.004673561546951532,
        -0.6364738345146179,
        -0.014455433934926987,
        0.016629183664917946,
        -0.00835534930229187,
        0.021329917013645172,
        -0.013083254918456078,
        -0.010352482087910175,
        -0.00566193787381053,
        -0.01171786803752184,
        0.03263341262936592,
        0.005447959527373314,
        -0.012607746757566929,
        -0.005906485021114349,
        -0.01502604316920042,
        -0.0013458565808832645,
        -0.022511892020702362,
        -0.00936070829629898,
        -0.00402143644168973,
        0.01869424618780613,
        -0.003973885904997587,
        -0.02446826733648777,
        0.02124840021133423,
        -0.04380105063319206,
        0.005852140951901674,
        -0.01752585545182228,
        0.007132615428417921,
        0.002350366674363613,
        -0.009313157759606838,
        0.007295646704733372,
        -0.007268474902957678,
        -0.01323269959539175,
        0.03336705267429352,
        0.03956224024295807,
        -0.015284175984561443,
        0.031709570437669754,
        0.011860520578920841,
        -0.002382633276283741,
        0.015759684145450592,
        -0.031193304806947708,
        0.0256638303399086,
        -0.00866782572120428,
        -0.019618088379502296,
        0.02329987660050392,
        -0.007010342087596655,
        0.03217149153351784,
        0.007248095702379942,
        0.01232923474162817,
        0.021425018087029457,
        0.0018884448800235987,
        0.007431505713611841,
        -0.027606617659330368,
        0.008722169324755669,
        0.0014333160361275077,
        -0.008464036509394646,
        0.014401090331375599,
        -0.0018341010436415672,
        0.022837955504655838,
        -0.014061441645026207,
        0.0002553731028456241,
        -0.022606994956731796,
        0.011643145233392715,
        0.034345243126153946,
        -0.010902712121605873,
        -0.02567741461098194,
        -0.011004606261849403,
        0.015229832381010056,
        0.0035323428455740213,
        0.012770777568221092,
        0.011887691915035248,
        -0.04477923735976219,
        0.010481548495590687,
        0.024984532967209816,
        0.0006393880466930568,
        -0.0006792966742068529,
        0.02008001133799553,
        0.017743229866027832,
        -0.00228923000395298,
        -0.015121144242584705,
        -0.0004844234499614686,
        0.022973814979195595,
        -0.009252021089196205,
        -0.005587215535342693,
        0.012607746757566929,
        -0.0028190813027322292,
        -0.018748588860034943,
        0.017797574400901794,
        -0.006918637081980705,
        -0.02309608832001686,
        0.005352857988327742,
        0.026207266375422478,
        -0.0012125446228310466,
        0.02782399207353592,
        -0.021601635962724686,
        -0.03942637890577316,
        0.02248472161591053,
        0.012369993142783642,
        0.003501774510368705,
        -0.016642769798636436,
        0.03247038275003433,
        -0.007241302635520697,
        0.002798702334985137,
        -0.017607370391488075,
        0.020243041217327118,
        0.023761799558997154,
        0.00856593158096075,
        0.016438979655504227,
        0.02270209603011608,
        0.020637033507227898,
        0.002071854891255498,
        -0.017199791967868805,
        0.004792438354343176,
        -0.016357464715838432,
        0.00825345516204834,
        0.008776513859629631,
        -0.006633332464843988,
        -0.0329594761133194,
        0.014183714985847473,
        0.0027817199006676674,
        0.012220547534525394,
        -0.021234814077615738,
        0.0019427885999903083,
        -0.009598462842404842,
        0.012444715946912766,
        -0.016642769798636436,
        0.019210509955883026,
        0.017675301060080528,
        -0.0031536349561065435,
        0.004310137592256069,
        -0.020650619640946388,
        -0.005298514384776354,
        -0.021832596510648727,
        -0.006426146719604731,
        0.0053800297901034355,
        -0.012152617797255516,
        0.04393691197037697,
        0.03899163007736206,
        0.03260624036192894,
        -0.0015937999123707414,
        -0.02050117403268814,
        0.005006416700780392,
        -0.012315649539232254,
        -0.010495133697986603,
        0.01567816734313965,
        -0.005740057211369276,
        -0.052034128457307816,
        -0.016167260706424713,
        0.008891994133591652,
        0.003070421051234007,
        0.001690599718131125,
        -0.011554837226867676,
        0.0015428526094183326,
        -0.02624802477657795,
        -0.018585557118058205,
        -0.005899691954255104,
        -0.013090047053992748,
        -0.028557633981108665,
        -0.009408259764313698,
        -0.04668127000331879,
        -0.019536573439836502,
        -0.031709570437669754,
        0.010325309820473194,
        0.00373273529112339,
        -0.011989586986601353,
        -0.012770777568221092,
        0.0011378219351172447,
        -0.03497019410133362,
        0.005515889264643192,
        0.022213002666831017,
        -0.027525102719664574,
        -0.033910490572452545,
        0.028938040137290955,
        -0.01846328377723694,
        -0.01033210288733244,
        0.005774022080004215,
        -0.026682773604989052,
        0.013776137493550777,
        -0.03325836732983589,
        -0.016017816960811615,
        0.0003791324852500111,
        -0.016044987365603447,
        0.005030191969126463,
        0.030704211443662643,
        -0.010012833401560783,
        -0.009224848821759224,
        0.012220547534525394,
        0.0020344937220215797,
        0.017172621563076973,
        -0.00310948072001338,
        -0.02488943189382553,
        0.00974111445248127,
        0.00874934159219265,
        0.026587672531604767,
        -0.003061929950490594,
        0.02090875245630741,
        0.012573782354593277,
        0.006110273767262697,
        0.016357464715838432,
        -0.009122954681515694,
        0.0008754436275921762,
        -0.005172844510525465,
        0.025297008454799652,
        0.010732888244092464,
        0.00034325712476857007,
        -0.012940602377057076,
        -0.008294212631881237,
        0.017457924783229828,
        -8.363840606762096e-05,
        -0.016438979655504227,
        0.008219489827752113,
        -0.00042880602995865047,
        0.01461846474558115,
        -0.029508648440241814,
        -0.02325911819934845,
        -0.0005476829828694463,
        -0.008708584122359753,
        0.021044611930847168,
        -0.009068611077964306,
        0.02289229817688465,
        -0.034263726323843,
        0.0033591222018003464,
        0.003783682594075799,
        -0.01793343387544155,
        0.004975848365575075,
        0.0009900749428197742,
        -0.011765418574213982,
        0.006297080311924219,
        -0.008097216486930847,
        -0.012702848762273788,
        0.012410750612616539,
        -0.022362448275089264,
        0.0005455601494759321,
        0.028285915032029152,
        0.02999774180352688,
        0.020025666803121567,
        -0.0012032042723149061,
        -0.00856593158096075,
        0.014469020068645477,
        -0.010196243412792683,
        0.025595899671316147,
        -0.017159035429358482,
        0.0026017064228653908,
        0.01154804416000843,
        0.013110426254570484,
        -0.0015776666114106774,
        0.03545928746461868,
        0.015814026817679405,
        0.03244321048259735,
        0.010651372373104095,
        -0.016072159633040428,
        0.02287871204316616,
        -0.01431957446038723,
        -0.008878407999873161,
        -0.030731383711099625,
        0.0011514079524204135,
        0.0240606889128685,
        -0.00924522802233696,
        0.0040757800452411175,
        0.012899843975901604,
        -0.0057094888761639595,
        0.033883318305015564,
        0.016357464715838432,
        0.008362142369151115,
        0.022797197103500366,
        0.007805119268596172,
        0.019183339551091194,
        -0.014455433934926987,
        -0.014591293409466743,
        -0.0058079869486391544,
        -0.009184091351926327,
        -0.004938486963510513,
        -0.017254136502742767,
        -0.015080386772751808,
        0.0010206432780250907,
        -0.010617407038807869,
        0.01441467646509409,
        0.03024228848516941,
        -0.02490301802754402,
        0.021438604220747948,
        -0.0067929672077298164,
        0.034127864986658096,
        -0.0023045141715556383,
        -0.029916226863861084,
        0.03160088136792183,
        -0.006130652967840433,
        -0.027959851548075676,
        -0.0171318631619215,
        0.0032283575274050236,
        -0.02169673703610897,
        -0.004269379656761885,
        -0.018830103799700737,
        0.0016956944018602371,
        0.009007474407553673,
        -0.0019852446857839823,
        0.025120392441749573,
        0.00111829221714288,
        0.013293836265802383,
        0.007302439771592617,
        -0.01986263506114483,
        0.004432410933077335,
        0.004486754536628723,
        0.0019190132152289152,
        0.00785946287214756,
        -0.006490679923444986,
        -0.028965210542082787,
        0.02029738575220108,
        0.001125934300944209,
        -0.029209759086370468,
        -0.016642769798636436,
        -0.016466151922941208,
        -0.015977058559656143,
        0.038257990032434464,
        0.009462603367865086,
        0.01868066005408764,
        -0.019346369430422783,
        -5.301698547555134e-05,
        0.0292369294911623,
        0.007893427275121212,
        -0.006996755953878164,
        0.008511587977409363,
        0.004408635664731264,
        -0.028313087299466133,
        -0.012268098071217537,
        -0.01671069860458374,
        0.008436865173280239,
        0.04317609965801239,
        0.04755076766014099,
        -0.004174278117716312,
        0.019020307809114456,
        -0.012757192365825176,
        0.03635596111416817,
        -0.025758931413292885,
        -0.0389372855424881,
        -0.021194057539105415,
        -0.01649332419037819,
        0.0015802140114828944,
        -0.02664201706647873,
        0.006531437858939171,
        -0.022810783237218857,
        0.005254359915852547,
        0.01025058701634407,
        -0.0003099291061516851,
        -0.02626161091029644,
        0.033747460693120956,
        -0.016547666862607002,
        -0.0009994152933359146,
        0.013701414689421654,
        0.006076308898627758,
        0.010467962361872196,
        0.001453694887459278,
        -0.019427886232733727,
        -0.012254512868821621,
        0.010807610116899014,
        -0.0035323428455740213,
        -0.011588801629841328,
        -0.016561252996325493,
        -0.022579822689294815,
        -0.01753944158554077,
        0.00787304900586605,
        -0.02051476016640663,
        0.03939921036362648,
        0.001176881487481296,
        0.006545023526996374,
        0.011004606261849403,
        0.020025666803121567,
        0.0029600353445857763,
        0.02662843093276024,
        -0.002523587318137288,
        -0.01540644932538271,
        7.790684321662411e-05,
        -0.030378147959709167,
        0.004031626041978598,
        0.016574839130043983,
        0.011317082680761814,
        -0.002888709306716919,
        0.0012337726075202227,
        -0.004809420555830002,
        -0.04111103713512421,
        -0.005013209767639637,
        0.014469020068645477,
        -0.014699980616569519,
        0.012471887283027172,
        0.010196243412792683,
        -0.01827308163046837,
        -0.02385690063238144,
        -0.017403582111001015,
        -0.014047855511307716,
        0.01872141659259796,
        -0.007275267504155636,
        0.00365801271982491,
        0.004639596678316593,
        -0.017362823709845543,
        -0.0244275089353323,
        -0.015596652403473854,
        0.04627368971705437,
        -0.008212696760892868,
        -0.04032305255532265,
        -0.04727904871106148,
        -0.016765043139457703,
        0.01221375446766615,
        0.011486907489597797,
        0.03260624036192894,
        -0.020786479115486145,
        -0.001669371617026627,
        -0.0023147035390138626,
        -0.007574158255010843,
        -0.03445392847061157,
        0.0223081037402153,
        -0.04051325470209122,
        0.0038006650283932686,
        -0.01102498546242714,
        -0.00746547058224678,
        -0.027565861120820045,
        -0.019332783296704292,
        0.0354321151971817,
        0.002744358731433749,
        0.005644955672323704,
        0.02961733564734459,
        -0.028965210542082787,
        0.01846328377723694,
        0.019984908401966095,
        0.0072005451656877995,
        0.005634766072034836,
        0.012886258773505688,
        -0.028313087299466133,
        -0.011955621652305126,
        -0.009285985492169857,
        -0.010420411825180054,
        -0.03325836732983589,
        0.03165522590279579,
        -0.006307269912213087,
        0.0036274443846195936,
        -0.021126126870512962,
        0.01733565144240856,
        -0.01072609517723322,
        0.008925958536565304,
        -0.023394977673888206,
        -0.00310948072001338,
        -0.01711827702820301,
        0.0313563346862793,
        -0.0038855771999806166,
        -0.001333968946710229,
        0.024970946833491325,
        -0.012268098071217537,
        -0.0019478832837194204,
        0.0025490608531981707,
        -0.02308250218629837,
        0.016588425263762474,
        0.019006721675395966,
        -0.0026017064228653908,
        0.021520119160413742,
        0.005498906597495079,
        -0.01688731648027897,
        -0.007037513889372349,
        0.025609485805034637,
        -0.024509025737643242,
        0.01668352633714676,
        -0.0017101294361054897,
        -0.0278511643409729,
        -0.03143785148859024,
        -0.016832971945405006,
        -0.002584723988547921,
        0.008932751603424549,
        -0.01461846474558115,
        -0.0012813233770430088,
        0.012179790064692497,
        -0.0006381143466569483,
        0.009992454200983047,
        -0.013225906528532505,
        0.01889803446829319,
        -0.022960228845477104,
        -0.014808667823672295,
        0.008531966246664524,
        0.001946185017004609,
        0.03206280618906021,
        -0.017634542658925056,
        0.02999774180352688,
        -0.030079258605837822,
        -0.0076692597940564156,
        0.0005748548428528011,
        -0.02983471192419529,
        -0.0004895181627944112,
        0.004462979268282652,
        0.03673636540770531,
        0.02324553392827511,
        0.03184542804956436,
        -0.000315448414767161,
        0.01082798931747675,
        0.01501245703548193,
        -0.0043169306591153145,
        -0.006378596182912588,
        -0.028394602239131927,
        -0.003245339961722493,
        -0.022117899730801582,
        0.017172621563076973,
        -0.001829006359912455,
        -0.013633484952151775,
        -0.008260248228907585,
        0.0026662396267056465,
        0.003905955934897065,
        0.002542268019169569,
        0.0028343654703348875,
        0.006945808883756399,
        0.0010180958779528737,
        -0.005757039412856102,
        -0.03635596111416817,
        0.02325911819934845,
        -0.004198053851723671,
        -0.0004954620380885899,
        -0.042279426008462906,
        -0.025595899671316147,
        0.013484039343893528,
        0.011065742932260036,
        0.014903769828379154,
        0.012397164478898048,
        0.013721792958676815,
        -0.015243417583405972,
        0.0006584932561963797,
        -0.008531966246664524,
        -0.003461016807705164,
        -0.001688052318058908,
        -0.014890183694660664,
        -0.029454305768013,
        -0.016846558079123497,
        0.013966340571641922,
        0.028557633981108665,
        -0.021139713004231453,
        -0.003255529562011361,
        -0.010603821836411953,
        0.01628953404724598,
        0.008321384899318218,
        0.01709110476076603,
        -0.008606689050793648,
        0.004378067329525948,
        -0.0013467057142406702,
        -0.009768286719918251,
        -0.03638312965631485,
        -0.028693493455648422,
        -0.0037633036263287067,
        0.036301616579294205,
        0.0034525254741311073,
        -0.02328629046678543,
        0.026519743725657463,
        0.011969207786023617,
        0.0026203871238976717,
        -0.01569175347685814,
        0.004496944136917591,
        0.05230584740638733,
        0.02051476016640663,
        0.011887691915035248,
        0.029481476172804832,
        -0.0005655144923366606,
        0.0016761645674705505,
        0.01870783045887947,
        -0.0005846197018399835,
        -0.014346746727824211,
        -0.004975848365575075,
        -0.007594536989927292,
        0.006755605805665255,
        -0.008321384899318218,
        0.02429164946079254,
        0.02844894677400589,
        -0.018802933394908905,
        -0.01164993830025196,
        0.0006028758361935616,
        -0.00596422515809536,
        0.012471887283027172,
        0.013572348281741142,
        -0.01690090261399746,
        -0.00231130700558424,
        0.012397164478898048,
        -0.026981664821505547,
        0.016602011397480965,
        -0.006602764129638672,
        -0.005070949904620647,
        0.012132239528000355,
        0.006830328144133091,
        -0.012927016243338585,
        0.014428261667490005,
        0.020392486825585365,
        0.0012550007086247206,
        0.001064797630533576,
        0.02070496417582035,
        -0.015841199085116386,
        -0.027389243245124817,
        0.02861197665333748,
        0.030514007434248924,
        -0.014958113431930542,
        0.015134730376303196,
        0.008973509073257446,
        0.014958113431930542,
        0.01589554361999035,
        0.012763985432684422,
        -0.00374971772544086,
        0.027008837088942528,
        -0.035731006413698196,
        -0.008593102917075157,
        -0.013253078795969486,
        -0.024128619581460953,
        0.025568727403879166,
        -0.0280685406178236,
        0.003950110170990229,
        -0.0007968999561853707,
        -0.017362823709845543,
        0.0157460980117321,
        0.03200846165418625,
        0.013171562924981117,
        -0.0250796340405941,
        0.005325686186552048,
        -0.006157824769616127,
        0.01313080545514822,
        -0.009904146194458008,
        0.0034915851429104805,
        0.02168315090239048,
        -0.03043249249458313,
        -0.022158658131957054,
        0.010026419535279274,
        -0.0067623988725245,
        0.01692807301878929,
        -0.015460792928934097,
        -0.0007009492837823927,
        0.006840517744421959,
        0.028476117178797722,
        -0.04290438070893288,
        -0.027511516585946083,
        -0.005994793493300676,
        0.0066061606630682945,
        -0.01849045604467392,
        -0.005111707840114832,
        -0.006235943641513586,
        -0.01806929148733616,
        0.01493094116449356,
        -0.011466528289020061,
        0.005991396959871054,
        -0.011018192395567894,
        -0.003729338990524411,
        -0.009095782414078712,
        -0.0018952378304675221,
        -0.03757869452238083,
        -0.016955245286226273,
        0.0071801659651100636,
        0.016411809250712395,
        -0.006619746331125498,
        -0.009720736183226109,
        -0.010046797804534435,
        0.022593408823013306,
        -0.0049724518321454525,
        0.013667449355125427,
        -0.03423655405640602,
        -0.003525549778714776,
        0.0023265911731868982,
        0.007397541310638189,
        -0.011969207786023617,
        -0.042279426008462906,
        -0.004378067329525948,
        -0.018639901652932167,
        0.006735226605087519,
        -0.018599143251776695,
        -0.011344254948198795,
        0.006898257881402969,
        0.005896295420825481,
        -0.0025303801521658897,
        -0.013762551359832287,
        -0.00305683515034616,
        0.0001466856338083744,
        0.013803308829665184,
        -0.008348556235432625,
        0.017648128792643547,
        0.0006079705781303346,
        -0.012302063405513763,
        -0.020976681262254715,
        -0.029345616698265076,
        0.001504642190411687,
        -0.014849426224827766,
        -0.023394977673888206,
        -0.014672808349132538,
        0.031193304806947708,
        0.010685336776077747,
        -0.01571892574429512,
        -0.01691448874771595,
        -0.020840823650360107,
        -0.03855688124895096,
        -0.00805645901709795,
        -0.007723603397607803,
        0.017267722636461258,
        -0.0014180318685248494,
        0.017403582111001015,
        0.027593031525611877,
        0.02425089292228222,
        0.007105443626642227,
        -0.003943317569792271,
        0.0024964152835309505,
        0.000519237422849983,
        -0.0002857291838154197,
        -0.02801419608294964,
        -0.019061066210269928,
        -0.0009433733066543937,
        -0.047414910048246384,
        -0.000902615487575531,
        0.04279569163918495,
        -0.018667073920369148,
        0.017607370391488075,
        0.05630011111497879,
        -0.008993888273835182,
        -0.016425393521785736,
        -0.001525870175100863,
        0.015161902643740177,
        0.010196243412792683,
        0.02980753965675831,
        0.015977058559656143,
        -0.026207266375422478,
        -0.00731602543964982,
        0.026709945872426033,
        0.00895992387086153,
        -0.0329594761133194,
        -0.0018120239255949855,
        0.01653408259153366,
        -0.003362518735229969,
        0.003048344049602747,
        -0.019210509955883026,
        0.01650691032409668,
        -0.0030636282172054052,
        -0.008858028799295425,
        0.0046531823463737965,
        0.0147271528840065,
        -0.02967168018221855,
        -0.0028530461713671684,
        0.029101070016622543,
        -0.019400713965296745,
        -0.021397845819592476,
        0.022606994956731796,
        -0.01400709804147482,
        -0.0023690471425652504,
        0.016221605241298676,
        -0.014740738086402416,
        0.006843914277851582,
        -0.004707525949925184,
        0.0009951696265488863,
        -0.01847686991095543,
        -0.0004294428799767047,
        -0.029345616698265076,
        0.028122883290052414,
        -0.023150430992245674,
        0.030731383711099625,
        0.011106501333415508,
        -0.02942713350057602,
        0.006497472990304232,
        0.0005608443170785904,
        -0.020243041217327118,
        -0.014686394482851028,
        -0.014876597560942173,
        0.009299571625888348,
        -0.009007474407553673,
        0.014088613912463188,
        0.0007591140456497669,
        -0.01351121161133051,
        -0.015610238537192345,
        -0.00285983900539577,
        0.01832742430269718,
        -0.006847310811281204,
        -0.00011325149534968659,
        0.23715606331825256,
        -0.03877425566315651,
        0.010936676524579525,
        0.0009178996551781893,
        -0.019183339551091194,
        0.0034678096417337656,
        -0.008531966246664524,
        0.010950262658298016,
        -0.009170505218207836,
        -0.0037667001597583294,
        0.02168315090239048,
        0.002808891935274005,
        -0.010705715976655483,
        0.0050437781028449535,
        0.016411809250712395,
        -0.01331421546638012,
        -0.013090047053992748,
        -0.021139713004231453,
        -0.015542308799922466,
        0.005030191969126463,
        0.010006040334701538,
        0.020637033507227898,
        -0.013769344426691532,
        -0.02226734533905983,
        0.03228018060326576,
        -0.002003925386816263,
        0.0007862859056331217,
        -0.004143709782510996,
        0.0354321151971817,
        0.011446149088442326,
        -0.009041438810527325,
        0.005003020167350769,
        -0.000519237422849983,
        -0.0013653864152729511,
        -0.0029005969408899546,
        0.003051740350201726,
        0.006072912365198135,
        0.009428638033568859,
        0.019061066210269928,
        -0.0006410862552002072,
        0.04154578596353531,
        0.021166885271668434,
        -0.008708584122359753,
        -0.019781120121479034,
        0.010637786239385605,
        0.009795458056032658,
        -0.020759306848049164,
        -0.012302063405513763,
        -0.008008908480405807,
        0.029698852449655533,
        -0.032089974731206894,
        0.022905884310603142,
        0.02465846948325634,
        0.037306975573301315,
        -0.0007421316695399582,
        -0.01732206530869007,
        0.00954411830753088,
        0.0015487964265048504,
        0.03279644623398781,
        -0.012994945980608463,
        -0.017362823709845543,
        0.009292778559029102,
        -0.005828365683555603,
        0.023367807269096375,
        -0.0037395283579826355,
        0.02627519704401493,
        -0.038692738860845566,
        0.04312175512313843,
        0.02268850989639759,
        0.014102199114859104,
        0.008919165469706059,
        -0.00975470058619976,
        -0.013558762148022652,
        0.004568270407617092,
        -0.02624802477657795,
        -0.02490301802754402,
        0.016941659152507782,
        0.01709110476076603,
        0.015555894933640957,
        0.03448110073804855,
        0.002479432849213481,
        -0.019047480076551437,
        -0.027538688853383064,
        -0.02209072932600975,
        -0.018585557118058205,
        -0.04298589378595352,
        -0.0010792326647788286,
        -0.020025666803121567,
        0.0010299836285412312,
        -0.03521474078297615,
        0.01729489490389824,
        -0.032334521412849426,
        -0.006168013904243708,
        -0.013490832410752773,
        0.02327270433306694,
        0.010732888244092464,
        0.004731301683932543,
        0.018177980557084084,
        0.010848368518054485,
        -0.010026419535279274,
        -0.0349973663687706,
        0.056626174598932266,
        0.017580198124051094,
        -0.0033455363009124994,
        -0.014292402192950249,
        0.004659975413233042,
        0.0033828974701464176,
        0.0019954340532422066,
        0.012383579276502132,
        -0.01074647344648838,
        0.0038482157979160547,
        -0.01532493345439434,
        0.013354972936213017,
        -0.016221605241298676,
        0.0037157528568059206,
        0.0009289382142014802,
        0.011955621652305126,
        0.012369993142783642,
        0.03024228848516941,
        -0.018001362681388855,
        0.003722545923665166,
        -0.019414300099015236,
        -0.0002935835509561002,
        0.030079258605837822,
        0.0014358634361997247,
        -0.01493094116449356,
        0.003742924891412258,
        0.007234510034322739,
        -0.02310967445373535,
        0.0024318823125213385,
        -0.004102952312678099,
        -0.016004230827093124,
        0.01123556774109602,
        -0.02107178419828415,
        -0.03575817868113518,
        0.013993511907756329,
        0.009965282864868641,
        -0.004975848365575075,
        -0.017226964235305786,
        0.012118653394281864,
        0.00012758040975313634,
        0.004038419108837843,
        0.005233981180936098,
        -0.01830025389790535,
        -0.011758625507354736,
        -0.009584876708686352,
        0.013205528259277344,
        -0.015868371352553368,
        -0.006881275679916143,
        -0.008776513859629631,
        -0.01811004988849163,
        -0.028394602239131927,
        -0.0058079869486391544,
        -0.03524191305041313,
        0.020039252936840057,
        0.010603821836411953,
        -0.008389314636588097,
        -0.022376032546162605,
        0.017362823709845543,
        0.009109368547797203,
        -0.020840823650360107,
        -0.005729867611080408,
        0.0032860978972166777,
        -0.023924829438328743,
        -0.001446052803657949,
        -0.004181071184575558,
        -0.17389994859695435,
        -0.011317082680761814,
        0.016778629273176193,
        -0.040241535753011703,
        0.008898787200450897,
        -0.0012066008057445288,
        0.01709110476076603,
        -0.02171032316982746,
        -0.038502536714076996,
        0.012981359846889973,
        0.02085440792143345,
        0.00885123573243618,
        -0.027008837088942528,
        -0.017063932493329048,
        -0.017036762088537216,
        0.004619217477738857,
        0.0025337766855955124,
        0.021737493574619293,
        0.0030670245178043842,
        0.0015411543427035213,
        0.042034879326820374,
        -0.0326605848968029,
        0.017661714926362038,
        -0.014075027778744698,
        0.00945581030100584,
        -0.0028615372721105814,
        -0.002742660464718938,
        0.009985661134123802,
        -0.0026662396267056465,
        -0.03722545877099037,
        0.008599895983934402,
        0.006239340174943209,
        0.025691000744700432,
        -0.009184091351926327,
        0.0023147035390138626,
        0.017199791967868805,
        0.016438979655504227,
        -0.006888068746775389,
        -0.009347123093903065,
        0.0025949133560061455,
        0.031111789867281914,
        0.01929202675819397,
        0.016547666862607002,
        -0.01400709804147482,
        -0.019061066210269928,
        0.020786479115486145,
        0.020134354010224342,
        -0.014387504197657108,
        0.021723909303545952,
        -0.019726775586605072,
        0.00318420329131186,
        -0.00817193929105997,
        -0.0016048384131863713,
        0.013151183724403381,
        0.023965587839484215,
        0.009190884418785572,
        -0.0033795011695474386,
        0.01666994020342827,
        -0.014795082621276379,
        0.004391652997583151,
        -0.00019243202405050397,
        -0.024617712944746017,
        0.011330668814480305,
        -0.017145449295639992,
        -0.02725338377058506,
        -0.01262133289128542,
        -0.017756815999746323,
        0.0047245086170732975,
        0.0006156126619316638,
        0.011004606261849403,
        -0.015447206795215607,
        -0.013844067230820656,
        -0.0068608964793384075,
        0.00015676894690841436,
        0.010474755428731441,
        0.032905131578445435,
        -0.009238434955477715,
        -0.005444562993943691,
        0.019821878522634506,
        0.009978868998587132,
        -0.013185149058699608,
        0.032714929431676865,
        -0.018599143251776695,
        0.018965963274240494,
        -0.006385388784110546,
        0.0009892258094623685,
        0.0009255417389795184,
        0.018558386713266373,
        -0.010624200105667114,
        -0.01869424618780613,
        0.005186430178582668,
        -0.03024228848516941,
        0.013205528259277344,
        0.0012210358399897814,
        0.0019852446857839823,
        0.011371427215635777,
        0.02310967445373535,
        0.0039161453023552895,
        0.004201449919492006,
        -0.015474379062652588,
        0.0053902193903923035,
        0.003841422963887453,
        0.009319950826466084,
        0.029399961233139038,
        0.011588801629841328,
        0.027959851548075676,
        -0.024413922801613808,
        0.013816894963383675,
        0.029726022854447365,
        0.0038244405295699835,
        -0.02429164946079254,
        -0.016642769798636436,
        0.015053214505314827,
        0.020650619640946388,
        0.013796515762805939,
        0.015352105721831322,
        -0.0035459287464618683,
        -0.017947018146514893,
        0.011208395473659039,
        -0.014061441645026207,
        0.03415503725409508,
        0.0034763009753078222,
        -0.01793343387544155,
        0.02168315090239048,
        -0.02207714319229126,
        -0.04717036336660385,
        -0.09417769312858582,
        -0.038094960153102875,
        -0.019658846780657768,
        0.0047143190167844296,
        -0.003844819264486432,
        0.016574839130043983,
        0.02688656374812126,
        0.018830103799700737,
        0.00039420436951331794,
        0.03282361850142479,
        -0.019047480076551437,
        -0.013558762148022652,
        0.002316401805728674,
        -0.02687297761440277,
        0.008104009553790092,
        0.0024607523810118437,
        0.0016260665142908692,
        0.004710922483354807,
        -0.017960604280233383,
        0.01969960518181324,
        0.0030789123848080635,
        -0.030785726383328438,
        0.032714929431676865,
        -0.03184542804956436,
        0.031138960272073746,
        -0.026207266375422478,
        -0.031763914972543716,
        0.008307798765599728,
        -0.0024641486816108227,
        -0.016778629273176193,
        -0.012336027808487415,
        -0.02926410175859928,
        9.860416321316734e-05,
        -0.014944527298212051,
        0.0032657189294695854,
        -0.008756134659051895,
        -0.016194432973861694,
        -0.027348484843969345,
        -0.003950110170990229,
        -0.029970571398735046,
        0.0020327954553067684,
        0.0007306685438379645,
        0.0004997076466679573,
        -0.01592271402478218,
        -0.012132239528000355,
        -0.04002416133880615,
        -0.010963848792016506,
        0.014387504197657108,
        0.011317082680761814,
        -0.007295646704733372,
        -0.014876597560942173,
        -0.008919165469706059,
        -0.03399200737476349,
        -0.007064685691148043,
        0.01313080545514822,
        0.012852293439209461,
        0.0032708137296140194,
        0.004452789667993784,
        -0.011147258803248405,
        -0.02171032316982746,
        0.007954563945531845,
        -0.007397541310638189,
        -0.01892520673573017,
        0.0072888536378741264,
        0.0010936676990240812,
        -0.004371274262666702,
        -0.00934033002704382,
        -0.025568727403879166,
        0.012600953690707684,
        -0.0306226946413517,
        0.0029464494436979294,
        0.014469020068645477,
        0.0035798936150968075,
        0.019604502245783806,
        -0.021207643672823906,
        -0.010705715976655483,
        -0.015515136532485485,
        0.021574463695287704,
        0.005845348350703716,
        -0.01609933190047741,
        -0.03228018060326576,
        -0.022729268297553062,
        0.01729489490389824,
        -0.025025291368365288,
        -0.006803156342357397,
        -0.00766246672719717,
        -0.024644885212183,
        -0.00037127811810933053,
        -0.0038040615618228912,
        -0.029698852449655533,
        0.012668883427977562,
        0.03045966476202011,
        -0.0013450074475258589,
        -0.013191942125558853,
        -0.006235943641513586,
        0.0068608964793384075,
        -0.025758931413292885,
        0.004374670796096325,
        0.006096688099205494,
        0.045458536595106125,
        -0.01364027801901102,
        -0.00641595758497715,
        -0.052822113037109375,
        0.008803685195744038,
        0.0031909963581711054,
        -0.022946642711758614,
        0.009218056686222553,
        0.023775383830070496,
        0.037497177720069885,
        0.009421844966709614,
        -0.013470453210175037,
        0.030731383711099625,
        -0.01073968131095171,
        0.02168315090239048,
        0.013049289584159851,
        0.0001267312909476459,
        -0.012994945980608463,
        -0.024821501225233078,
        0.02385690063238144,
        -0.021968455985188484,
        0.015814026817679405,
        0.024726400151848793,
        -0.003197789192199707,
        -0.011378219351172447,
        0.012859086506068707,
        0.01253981702029705,
        -0.0019173149485141039,
        -0.0010130011942237616,
        -0.015433620661497116,
        0.014686394482851028,
        -0.01733565144240856,
        -0.02268850989639759,
        -0.002095630392432213,
        -0.019590916112065315,
        0.006246133241802454,
        0.00995169673115015,
        -0.010889125987887383,
        -0.002598309889435768,
        0.0007387351361103356,
        0.008830857463181019,
        0.02268850989639759,
        0.009462603367865086,
        -0.01194203644990921,
        -0.026112165302038193,
        0.02882935293018818,
        -0.025976305827498436,
        0.0027494532987475395,
        -0.0033964836038649082,
        0.004632803611457348,
        -0.002212808933109045,
        -0.003817647462710738,
        -0.006483886856585741,
        0.024155789986252785,
        0.02108537033200264,
        0.009435431100428104,
        -0.022973814979195595,
        -0.01631670631468296,
        0.005447959527373314,
        0.013551969081163406,
        0.026152923703193665,
        -0.026913736015558243,
        -0.009809044189751148,
        0.017607370391488075,
        0.007825498469173908,
        0.002411503344774246,
        -0.00325383129529655,
        0.0016421998152509332,
        0.012859086506068707,
        0.019237682223320007,
        -0.0017186206532642245,
        -0.009197677485644817,
        -0.034535445272922516,
        -0.0014018985675647855,
        -0.0013008532114326954,
        0.012071102857589722,
        0.011575215496122837,
        0.03564948961138725,
        0.006229150574654341,
        0.00017884607950691134,
        0.00716658029705286,
        -0.01463205087929964,
        0.03662768006324768,
        0.015596652403473854,
        -0.028965210542082787,
        -0.01342969574034214,
        -0.017770402133464813,
        0.004829799756407738,
        -0.0027358673978596926,
        0.006796363741159439,
        -0.0024556575808674097,
        -0.01402068417519331,
        -0.0037870791275054216,
        0.005712885409593582,
        0.007988529279828072,
        0.002396219177171588,
        0.0053800297901034355,
        -0.004089366178959608,
        0.009809044189751148,
        0.010984227992594242,
        0.00021461529831867665,
        0.012607746757566929,
        -0.013592727482318878,
        0.001277926960028708,
        0.0013322706799954176,
        -0.004595442209392786,
        -0.005634766072034836,
        -0.014536949805915356,
        0.020990267395973206,
        -0.019020307809114456,
        -0.028313087299466133,
        0.00040587977855466306,
        0.0034151640720665455,
        0.010298137553036213,
        0.020243041217327118,
        -0.015257003717124462,
        0.0015292667085304856,
        -0.02861197665333748,
        0.03423655405640602,
        0.013735379092395306,
        -0.017457924783229828,
        -0.008498001843690872,
        0.007995322346687317,
        0.007085064426064491,
        -0.00015952858666423708,
        0.028557633981108665,
        0.0013874635333195329,
        0.01592271402478218,
        0.021642392501235008,
        0.04121972247958183,
        -0.010481548495590687,
        0.00894633773714304,
        0.008287419565021992,
        -0.013253078795969486,
        -0.017145449295639992,
        -0.005529474932700396,
        -0.019020307809114456,
        -0.00023456964117940515,
        0.0033217607997357845,
        0.0027137903962284327,
        0.022172244265675545,
        -0.0034253536723554134,
        0.0892324149608612,
        0.02586761862039566,
        0.004996227100491524,
        0.0027341691311448812,
        -0.0010401731124147773,
        0.025813274085521698,
        0.019210509955883026,
        0.014346746727824211,
        -0.0023418753407895565,
        0.00481961015611887,
        0.02449543960392475,
        0.01633029244840145,
        0.006545023526996374,
        -0.012492266483604908,
        -0.014795082621276379,
        0.006239340174943209,
        0.011595594696700573,
        -0.0030924982856959105,
        0.01787908934056759,
        -0.013103633187711239,
        0.046056315302848816,
        -0.026492571458220482,
        0.007458677981048822,
        0.011955621652305126,
        -0.03461695834994316,
        -0.006049137096852064,
        0.010760059580206871,
        0.015664581209421158,
        -0.018667073920369148,
        -0.039073146879673004,
        0.019658846780657768,
        -0.002095630392432213,
        -0.028150055557489395,
        -0.012845500372350216,
        0.022213002666831017,
        -0.002409805078059435,
        -0.001168390386737883,
        -0.005818176083266735,
        0.018245909363031387,
        0.010617407038807869,
        0.01501245703548193,
        0.01690090261399746,
        -0.004459582734853029,
        -0.03497019410133362,
        -0.01194203644990921,
        -0.0059676216915249825,
        -0.02008001133799553,
        -0.027348484843969345,
        -0.016588425263762474
      ]
    },
    {
      "node": "check_cache_answer",
      "flow_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_check_cache_answer_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.022036384791135788,
          -0.0164797380566597,
          0.0005999039276503026,
          -0.007227716967463493,
          0.003868594765663147,
          0.01381010189652443,
          -0.006500869523733854,
          -0.011711074970662594,
          -0.01653408259153366,
          -0.015121144242584705,
          0.0027137903962284327,
          0.027362070977687836,
          -0.025758931413292885,
          0.00944222416728735,
          3.94045164284762e-05,
          0.0016226699808612466,
          0.016561252996325493,
          0.015487965196371078,
          0.004052004776895046,
          0.0009892258094623685,
          -0.005791004281491041,
          -0.014170128852128983,
          -0.019210509955883026,
          0.020990267395973206,
          -0.006324252113699913,
          -0.02627519704401493,
          0.05154503509402275,
          -0.033502914011478424,
          0.00033030801569111645,
          -0.013558762148022652,
          0.027144696563482285,
          0.030269460752606392,
          -0.010019626468420029,
          -0.008131181821227074,
          -0.013069668784737587,
          0.0003339167742524296,
          0.01770247146487236,
          -0.002297721104696393,
          0.017009589821100235,
          -0.01224092673510313,
          0.015800440683960915,
          0.01892520673573017,
          0.013878031633794308,
          -0.001744094304740429,
          -0.0005765530513599515,
          0.003309873165562749,
          0.01252623088657856,
          -0.004153899382799864,
          0.0005892898770980537,
          -0.023788969963788986,
          0.018653487786650658,
          0.01949581503868103,
          -0.02384331449866295,
          -0.002343573607504368,
          -0.03442675620317459,
          -0.0030534386169165373,
          -0.0077575682662427425,
          0.02582686021924019,
          0.009068611077964306,
          -0.02248472161591053,
          0.010753266513347626,
          0.0006746265571564436,
          -0.015664581209421158,
          0.019156167283654213,
          0.0048841433599591255,
          -0.009299571625888348,
          -0.022946642711758614,
          0.029101070016622543,
          0.010603821836411953,
          0.008885201066732407,
          0.007628501858562231,
          0.020623447373509407,
          0.019455058500170708,
          -0.011086122132837772,
          0.03654616326093674,
          -0.002844554837793112,
          -0.01502604316920042,
          -0.005852140951901674,
          0.008036079816520214,
          0.0004491849394980818,
          0.006405767984688282,
          0.02306891605257988,
          -0.02109895646572113,
          0.020922338590025902,
          0.013062875717878342,
          -0.007594536989927292,
          0.013103633187711239,
          0.01690090261399746,
          -0.019577331840991974,
          -0.0003455921832937747,
          0.006201978772878647,
          0.002939656376838684,
          0.03314967826008797,
          0.004282965790480375,
          0.000431353400927037,
          0.03024228848516941,
          -0.004602235276252031,
          0.029101070016622543,
          0.0016498418990522623,
          -0.035106055438518524,
          0.009027853608131409,
          -0.0075266072526574135,
          -0.007940978743135929,
          -0.01224092673510313,
          -0.019224096089601517,
          0.0021533705294132233,
          0.00983621645718813,
          -0.010372860357165337,
          0.028150055557489395,
          -0.008076838217675686,
          -0.007716810330748558,
          0.03203563392162323,
          -0.0034202588722109795,
          -0.020215868949890137,
          0.005529474932700396,
          0.021533705294132233,
          0.0010299836285412312,
          0.0005413145408965647,
          0.017457924783229828,
          0.020732134580612183,
          0.03328553959727287,
          -0.002212808933109045,
          0.0011565026361495256,
          -0.01827308163046837,
          0.022348862141370773,
          -0.008137974888086319,
          -0.021615220233798027,
          -0.019998494535684586,
          0.008701791055500507,
          -0.029481476172804832,
          0.006055930163711309,
          -0.0036444268189370632,
          0.0013798214495182037,
          -0.023816142231225967,
          -0.002878519706428051,
          0.023191189393401146,
          -0.00796135701239109,
          -0.010107934474945068,
          -0.026424642652273178,
          -0.01768888533115387,
          0.016819385811686516,
          0.022593408823013306,
          -0.023177603259682655,
          -0.03518756851553917,
          -0.026601258665323257,
          0.02546004019677639,
          0.0007625105208717287,
          0.02842177450656891,
          0.010114727541804314,
          0.002683222061023116,
          0.0042456043884158134,
          -0.031138960272073746,
          -0.018055707216262817,
          0.009591669775545597,
          0.0022875317372381687,
          0.013979925774037838,
          -0.00032075541093945503,
          0.0026458606589585543,
          -0.000471262086648494,
          -0.00011494974023662508,
          0.0030279650818556547,
          0.014550535008311272,
          0.011554837226867676,
          -0.009598462842404842,
          0.03760586678981781,
          0.04051325470209122,
          -0.004279569257050753,
          -0.0026611448265612125,
          0.012397164478898048,
          0.003573100548237562,
          -0.008688204921782017,
          0.01570533961057663,
          -0.026981664821505547,
          0.004629407078027725,
          0.015433620661497116,
          0.00746547058224678,
          0.017430752515792847,
          0.007805119268596172,
          0.0007680298294872046,
          -0.013361766003072262,
          -0.0051320865750312805,
          0.006398974917829037,
          0.03399200737476349,
          0.005003020167350769,
          -0.015257003717124462,
          -0.002204317832365632,
          0.003770096693187952,
          -0.035350602120161057,
          0.0028004006016999483,
          -0.044643379747867584,
          0.018205150961875916,
          0.019061066210269928,
          0.011711074970662594,
          -0.004673561546951532,
          -0.6364738345146179,
          -0.014455433934926987,
          0.016629183664917946,
          -0.00835534930229187,
          0.021329917013645172,
          -0.013083254918456078,
          -0.010352482087910175,
          -0.00566193787381053,
          -0.01171786803752184,
          0.03263341262936592,
          0.005447959527373314,
          -0.012607746757566929,
          -0.005906485021114349,
          -0.01502604316920042,
          -0.0013458565808832645,
          -0.022511892020702362,
          -0.00936070829629898,
          -0.00402143644168973,
          0.01869424618780613,
          -0.003973885904997587,
          -0.02446826733648777,
          0.02124840021133423,
          -0.04380105063319206,
          0.005852140951901674,
          -0.01752585545182228,
          0.007132615428417921,
          0.002350366674363613,
          -0.009313157759606838,
          0.007295646704733372,
          -0.007268474902957678,
          -0.01323269959539175,
          0.03336705267429352,
          0.03956224024295807,
          -0.015284175984561443,
          0.031709570437669754,
          0.011860520578920841,
          -0.002382633276283741,
          0.015759684145450592,
          -0.031193304806947708,
          0.0256638303399086,
          -0.00866782572120428,
          -0.019618088379502296,
          0.02329987660050392,
          -0.007010342087596655,
          0.03217149153351784,
          0.007248095702379942,
          0.01232923474162817,
          0.021425018087029457,
          0.0018884448800235987,
          0.007431505713611841,
          -0.027606617659330368,
          0.008722169324755669,
          0.0014333160361275077,
          -0.008464036509394646,
          0.014401090331375599,
          -0.0018341010436415672,
          0.022837955504655838,
          -0.014061441645026207,
          0.0002553731028456241,
          -0.022606994956731796,
          0.011643145233392715,
          0.034345243126153946,
          -0.010902712121605873,
          -0.02567741461098194,
          -0.011004606261849403,
          0.015229832381010056,
          0.0035323428455740213,
          0.012770777568221092,
          0.011887691915035248,
          -0.04477923735976219,
          0.010481548495590687,
          0.024984532967209816,
          0.0006393880466930568,
          -0.0006792966742068529,
          0.02008001133799553,
          0.017743229866027832,
          -0.00228923000395298,
          -0.015121144242584705,
          -0.0004844234499614686,
          0.022973814979195595,
          -0.009252021089196205,
          -0.005587215535342693,
          0.012607746757566929,
          -0.0028190813027322292,
          -0.018748588860034943,
          0.017797574400901794,
          -0.006918637081980705,
          -0.02309608832001686,
          0.005352857988327742,
          0.026207266375422478,
          -0.0012125446228310466,
          0.02782399207353592,
          -0.021601635962724686,
          -0.03942637890577316,
          0.02248472161591053,
          0.012369993142783642,
          0.003501774510368705,
          -0.016642769798636436,
          0.03247038275003433,
          -0.007241302635520697,
          0.002798702334985137,
          -0.017607370391488075,
          0.020243041217327118,
          0.023761799558997154,
          0.00856593158096075,
          0.016438979655504227,
          0.02270209603011608,
          0.020637033507227898,
          0.002071854891255498,
          -0.017199791967868805,
          0.004792438354343176,
          -0.016357464715838432,
          0.00825345516204834,
          0.008776513859629631,
          -0.006633332464843988,
          -0.0329594761133194,
          0.014183714985847473,
          0.0027817199006676674,
          0.012220547534525394,
          -0.021234814077615738,
          0.0019427885999903083,
          -0.009598462842404842,
          0.012444715946912766,
          -0.016642769798636436,
          0.019210509955883026,
          0.017675301060080528,
          -0.0031536349561065435,
          0.004310137592256069,
          -0.020650619640946388,
          -0.005298514384776354,
          -0.021832596510648727,
          -0.006426146719604731,
          0.0053800297901034355,
          -0.012152617797255516,
          0.04393691197037697,
          0.03899163007736206,
          0.03260624036192894,
          -0.0015937999123707414,
          -0.02050117403268814,
          0.005006416700780392,
          -0.012315649539232254,
          -0.010495133697986603,
          0.01567816734313965,
          -0.005740057211369276,
          -0.052034128457307816,
          -0.016167260706424713,
          0.008891994133591652,
          0.003070421051234007,
          0.001690599718131125,
          -0.011554837226867676,
          0.0015428526094183326,
          -0.02624802477657795,
          -0.018585557118058205,
          -0.005899691954255104,
          -0.013090047053992748,
          -0.028557633981108665,
          -0.009408259764313698,
          -0.04668127000331879,
          -0.019536573439836502,
          -0.031709570437669754,
          0.010325309820473194,
          0.00373273529112339,
          -0.011989586986601353,
          -0.012770777568221092,
          0.0011378219351172447,
          -0.03497019410133362,
          0.005515889264643192,
          0.022213002666831017,
          -0.027525102719664574,
          -0.033910490572452545,
          0.028938040137290955,
          -0.01846328377723694,
          -0.01033210288733244,
          0.005774022080004215,
          -0.026682773604989052,
          0.013776137493550777,
          -0.03325836732983589,
          -0.016017816960811615,
          0.0003791324852500111,
          -0.016044987365603447,
          0.005030191969126463,
          0.030704211443662643,
          -0.010012833401560783,
          -0.009224848821759224,
          0.012220547534525394,
          0.0020344937220215797,
          0.017172621563076973,
          -0.00310948072001338,
          -0.02488943189382553,
          0.00974111445248127,
          0.00874934159219265,
          0.026587672531604767,
          -0.003061929950490594,
          0.02090875245630741,
          0.012573782354593277,
          0.006110273767262697,
          0.016357464715838432,
          -0.009122954681515694,
          0.0008754436275921762,
          -0.005172844510525465,
          0.025297008454799652,
          0.010732888244092464,
          0.00034325712476857007,
          -0.012940602377057076,
          -0.008294212631881237,
          0.017457924783229828,
          -8.363840606762096e-05,
          -0.016438979655504227,
          0.008219489827752113,
          -0.00042880602995865047,
          0.01461846474558115,
          -0.029508648440241814,
          -0.02325911819934845,
          -0.0005476829828694463,
          -0.008708584122359753,
          0.021044611930847168,
          -0.009068611077964306,
          0.02289229817688465,
          -0.034263726323843,
          0.0033591222018003464,
          0.003783682594075799,
          -0.01793343387544155,
          0.004975848365575075,
          0.0009900749428197742,
          -0.011765418574213982,
          0.006297080311924219,
          -0.008097216486930847,
          -0.012702848762273788,
          0.012410750612616539,
          -0.022362448275089264,
          0.0005455601494759321,
          0.028285915032029152,
          0.02999774180352688,
          0.020025666803121567,
          -0.0012032042723149061,
          -0.00856593158096075,
          0.014469020068645477,
          -0.010196243412792683,
          0.025595899671316147,
          -0.017159035429358482,
          0.0026017064228653908,
          0.01154804416000843,
          0.013110426254570484,
          -0.0015776666114106774,
          0.03545928746461868,
          0.015814026817679405,
          0.03244321048259735,
          0.010651372373104095,
          -0.016072159633040428,
          0.02287871204316616,
          -0.01431957446038723,
          -0.008878407999873161,
          -0.030731383711099625,
          0.0011514079524204135,
          0.0240606889128685,
          -0.00924522802233696,
          0.0040757800452411175,
          0.012899843975901604,
          -0.0057094888761639595,
          0.033883318305015564,
          0.016357464715838432,
          0.008362142369151115,
          0.022797197103500366,
          0.007805119268596172,
          0.019183339551091194,
          -0.014455433934926987,
          -0.014591293409466743,
          -0.0058079869486391544,
          -0.009184091351926327,
          -0.004938486963510513,
          -0.017254136502742767,
          -0.015080386772751808,
          0.0010206432780250907,
          -0.010617407038807869,
          0.01441467646509409,
          0.03024228848516941,
          -0.02490301802754402,
          0.021438604220747948,
          -0.0067929672077298164,
          0.034127864986658096,
          -0.0023045141715556383,
          -0.029916226863861084,
          0.03160088136792183,
          -0.006130652967840433,
          -0.027959851548075676,
          -0.0171318631619215,
          0.0032283575274050236,
          -0.02169673703610897,
          -0.004269379656761885,
          -0.018830103799700737,
          0.0016956944018602371,
          0.009007474407553673,
          -0.0019852446857839823,
          0.025120392441749573,
          0.00111829221714288,
          0.013293836265802383,
          0.007302439771592617,
          -0.01986263506114483,
          0.004432410933077335,
          0.004486754536628723,
          0.0019190132152289152,
          0.00785946287214756,
          -0.006490679923444986,
          -0.028965210542082787,
          0.02029738575220108,
          0.001125934300944209,
          -0.029209759086370468,
          -0.016642769798636436,
          -0.016466151922941208,
          -0.015977058559656143,
          0.038257990032434464,
          0.009462603367865086,
          0.01868066005408764,
          -0.019346369430422783,
          -5.301698547555134e-05,
          0.0292369294911623,
          0.007893427275121212,
          -0.006996755953878164,
          0.008511587977409363,
          0.004408635664731264,
          -0.028313087299466133,
          -0.012268098071217537,
          -0.01671069860458374,
          0.008436865173280239,
          0.04317609965801239,
          0.04755076766014099,
          -0.004174278117716312,
          0.019020307809114456,
          -0.012757192365825176,
          0.03635596111416817,
          -0.025758931413292885,
          -0.0389372855424881,
          -0.021194057539105415,
          -0.01649332419037819,
          0.0015802140114828944,
          -0.02664201706647873,
          0.006531437858939171,
          -0.022810783237218857,
          0.005254359915852547,
          0.01025058701634407,
          -0.0003099291061516851,
          -0.02626161091029644,
          0.033747460693120956,
          -0.016547666862607002,
          -0.0009994152933359146,
          0.013701414689421654,
          0.006076308898627758,
          0.010467962361872196,
          0.001453694887459278,
          -0.019427886232733727,
          -0.012254512868821621,
          0.010807610116899014,
          -0.0035323428455740213,
          -0.011588801629841328,
          -0.016561252996325493,
          -0.022579822689294815,
          -0.01753944158554077,
          0.00787304900586605,
          -0.02051476016640663,
          0.03939921036362648,
          0.001176881487481296,
          0.006545023526996374,
          0.011004606261849403,
          0.020025666803121567,
          0.0029600353445857763,
          0.02662843093276024,
          -0.002523587318137288,
          -0.01540644932538271,
          7.790684321662411e-05,
          -0.030378147959709167,
          0.004031626041978598,
          0.016574839130043983,
          0.011317082680761814,
          -0.002888709306716919,
          0.0012337726075202227,
          -0.004809420555830002,
          -0.04111103713512421,
          -0.005013209767639637,
          0.014469020068645477,
          -0.014699980616569519,
          0.012471887283027172,
          0.010196243412792683,
          -0.01827308163046837,
          -0.02385690063238144,
          -0.017403582111001015,
          -0.014047855511307716,
          0.01872141659259796,
          -0.007275267504155636,
          0.00365801271982491,
          0.004639596678316593,
          -0.017362823709845543,
          -0.0244275089353323,
          -0.015596652403473854,
          0.04627368971705437,
          -0.008212696760892868,
          -0.04032305255532265,
          -0.04727904871106148,
          -0.016765043139457703,
          0.01221375446766615,
          0.011486907489597797,
          0.03260624036192894,
          -0.020786479115486145,
          -0.001669371617026627,
          -0.0023147035390138626,
          -0.007574158255010843,
          -0.03445392847061157,
          0.0223081037402153,
          -0.04051325470209122,
          0.0038006650283932686,
          -0.01102498546242714,
          -0.00746547058224678,
          -0.027565861120820045,
          -0.019332783296704292,
          0.0354321151971817,
          0.002744358731433749,
          0.005644955672323704,
          0.02961733564734459,
          -0.028965210542082787,
          0.01846328377723694,
          0.019984908401966095,
          0.0072005451656877995,
          0.005634766072034836,
          0.012886258773505688,
          -0.028313087299466133,
          -0.011955621652305126,
          -0.009285985492169857,
          -0.010420411825180054,
          -0.03325836732983589,
          0.03165522590279579,
          -0.006307269912213087,
          0.0036274443846195936,
          -0.021126126870512962,
          0.01733565144240856,
          -0.01072609517723322,
          0.008925958536565304,
          -0.023394977673888206,
          -0.00310948072001338,
          -0.01711827702820301,
          0.0313563346862793,
          -0.0038855771999806166,
          -0.001333968946710229,
          0.024970946833491325,
          -0.012268098071217537,
          -0.0019478832837194204,
          0.0025490608531981707,
          -0.02308250218629837,
          0.016588425263762474,
          0.019006721675395966,
          -0.0026017064228653908,
          0.021520119160413742,
          0.005498906597495079,
          -0.01688731648027897,
          -0.007037513889372349,
          0.025609485805034637,
          -0.024509025737643242,
          0.01668352633714676,
          -0.0017101294361054897,
          -0.0278511643409729,
          -0.03143785148859024,
          -0.016832971945405006,
          -0.002584723988547921,
          0.008932751603424549,
          -0.01461846474558115,
          -0.0012813233770430088,
          0.012179790064692497,
          -0.0006381143466569483,
          0.009992454200983047,
          -0.013225906528532505,
          0.01889803446829319,
          -0.022960228845477104,
          -0.014808667823672295,
          0.008531966246664524,
          0.001946185017004609,
          0.03206280618906021,
          -0.017634542658925056,
          0.02999774180352688,
          -0.030079258605837822,
          -0.0076692597940564156,
          0.0005748548428528011,
          -0.02983471192419529,
          -0.0004895181627944112,
          0.004462979268282652,
          0.03673636540770531,
          0.02324553392827511,
          0.03184542804956436,
          -0.000315448414767161,
          0.01082798931747675,
          0.01501245703548193,
          -0.0043169306591153145,
          -0.006378596182912588,
          -0.028394602239131927,
          -0.003245339961722493,
          -0.022117899730801582,
          0.017172621563076973,
          -0.001829006359912455,
          -0.013633484952151775,
          -0.008260248228907585,
          0.0026662396267056465,
          0.003905955934897065,
          0.002542268019169569,
          0.0028343654703348875,
          0.006945808883756399,
          0.0010180958779528737,
          -0.005757039412856102,
          -0.03635596111416817,
          0.02325911819934845,
          -0.004198053851723671,
          -0.0004954620380885899,
          -0.042279426008462906,
          -0.025595899671316147,
          0.013484039343893528,
          0.011065742932260036,
          0.014903769828379154,
          0.012397164478898048,
          0.013721792958676815,
          -0.015243417583405972,
          0.0006584932561963797,
          -0.008531966246664524,
          -0.003461016807705164,
          -0.001688052318058908,
          -0.014890183694660664,
          -0.029454305768013,
          -0.016846558079123497,
          0.013966340571641922,
          0.028557633981108665,
          -0.021139713004231453,
          -0.003255529562011361,
          -0.010603821836411953,
          0.01628953404724598,
          0.008321384899318218,
          0.01709110476076603,
          -0.008606689050793648,
          0.004378067329525948,
          -0.0013467057142406702,
          -0.009768286719918251,
          -0.03638312965631485,
          -0.028693493455648422,
          -0.0037633036263287067,
          0.036301616579294205,
          0.0034525254741311073,
          -0.02328629046678543,
          0.026519743725657463,
          0.011969207786023617,
          0.0026203871238976717,
          -0.01569175347685814,
          0.004496944136917591,
          0.05230584740638733,
          0.02051476016640663,
          0.011887691915035248,
          0.029481476172804832,
          -0.0005655144923366606,
          0.0016761645674705505,
          0.01870783045887947,
          -0.0005846197018399835,
          -0.014346746727824211,
          -0.004975848365575075,
          -0.007594536989927292,
          0.006755605805665255,
          -0.008321384899318218,
          0.02429164946079254,
          0.02844894677400589,
          -0.018802933394908905,
          -0.01164993830025196,
          0.0006028758361935616,
          -0.00596422515809536,
          0.012471887283027172,
          0.013572348281741142,
          -0.01690090261399746,
          -0.00231130700558424,
          0.012397164478898048,
          -0.026981664821505547,
          0.016602011397480965,
          -0.006602764129638672,
          -0.005070949904620647,
          0.012132239528000355,
          0.006830328144133091,
          -0.012927016243338585,
          0.014428261667490005,
          0.020392486825585365,
          0.0012550007086247206,
          0.001064797630533576,
          0.02070496417582035,
          -0.015841199085116386,
          -0.027389243245124817,
          0.02861197665333748,
          0.030514007434248924,
          -0.014958113431930542,
          0.015134730376303196,
          0.008973509073257446,
          0.014958113431930542,
          0.01589554361999035,
          0.012763985432684422,
          -0.00374971772544086,
          0.027008837088942528,
          -0.035731006413698196,
          -0.008593102917075157,
          -0.013253078795969486,
          -0.024128619581460953,
          0.025568727403879166,
          -0.0280685406178236,
          0.003950110170990229,
          -0.0007968999561853707,
          -0.017362823709845543,
          0.0157460980117321,
          0.03200846165418625,
          0.013171562924981117,
          -0.0250796340405941,
          0.005325686186552048,
          -0.006157824769616127,
          0.01313080545514822,
          -0.009904146194458008,
          0.0034915851429104805,
          0.02168315090239048,
          -0.03043249249458313,
          -0.022158658131957054,
          0.010026419535279274,
          -0.0067623988725245,
          0.01692807301878929,
          -0.015460792928934097,
          -0.0007009492837823927,
          0.006840517744421959,
          0.028476117178797722,
          -0.04290438070893288,
          -0.027511516585946083,
          -0.005994793493300676,
          0.0066061606630682945,
          -0.01849045604467392,
          -0.005111707840114832,
          -0.006235943641513586,
          -0.01806929148733616,
          0.01493094116449356,
          -0.011466528289020061,
          0.005991396959871054,
          -0.011018192395567894,
          -0.003729338990524411,
          -0.009095782414078712,
          -0.0018952378304675221,
          -0.03757869452238083,
          -0.016955245286226273,
          0.0071801659651100636,
          0.016411809250712395,
          -0.006619746331125498,
          -0.009720736183226109,
          -0.010046797804534435,
          0.022593408823013306,
          -0.0049724518321454525,
          0.013667449355125427,
          -0.03423655405640602,
          -0.003525549778714776,
          0.0023265911731868982,
          0.007397541310638189,
          -0.011969207786023617,
          -0.042279426008462906,
          -0.004378067329525948,
          -0.018639901652932167,
          0.006735226605087519,
          -0.018599143251776695,
          -0.011344254948198795,
          0.006898257881402969,
          0.005896295420825481,
          -0.0025303801521658897,
          -0.013762551359832287,
          -0.00305683515034616,
          0.0001466856338083744,
          0.013803308829665184,
          -0.008348556235432625,
          0.017648128792643547,
          0.0006079705781303346,
          -0.012302063405513763,
          -0.020976681262254715,
          -0.029345616698265076,
          0.001504642190411687,
          -0.014849426224827766,
          -0.023394977673888206,
          -0.014672808349132538,
          0.031193304806947708,
          0.010685336776077747,
          -0.01571892574429512,
          -0.01691448874771595,
          -0.020840823650360107,
          -0.03855688124895096,
          -0.00805645901709795,
          -0.007723603397607803,
          0.017267722636461258,
          -0.0014180318685248494,
          0.017403582111001015,
          0.027593031525611877,
          0.02425089292228222,
          0.007105443626642227,
          -0.003943317569792271,
          0.0024964152835309505,
          0.000519237422849983,
          -0.0002857291838154197,
          -0.02801419608294964,
          -0.019061066210269928,
          -0.0009433733066543937,
          -0.047414910048246384,
          -0.000902615487575531,
          0.04279569163918495,
          -0.018667073920369148,
          0.017607370391488075,
          0.05630011111497879,
          -0.008993888273835182,
          -0.016425393521785736,
          -0.001525870175100863,
          0.015161902643740177,
          0.010196243412792683,
          0.02980753965675831,
          0.015977058559656143,
          -0.026207266375422478,
          -0.00731602543964982,
          0.026709945872426033,
          0.00895992387086153,
          -0.0329594761133194,
          -0.0018120239255949855,
          0.01653408259153366,
          -0.003362518735229969,
          0.003048344049602747,
          -0.019210509955883026,
          0.01650691032409668,
          -0.0030636282172054052,
          -0.008858028799295425,
          0.0046531823463737965,
          0.0147271528840065,
          -0.02967168018221855,
          -0.0028530461713671684,
          0.029101070016622543,
          -0.019400713965296745,
          -0.021397845819592476,
          0.022606994956731796,
          -0.01400709804147482,
          -0.0023690471425652504,
          0.016221605241298676,
          -0.014740738086402416,
          0.006843914277851582,
          -0.004707525949925184,
          0.0009951696265488863,
          -0.01847686991095543,
          -0.0004294428799767047,
          -0.029345616698265076,
          0.028122883290052414,
          -0.023150430992245674,
          0.030731383711099625,
          0.011106501333415508,
          -0.02942713350057602,
          0.006497472990304232,
          0.0005608443170785904,
          -0.020243041217327118,
          -0.014686394482851028,
          -0.014876597560942173,
          0.009299571625888348,
          -0.009007474407553673,
          0.014088613912463188,
          0.0007591140456497669,
          -0.01351121161133051,
          -0.015610238537192345,
          -0.00285983900539577,
          0.01832742430269718,
          -0.006847310811281204,
          -0.00011325149534968659,
          0.23715606331825256,
          -0.03877425566315651,
          0.010936676524579525,
          0.0009178996551781893,
          -0.019183339551091194,
          0.0034678096417337656,
          -0.008531966246664524,
          0.010950262658298016,
          -0.009170505218207836,
          -0.0037667001597583294,
          0.02168315090239048,
          0.002808891935274005,
          -0.010705715976655483,
          0.0050437781028449535,
          0.016411809250712395,
          -0.01331421546638012,
          -0.013090047053992748,
          -0.021139713004231453,
          -0.015542308799922466,
          0.005030191969126463,
          0.010006040334701538,
          0.020637033507227898,
          -0.013769344426691532,
          -0.02226734533905983,
          0.03228018060326576,
          -0.002003925386816263,
          0.0007862859056331217,
          -0.004143709782510996,
          0.0354321151971817,
          0.011446149088442326,
          -0.009041438810527325,
          0.005003020167350769,
          -0.000519237422849983,
          -0.0013653864152729511,
          -0.0029005969408899546,
          0.003051740350201726,
          0.006072912365198135,
          0.009428638033568859,
          0.019061066210269928,
          -0.0006410862552002072,
          0.04154578596353531,
          0.021166885271668434,
          -0.008708584122359753,
          -0.019781120121479034,
          0.010637786239385605,
          0.009795458056032658,
          -0.020759306848049164,
          -0.012302063405513763,
          -0.008008908480405807,
          0.029698852449655533,
          -0.032089974731206894,
          0.022905884310603142,
          0.02465846948325634,
          0.037306975573301315,
          -0.0007421316695399582,
          -0.01732206530869007,
          0.00954411830753088,
          0.0015487964265048504,
          0.03279644623398781,
          -0.012994945980608463,
          -0.017362823709845543,
          0.009292778559029102,
          -0.005828365683555603,
          0.023367807269096375,
          -0.0037395283579826355,
          0.02627519704401493,
          -0.038692738860845566,
          0.04312175512313843,
          0.02268850989639759,
          0.014102199114859104,
          0.008919165469706059,
          -0.00975470058619976,
          -0.013558762148022652,
          0.004568270407617092,
          -0.02624802477657795,
          -0.02490301802754402,
          0.016941659152507782,
          0.01709110476076603,
          0.015555894933640957,
          0.03448110073804855,
          0.002479432849213481,
          -0.019047480076551437,
          -0.027538688853383064,
          -0.02209072932600975,
          -0.018585557118058205,
          -0.04298589378595352,
          -0.0010792326647788286,
          -0.020025666803121567,
          0.0010299836285412312,
          -0.03521474078297615,
          0.01729489490389824,
          -0.032334521412849426,
          -0.006168013904243708,
          -0.013490832410752773,
          0.02327270433306694,
          0.010732888244092464,
          0.004731301683932543,
          0.018177980557084084,
          0.010848368518054485,
          -0.010026419535279274,
          -0.0349973663687706,
          0.056626174598932266,
          0.017580198124051094,
          -0.0033455363009124994,
          -0.014292402192950249,
          0.004659975413233042,
          0.0033828974701464176,
          0.0019954340532422066,
          0.012383579276502132,
          -0.01074647344648838,
          0.0038482157979160547,
          -0.01532493345439434,
          0.013354972936213017,
          -0.016221605241298676,
          0.0037157528568059206,
          0.0009289382142014802,
          0.011955621652305126,
          0.012369993142783642,
          0.03024228848516941,
          -0.018001362681388855,
          0.003722545923665166,
          -0.019414300099015236,
          -0.0002935835509561002,
          0.030079258605837822,
          0.0014358634361997247,
          -0.01493094116449356,
          0.003742924891412258,
          0.007234510034322739,
          -0.02310967445373535,
          0.0024318823125213385,
          -0.004102952312678099,
          -0.016004230827093124,
          0.01123556774109602,
          -0.02107178419828415,
          -0.03575817868113518,
          0.013993511907756329,
          0.009965282864868641,
          -0.004975848365575075,
          -0.017226964235305786,
          0.012118653394281864,
          0.00012758040975313634,
          0.004038419108837843,
          0.005233981180936098,
          -0.01830025389790535,
          -0.011758625507354736,
          -0.009584876708686352,
          0.013205528259277344,
          -0.015868371352553368,
          -0.006881275679916143,
          -0.008776513859629631,
          -0.01811004988849163,
          -0.028394602239131927,
          -0.0058079869486391544,
          -0.03524191305041313,
          0.020039252936840057,
          0.010603821836411953,
          -0.008389314636588097,
          -0.022376032546162605,
          0.017362823709845543,
          0.009109368547797203,
          -0.020840823650360107,
          -0.005729867611080408,
          0.0032860978972166777,
          -0.023924829438328743,
          -0.001446052803657949,
          -0.004181071184575558,
          -0.17389994859695435,
          -0.011317082680761814,
          0.016778629273176193,
          -0.040241535753011703,
          0.008898787200450897,
          -0.0012066008057445288,
          0.01709110476076603,
          -0.02171032316982746,
          -0.038502536714076996,
          0.012981359846889973,
          0.02085440792143345,
          0.00885123573243618,
          -0.027008837088942528,
          -0.017063932493329048,
          -0.017036762088537216,
          0.004619217477738857,
          0.0025337766855955124,
          0.021737493574619293,
          0.0030670245178043842,
          0.0015411543427035213,
          0.042034879326820374,
          -0.0326605848968029,
          0.017661714926362038,
          -0.014075027778744698,
          0.00945581030100584,
          -0.0028615372721105814,
          -0.002742660464718938,
          0.009985661134123802,
          -0.0026662396267056465,
          -0.03722545877099037,
          0.008599895983934402,
          0.006239340174943209,
          0.025691000744700432,
          -0.009184091351926327,
          0.0023147035390138626,
          0.017199791967868805,
          0.016438979655504227,
          -0.006888068746775389,
          -0.009347123093903065,
          0.0025949133560061455,
          0.031111789867281914,
          0.01929202675819397,
          0.016547666862607002,
          -0.01400709804147482,
          -0.019061066210269928,
          0.020786479115486145,
          0.020134354010224342,
          -0.014387504197657108,
          0.021723909303545952,
          -0.019726775586605072,
          0.00318420329131186,
          -0.00817193929105997,
          -0.0016048384131863713,
          0.013151183724403381,
          0.023965587839484215,
          0.009190884418785572,
          -0.0033795011695474386,
          0.01666994020342827,
          -0.014795082621276379,
          0.004391652997583151,
          -0.00019243202405050397,
          -0.024617712944746017,
          0.011330668814480305,
          -0.017145449295639992,
          -0.02725338377058506,
          -0.01262133289128542,
          -0.017756815999746323,
          0.0047245086170732975,
          0.0006156126619316638,
          0.011004606261849403,
          -0.015447206795215607,
          -0.013844067230820656,
          -0.0068608964793384075,
          0.00015676894690841436,
          0.010474755428731441,
          0.032905131578445435,
          -0.009238434955477715,
          -0.005444562993943691,
          0.019821878522634506,
          0.009978868998587132,
          -0.013185149058699608,
          0.032714929431676865,
          -0.018599143251776695,
          0.018965963274240494,
          -0.006385388784110546,
          0.0009892258094623685,
          0.0009255417389795184,
          0.018558386713266373,
          -0.010624200105667114,
          -0.01869424618780613,
          0.005186430178582668,
          -0.03024228848516941,
          0.013205528259277344,
          0.0012210358399897814,
          0.0019852446857839823,
          0.011371427215635777,
          0.02310967445373535,
          0.0039161453023552895,
          0.004201449919492006,
          -0.015474379062652588,
          0.0053902193903923035,
          0.003841422963887453,
          0.009319950826466084,
          0.029399961233139038,
          0.011588801629841328,
          0.027959851548075676,
          -0.024413922801613808,
          0.013816894963383675,
          0.029726022854447365,
          0.0038244405295699835,
          -0.02429164946079254,
          -0.016642769798636436,
          0.015053214505314827,
          0.020650619640946388,
          0.013796515762805939,
          0.015352105721831322,
          -0.0035459287464618683,
          -0.017947018146514893,
          0.011208395473659039,
          -0.014061441645026207,
          0.03415503725409508,
          0.0034763009753078222,
          -0.01793343387544155,
          0.02168315090239048,
          -0.02207714319229126,
          -0.04717036336660385,
          -0.09417769312858582,
          -0.038094960153102875,
          -0.019658846780657768,
          0.0047143190167844296,
          -0.003844819264486432,
          0.016574839130043983,
          0.02688656374812126,
          0.018830103799700737,
          0.00039420436951331794,
          0.03282361850142479,
          -0.019047480076551437,
          -0.013558762148022652,
          0.002316401805728674,
          -0.02687297761440277,
          0.008104009553790092,
          0.0024607523810118437,
          0.0016260665142908692,
          0.004710922483354807,
          -0.017960604280233383,
          0.01969960518181324,
          0.0030789123848080635,
          -0.030785726383328438,
          0.032714929431676865,
          -0.03184542804956436,
          0.031138960272073746,
          -0.026207266375422478,
          -0.031763914972543716,
          0.008307798765599728,
          -0.0024641486816108227,
          -0.016778629273176193,
          -0.012336027808487415,
          -0.02926410175859928,
          9.860416321316734e-05,
          -0.014944527298212051,
          0.0032657189294695854,
          -0.008756134659051895,
          -0.016194432973861694,
          -0.027348484843969345,
          -0.003950110170990229,
          -0.029970571398735046,
          0.0020327954553067684,
          0.0007306685438379645,
          0.0004997076466679573,
          -0.01592271402478218,
          -0.012132239528000355,
          -0.04002416133880615,
          -0.010963848792016506,
          0.014387504197657108,
          0.011317082680761814,
          -0.007295646704733372,
          -0.014876597560942173,
          -0.008919165469706059,
          -0.03399200737476349,
          -0.007064685691148043,
          0.01313080545514822,
          0.012852293439209461,
          0.0032708137296140194,
          0.004452789667993784,
          -0.011147258803248405,
          -0.02171032316982746,
          0.007954563945531845,
          -0.007397541310638189,
          -0.01892520673573017,
          0.0072888536378741264,
          0.0010936676990240812,
          -0.004371274262666702,
          -0.00934033002704382,
          -0.025568727403879166,
          0.012600953690707684,
          -0.0306226946413517,
          0.0029464494436979294,
          0.014469020068645477,
          0.0035798936150968075,
          0.019604502245783806,
          -0.021207643672823906,
          -0.010705715976655483,
          -0.015515136532485485,
          0.021574463695287704,
          0.005845348350703716,
          -0.01609933190047741,
          -0.03228018060326576,
          -0.022729268297553062,
          0.01729489490389824,
          -0.025025291368365288,
          -0.006803156342357397,
          -0.00766246672719717,
          -0.024644885212183,
          -0.00037127811810933053,
          -0.0038040615618228912,
          -0.029698852449655533,
          0.012668883427977562,
          0.03045966476202011,
          -0.0013450074475258589,
          -0.013191942125558853,
          -0.006235943641513586,
          0.0068608964793384075,
          -0.025758931413292885,
          0.004374670796096325,
          0.006096688099205494,
          0.045458536595106125,
          -0.01364027801901102,
          -0.00641595758497715,
          -0.052822113037109375,
          0.008803685195744038,
          0.0031909963581711054,
          -0.022946642711758614,
          0.009218056686222553,
          0.023775383830070496,
          0.037497177720069885,
          0.009421844966709614,
          -0.013470453210175037,
          0.030731383711099625,
          -0.01073968131095171,
          0.02168315090239048,
          0.013049289584159851,
          0.0001267312909476459,
          -0.012994945980608463,
          -0.024821501225233078,
          0.02385690063238144,
          -0.021968455985188484,
          0.015814026817679405,
          0.024726400151848793,
          -0.003197789192199707,
          -0.011378219351172447,
          0.012859086506068707,
          0.01253981702029705,
          -0.0019173149485141039,
          -0.0010130011942237616,
          -0.015433620661497116,
          0.014686394482851028,
          -0.01733565144240856,
          -0.02268850989639759,
          -0.002095630392432213,
          -0.019590916112065315,
          0.006246133241802454,
          0.00995169673115015,
          -0.010889125987887383,
          -0.002598309889435768,
          0.0007387351361103356,
          0.008830857463181019,
          0.02268850989639759,
          0.009462603367865086,
          -0.01194203644990921,
          -0.026112165302038193,
          0.02882935293018818,
          -0.025976305827498436,
          0.0027494532987475395,
          -0.0033964836038649082,
          0.004632803611457348,
          -0.002212808933109045,
          -0.003817647462710738,
          -0.006483886856585741,
          0.024155789986252785,
          0.02108537033200264,
          0.009435431100428104,
          -0.022973814979195595,
          -0.01631670631468296,
          0.005447959527373314,
          0.013551969081163406,
          0.026152923703193665,
          -0.026913736015558243,
          -0.009809044189751148,
          0.017607370391488075,
          0.007825498469173908,
          0.002411503344774246,
          -0.00325383129529655,
          0.0016421998152509332,
          0.012859086506068707,
          0.019237682223320007,
          -0.0017186206532642245,
          -0.009197677485644817,
          -0.034535445272922516,
          -0.0014018985675647855,
          -0.0013008532114326954,
          0.012071102857589722,
          0.011575215496122837,
          0.03564948961138725,
          0.006229150574654341,
          0.00017884607950691134,
          0.00716658029705286,
          -0.01463205087929964,
          0.03662768006324768,
          0.015596652403473854,
          -0.028965210542082787,
          -0.01342969574034214,
          -0.017770402133464813,
          0.004829799756407738,
          -0.0027358673978596926,
          0.006796363741159439,
          -0.0024556575808674097,
          -0.01402068417519331,
          -0.0037870791275054216,
          0.005712885409593582,
          0.007988529279828072,
          0.002396219177171588,
          0.0053800297901034355,
          -0.004089366178959608,
          0.009809044189751148,
          0.010984227992594242,
          0.00021461529831867665,
          0.012607746757566929,
          -0.013592727482318878,
          0.001277926960028708,
          0.0013322706799954176,
          -0.004595442209392786,
          -0.005634766072034836,
          -0.014536949805915356,
          0.020990267395973206,
          -0.019020307809114456,
          -0.028313087299466133,
          0.00040587977855466306,
          0.0034151640720665455,
          0.010298137553036213,
          0.020243041217327118,
          -0.015257003717124462,
          0.0015292667085304856,
          -0.02861197665333748,
          0.03423655405640602,
          0.013735379092395306,
          -0.017457924783229828,
          -0.008498001843690872,
          0.007995322346687317,
          0.007085064426064491,
          -0.00015952858666423708,
          0.028557633981108665,
          0.0013874635333195329,
          0.01592271402478218,
          0.021642392501235008,
          0.04121972247958183,
          -0.010481548495590687,
          0.00894633773714304,
          0.008287419565021992,
          -0.013253078795969486,
          -0.017145449295639992,
          -0.005529474932700396,
          -0.019020307809114456,
          -0.00023456964117940515,
          0.0033217607997357845,
          0.0027137903962284327,
          0.022172244265675545,
          -0.0034253536723554134,
          0.0892324149608612,
          0.02586761862039566,
          0.004996227100491524,
          0.0027341691311448812,
          -0.0010401731124147773,
          0.025813274085521698,
          0.019210509955883026,
          0.014346746727824211,
          -0.0023418753407895565,
          0.00481961015611887,
          0.02449543960392475,
          0.01633029244840145,
          0.006545023526996374,
          -0.012492266483604908,
          -0.014795082621276379,
          0.006239340174943209,
          0.011595594696700573,
          -0.0030924982856959105,
          0.01787908934056759,
          -0.013103633187711239,
          0.046056315302848816,
          -0.026492571458220482,
          0.007458677981048822,
          0.011955621652305126,
          -0.03461695834994316,
          -0.006049137096852064,
          0.010760059580206871,
          0.015664581209421158,
          -0.018667073920369148,
          -0.039073146879673004,
          0.019658846780657768,
          -0.002095630392432213,
          -0.028150055557489395,
          -0.012845500372350216,
          0.022213002666831017,
          -0.002409805078059435,
          -0.001168390386737883,
          -0.005818176083266735,
          0.018245909363031387,
          0.010617407038807869,
          0.01501245703548193,
          0.01690090261399746,
          -0.004459582734853029,
          -0.03497019410133362,
          -0.01194203644990921,
          -0.0059676216915249825,
          -0.02008001133799553,
          -0.027348484843969345,
          -0.016588425263762474
        ],
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "question": "What is the purpose of BERT?"
      },
      "output": {
        "jsonAnswer": {
          "data_points": [
            "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
          ],
          "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
          "error": ""
        },
        "existingAnswer": true
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "start_time": "2023-09-15T21:55:10.628216Z",
      "end_time": "2023-09-15T21:55:12.457556Z",
      "index": 0,
      "api_calls": [
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.022036384791135788,
              -0.0164797380566597,
              0.0005999039276503026,
              -0.007227716967463493,
              0.003868594765663147,
              0.01381010189652443,
              -0.006500869523733854,
              -0.011711074970662594,
              -0.01653408259153366,
              -0.015121144242584705,
              0.0027137903962284327,
              0.027362070977687836,
              -0.025758931413292885,
              0.00944222416728735,
              3.94045164284762e-05,
              0.0016226699808612466,
              0.016561252996325493,
              0.015487965196371078,
              0.004052004776895046,
              0.0009892258094623685,
              -0.005791004281491041,
              -0.014170128852128983,
              -0.019210509955883026,
              0.020990267395973206,
              -0.006324252113699913,
              -0.02627519704401493,
              0.05154503509402275,
              -0.033502914011478424,
              0.00033030801569111645,
              -0.013558762148022652,
              0.027144696563482285,
              0.030269460752606392,
              -0.010019626468420029,
              -0.008131181821227074,
              -0.013069668784737587,
              0.0003339167742524296,
              0.01770247146487236,
              -0.002297721104696393,
              0.017009589821100235,
              -0.01224092673510313,
              0.015800440683960915,
              0.01892520673573017,
              0.013878031633794308,
              -0.001744094304740429,
              -0.0005765530513599515,
              0.003309873165562749,
              0.01252623088657856,
              -0.004153899382799864,
              0.0005892898770980537,
              -0.023788969963788986,
              0.018653487786650658,
              0.01949581503868103,
              -0.02384331449866295,
              -0.002343573607504368,
              -0.03442675620317459,
              -0.0030534386169165373,
              -0.0077575682662427425,
              0.02582686021924019,
              0.009068611077964306,
              -0.02248472161591053,
              0.010753266513347626,
              0.0006746265571564436,
              -0.015664581209421158,
              0.019156167283654213,
              0.0048841433599591255,
              -0.009299571625888348,
              -0.022946642711758614,
              0.029101070016622543,
              0.010603821836411953,
              0.008885201066732407,
              0.007628501858562231,
              0.020623447373509407,
              0.019455058500170708,
              -0.011086122132837772,
              0.03654616326093674,
              -0.002844554837793112,
              -0.01502604316920042,
              -0.005852140951901674,
              0.008036079816520214,
              0.0004491849394980818,
              0.006405767984688282,
              0.02306891605257988,
              -0.02109895646572113,
              0.020922338590025902,
              0.013062875717878342,
              -0.007594536989927292,
              0.013103633187711239,
              0.01690090261399746,
              -0.019577331840991974,
              -0.0003455921832937747,
              0.006201978772878647,
              0.002939656376838684,
              0.03314967826008797,
              0.004282965790480375,
              0.000431353400927037,
              0.03024228848516941,
              -0.004602235276252031,
              0.029101070016622543,
              0.0016498418990522623,
              -0.035106055438518524,
              0.009027853608131409,
              -0.0075266072526574135,
              -0.007940978743135929,
              -0.01224092673510313,
              -0.019224096089601517,
              0.0021533705294132233,
              0.00983621645718813,
              -0.010372860357165337,
              0.028150055557489395,
              -0.008076838217675686,
              -0.007716810330748558,
              0.03203563392162323,
              -0.0034202588722109795,
              -0.020215868949890137,
              0.005529474932700396,
              0.021533705294132233,
              0.0010299836285412312,
              0.0005413145408965647,
              0.017457924783229828,
              0.020732134580612183,
              0.03328553959727287,
              -0.002212808933109045,
              0.0011565026361495256,
              -0.01827308163046837,
              0.022348862141370773,
              -0.008137974888086319,
              -0.021615220233798027,
              -0.019998494535684586,
              0.008701791055500507,
              -0.029481476172804832,
              0.006055930163711309,
              -0.0036444268189370632,
              0.0013798214495182037,
              -0.023816142231225967,
              -0.002878519706428051,
              0.023191189393401146,
              -0.00796135701239109,
              -0.010107934474945068,
              -0.026424642652273178,
              -0.01768888533115387,
              0.016819385811686516,
              0.022593408823013306,
              -0.023177603259682655,
              -0.03518756851553917,
              -0.026601258665323257,
              0.02546004019677639,
              0.0007625105208717287,
              0.02842177450656891,
              0.010114727541804314,
              0.002683222061023116,
              0.0042456043884158134,
              -0.031138960272073746,
              -0.018055707216262817,
              0.009591669775545597,
              0.0022875317372381687,
              0.013979925774037838,
              -0.00032075541093945503,
              0.0026458606589585543,
              -0.000471262086648494,
              -0.00011494974023662508,
              0.0030279650818556547,
              0.014550535008311272,
              0.011554837226867676,
              -0.009598462842404842,
              0.03760586678981781,
              0.04051325470209122,
              -0.004279569257050753,
              -0.0026611448265612125,
              0.012397164478898048,
              0.003573100548237562,
              -0.008688204921782017,
              0.01570533961057663,
              -0.026981664821505547,
              0.004629407078027725,
              0.015433620661497116,
              0.00746547058224678,
              0.017430752515792847,
              0.007805119268596172,
              0.0007680298294872046,
              -0.013361766003072262,
              -0.0051320865750312805,
              0.006398974917829037,
              0.03399200737476349,
              0.005003020167350769,
              -0.015257003717124462,
              -0.002204317832365632,
              0.003770096693187952,
              -0.035350602120161057,
              0.0028004006016999483,
              -0.044643379747867584,
              0.018205150961875916,
              0.019061066210269928,
              0.011711074970662594,
              -0.004673561546951532,
              -0.6364738345146179,
              -0.014455433934926987,
              0.016629183664917946,
              -0.00835534930229187,
              0.021329917013645172,
              -0.013083254918456078,
              -0.010352482087910175,
              -0.00566193787381053,
              -0.01171786803752184,
              0.03263341262936592,
              0.005447959527373314,
              -0.012607746757566929,
              -0.005906485021114349,
              -0.01502604316920042,
              -0.0013458565808832645,
              -0.022511892020702362,
              -0.00936070829629898,
              -0.00402143644168973,
              0.01869424618780613,
              -0.003973885904997587,
              -0.02446826733648777,
              0.02124840021133423,
              -0.04380105063319206,
              0.005852140951901674,
              -0.01752585545182228,
              0.007132615428417921,
              0.002350366674363613,
              -0.009313157759606838,
              0.007295646704733372,
              -0.007268474902957678,
              -0.01323269959539175,
              0.03336705267429352,
              0.03956224024295807,
              -0.015284175984561443,
              0.031709570437669754,
              0.011860520578920841,
              -0.002382633276283741,
              0.015759684145450592,
              -0.031193304806947708,
              0.0256638303399086,
              -0.00866782572120428,
              -0.019618088379502296,
              0.02329987660050392,
              -0.007010342087596655,
              0.03217149153351784,
              0.007248095702379942,
              0.01232923474162817,
              0.021425018087029457,
              0.0018884448800235987,
              0.007431505713611841,
              -0.027606617659330368,
              0.008722169324755669,
              0.0014333160361275077,
              -0.008464036509394646,
              0.014401090331375599,
              -0.0018341010436415672,
              0.022837955504655838,
              -0.014061441645026207,
              0.0002553731028456241,
              -0.022606994956731796,
              0.011643145233392715,
              0.034345243126153946,
              -0.010902712121605873,
              -0.02567741461098194,
              -0.011004606261849403,
              0.015229832381010056,
              0.0035323428455740213,
              0.012770777568221092,
              0.011887691915035248,
              -0.04477923735976219,
              0.010481548495590687,
              0.024984532967209816,
              0.0006393880466930568,
              -0.0006792966742068529,
              0.02008001133799553,
              0.017743229866027832,
              -0.00228923000395298,
              -0.015121144242584705,
              -0.0004844234499614686,
              0.022973814979195595,
              -0.009252021089196205,
              -0.005587215535342693,
              0.012607746757566929,
              -0.0028190813027322292,
              -0.018748588860034943,
              0.017797574400901794,
              -0.006918637081980705,
              -0.02309608832001686,
              0.005352857988327742,
              0.026207266375422478,
              -0.0012125446228310466,
              0.02782399207353592,
              -0.021601635962724686,
              -0.03942637890577316,
              0.02248472161591053,
              0.012369993142783642,
              0.003501774510368705,
              -0.016642769798636436,
              0.03247038275003433,
              -0.007241302635520697,
              0.002798702334985137,
              -0.017607370391488075,
              0.020243041217327118,
              0.023761799558997154,
              0.00856593158096075,
              0.016438979655504227,
              0.02270209603011608,
              0.020637033507227898,
              0.002071854891255498,
              -0.017199791967868805,
              0.004792438354343176,
              -0.016357464715838432,
              0.00825345516204834,
              0.008776513859629631,
              -0.006633332464843988,
              -0.0329594761133194,
              0.014183714985847473,
              0.0027817199006676674,
              0.012220547534525394,
              -0.021234814077615738,
              0.0019427885999903083,
              -0.009598462842404842,
              0.012444715946912766,
              -0.016642769798636436,
              0.019210509955883026,
              0.017675301060080528,
              -0.0031536349561065435,
              0.004310137592256069,
              -0.020650619640946388,
              -0.005298514384776354,
              -0.021832596510648727,
              -0.006426146719604731,
              0.0053800297901034355,
              -0.012152617797255516,
              0.04393691197037697,
              0.03899163007736206,
              0.03260624036192894,
              -0.0015937999123707414,
              -0.02050117403268814,
              0.005006416700780392,
              -0.012315649539232254,
              -0.010495133697986603,
              0.01567816734313965,
              -0.005740057211369276,
              -0.052034128457307816,
              -0.016167260706424713,
              0.008891994133591652,
              0.003070421051234007,
              0.001690599718131125,
              -0.011554837226867676,
              0.0015428526094183326,
              -0.02624802477657795,
              -0.018585557118058205,
              -0.005899691954255104,
              -0.013090047053992748,
              -0.028557633981108665,
              -0.009408259764313698,
              -0.04668127000331879,
              -0.019536573439836502,
              -0.031709570437669754,
              0.010325309820473194,
              0.00373273529112339,
              -0.011989586986601353,
              -0.012770777568221092,
              0.0011378219351172447,
              -0.03497019410133362,
              0.005515889264643192,
              0.022213002666831017,
              -0.027525102719664574,
              -0.033910490572452545,
              0.028938040137290955,
              -0.01846328377723694,
              -0.01033210288733244,
              0.005774022080004215,
              -0.026682773604989052,
              0.013776137493550777,
              -0.03325836732983589,
              -0.016017816960811615,
              0.0003791324852500111,
              -0.016044987365603447,
              0.005030191969126463,
              0.030704211443662643,
              -0.010012833401560783,
              -0.009224848821759224,
              0.012220547534525394,
              0.0020344937220215797,
              0.017172621563076973,
              -0.00310948072001338,
              -0.02488943189382553,
              0.00974111445248127,
              0.00874934159219265,
              0.026587672531604767,
              -0.003061929950490594,
              0.02090875245630741,
              0.012573782354593277,
              0.006110273767262697,
              0.016357464715838432,
              -0.009122954681515694,
              0.0008754436275921762,
              -0.005172844510525465,
              0.025297008454799652,
              0.010732888244092464,
              0.00034325712476857007,
              -0.012940602377057076,
              -0.008294212631881237,
              0.017457924783229828,
              -8.363840606762096e-05,
              -0.016438979655504227,
              0.008219489827752113,
              -0.00042880602995865047,
              0.01461846474558115,
              -0.029508648440241814,
              -0.02325911819934845,
              -0.0005476829828694463,
              -0.008708584122359753,
              0.021044611930847168,
              -0.009068611077964306,
              0.02289229817688465,
              -0.034263726323843,
              0.0033591222018003464,
              0.003783682594075799,
              -0.01793343387544155,
              0.004975848365575075,
              0.0009900749428197742,
              -0.011765418574213982,
              0.006297080311924219,
              -0.008097216486930847,
              -0.012702848762273788,
              0.012410750612616539,
              -0.022362448275089264,
              0.0005455601494759321,
              0.028285915032029152,
              0.02999774180352688,
              0.020025666803121567,
              -0.0012032042723149061,
              -0.00856593158096075,
              0.014469020068645477,
              -0.010196243412792683,
              0.025595899671316147,
              -0.017159035429358482,
              0.0026017064228653908,
              0.01154804416000843,
              0.013110426254570484,
              -0.0015776666114106774,
              0.03545928746461868,
              0.015814026817679405,
              0.03244321048259735,
              0.010651372373104095,
              -0.016072159633040428,
              0.02287871204316616,
              -0.01431957446038723,
              -0.008878407999873161,
              -0.030731383711099625,
              0.0011514079524204135,
              0.0240606889128685,
              -0.00924522802233696,
              0.0040757800452411175,
              0.012899843975901604,
              -0.0057094888761639595,
              0.033883318305015564,
              0.016357464715838432,
              0.008362142369151115,
              0.022797197103500366,
              0.007805119268596172,
              0.019183339551091194,
              -0.014455433934926987,
              -0.014591293409466743,
              -0.0058079869486391544,
              -0.009184091351926327,
              -0.004938486963510513,
              -0.017254136502742767,
              -0.015080386772751808,
              0.0010206432780250907,
              -0.010617407038807869,
              0.01441467646509409,
              0.03024228848516941,
              -0.02490301802754402,
              0.021438604220747948,
              -0.0067929672077298164,
              0.034127864986658096,
              -0.0023045141715556383,
              -0.029916226863861084,
              0.03160088136792183,
              -0.006130652967840433,
              -0.027959851548075676,
              -0.0171318631619215,
              0.0032283575274050236,
              -0.02169673703610897,
              -0.004269379656761885,
              -0.018830103799700737,
              0.0016956944018602371,
              0.009007474407553673,
              -0.0019852446857839823,
              0.025120392441749573,
              0.00111829221714288,
              0.013293836265802383,
              0.007302439771592617,
              -0.01986263506114483,
              0.004432410933077335,
              0.004486754536628723,
              0.0019190132152289152,
              0.00785946287214756,
              -0.006490679923444986,
              -0.028965210542082787,
              0.02029738575220108,
              0.001125934300944209,
              -0.029209759086370468,
              -0.016642769798636436,
              -0.016466151922941208,
              -0.015977058559656143,
              0.038257990032434464,
              0.009462603367865086,
              0.01868066005408764,
              -0.019346369430422783,
              -5.301698547555134e-05,
              0.0292369294911623,
              0.007893427275121212,
              -0.006996755953878164,
              0.008511587977409363,
              0.004408635664731264,
              -0.028313087299466133,
              -0.012268098071217537,
              -0.01671069860458374,
              0.008436865173280239,
              0.04317609965801239,
              0.04755076766014099,
              -0.004174278117716312,
              0.019020307809114456,
              -0.012757192365825176,
              0.03635596111416817,
              -0.025758931413292885,
              -0.0389372855424881,
              -0.021194057539105415,
              -0.01649332419037819,
              0.0015802140114828944,
              -0.02664201706647873,
              0.006531437858939171,
              -0.022810783237218857,
              0.005254359915852547,
              0.01025058701634407,
              -0.0003099291061516851,
              -0.02626161091029644,
              0.033747460693120956,
              -0.016547666862607002,
              -0.0009994152933359146,
              0.013701414689421654,
              0.006076308898627758,
              0.010467962361872196,
              0.001453694887459278,
              -0.019427886232733727,
              -0.012254512868821621,
              0.010807610116899014,
              -0.0035323428455740213,
              -0.011588801629841328,
              -0.016561252996325493,
              -0.022579822689294815,
              -0.01753944158554077,
              0.00787304900586605,
              -0.02051476016640663,
              0.03939921036362648,
              0.001176881487481296,
              0.006545023526996374,
              0.011004606261849403,
              0.020025666803121567,
              0.0029600353445857763,
              0.02662843093276024,
              -0.002523587318137288,
              -0.01540644932538271,
              7.790684321662411e-05,
              -0.030378147959709167,
              0.004031626041978598,
              0.016574839130043983,
              0.011317082680761814,
              -0.002888709306716919,
              0.0012337726075202227,
              -0.004809420555830002,
              -0.04111103713512421,
              -0.005013209767639637,
              0.014469020068645477,
              -0.014699980616569519,
              0.012471887283027172,
              0.010196243412792683,
              -0.01827308163046837,
              -0.02385690063238144,
              -0.017403582111001015,
              -0.014047855511307716,
              0.01872141659259796,
              -0.007275267504155636,
              0.00365801271982491,
              0.004639596678316593,
              -0.017362823709845543,
              -0.0244275089353323,
              -0.015596652403473854,
              0.04627368971705437,
              -0.008212696760892868,
              -0.04032305255532265,
              -0.04727904871106148,
              -0.016765043139457703,
              0.01221375446766615,
              0.011486907489597797,
              0.03260624036192894,
              -0.020786479115486145,
              -0.001669371617026627,
              -0.0023147035390138626,
              -0.007574158255010843,
              -0.03445392847061157,
              0.0223081037402153,
              -0.04051325470209122,
              0.0038006650283932686,
              -0.01102498546242714,
              -0.00746547058224678,
              -0.027565861120820045,
              -0.019332783296704292,
              0.0354321151971817,
              0.002744358731433749,
              0.005644955672323704,
              0.02961733564734459,
              -0.028965210542082787,
              0.01846328377723694,
              0.019984908401966095,
              0.0072005451656877995,
              0.005634766072034836,
              0.012886258773505688,
              -0.028313087299466133,
              -0.011955621652305126,
              -0.009285985492169857,
              -0.010420411825180054,
              -0.03325836732983589,
              0.03165522590279579,
              -0.006307269912213087,
              0.0036274443846195936,
              -0.021126126870512962,
              0.01733565144240856,
              -0.01072609517723322,
              0.008925958536565304,
              -0.023394977673888206,
              -0.00310948072001338,
              -0.01711827702820301,
              0.0313563346862793,
              -0.0038855771999806166,
              -0.001333968946710229,
              0.024970946833491325,
              -0.012268098071217537,
              -0.0019478832837194204,
              0.0025490608531981707,
              -0.02308250218629837,
              0.016588425263762474,
              0.019006721675395966,
              -0.0026017064228653908,
              0.021520119160413742,
              0.005498906597495079,
              -0.01688731648027897,
              -0.007037513889372349,
              0.025609485805034637,
              -0.024509025737643242,
              0.01668352633714676,
              -0.0017101294361054897,
              -0.0278511643409729,
              -0.03143785148859024,
              -0.016832971945405006,
              -0.002584723988547921,
              0.008932751603424549,
              -0.01461846474558115,
              -0.0012813233770430088,
              0.012179790064692497,
              -0.0006381143466569483,
              0.009992454200983047,
              -0.013225906528532505,
              0.01889803446829319,
              -0.022960228845477104,
              -0.014808667823672295,
              0.008531966246664524,
              0.001946185017004609,
              0.03206280618906021,
              -0.017634542658925056,
              0.02999774180352688,
              -0.030079258605837822,
              -0.0076692597940564156,
              0.0005748548428528011,
              -0.02983471192419529,
              -0.0004895181627944112,
              0.004462979268282652,
              0.03673636540770531,
              0.02324553392827511,
              0.03184542804956436,
              -0.000315448414767161,
              0.01082798931747675,
              0.01501245703548193,
              -0.0043169306591153145,
              -0.006378596182912588,
              -0.028394602239131927,
              -0.003245339961722493,
              -0.022117899730801582,
              0.017172621563076973,
              -0.001829006359912455,
              -0.013633484952151775,
              -0.008260248228907585,
              0.0026662396267056465,
              0.003905955934897065,
              0.002542268019169569,
              0.0028343654703348875,
              0.006945808883756399,
              0.0010180958779528737,
              -0.005757039412856102,
              -0.03635596111416817,
              0.02325911819934845,
              -0.004198053851723671,
              -0.0004954620380885899,
              -0.042279426008462906,
              -0.025595899671316147,
              0.013484039343893528,
              0.011065742932260036,
              0.014903769828379154,
              0.012397164478898048,
              0.013721792958676815,
              -0.015243417583405972,
              0.0006584932561963797,
              -0.008531966246664524,
              -0.003461016807705164,
              -0.001688052318058908,
              -0.014890183694660664,
              -0.029454305768013,
              -0.016846558079123497,
              0.013966340571641922,
              0.028557633981108665,
              -0.021139713004231453,
              -0.003255529562011361,
              -0.010603821836411953,
              0.01628953404724598,
              0.008321384899318218,
              0.01709110476076603,
              -0.008606689050793648,
              0.004378067329525948,
              -0.0013467057142406702,
              -0.009768286719918251,
              -0.03638312965631485,
              -0.028693493455648422,
              -0.0037633036263287067,
              0.036301616579294205,
              0.0034525254741311073,
              -0.02328629046678543,
              0.026519743725657463,
              0.011969207786023617,
              0.0026203871238976717,
              -0.01569175347685814,
              0.004496944136917591,
              0.05230584740638733,
              0.02051476016640663,
              0.011887691915035248,
              0.029481476172804832,
              -0.0005655144923366606,
              0.0016761645674705505,
              0.01870783045887947,
              -0.0005846197018399835,
              -0.014346746727824211,
              -0.004975848365575075,
              -0.007594536989927292,
              0.006755605805665255,
              -0.008321384899318218,
              0.02429164946079254,
              0.02844894677400589,
              -0.018802933394908905,
              -0.01164993830025196,
              0.0006028758361935616,
              -0.00596422515809536,
              0.012471887283027172,
              0.013572348281741142,
              -0.01690090261399746,
              -0.00231130700558424,
              0.012397164478898048,
              -0.026981664821505547,
              0.016602011397480965,
              -0.006602764129638672,
              -0.005070949904620647,
              0.012132239528000355,
              0.006830328144133091,
              -0.012927016243338585,
              0.014428261667490005,
              0.020392486825585365,
              0.0012550007086247206,
              0.001064797630533576,
              0.02070496417582035,
              -0.015841199085116386,
              -0.027389243245124817,
              0.02861197665333748,
              0.030514007434248924,
              -0.014958113431930542,
              0.015134730376303196,
              0.008973509073257446,
              0.014958113431930542,
              0.01589554361999035,
              0.012763985432684422,
              -0.00374971772544086,
              0.027008837088942528,
              -0.035731006413698196,
              -0.008593102917075157,
              -0.013253078795969486,
              -0.024128619581460953,
              0.025568727403879166,
              -0.0280685406178236,
              0.003950110170990229,
              -0.0007968999561853707,
              -0.017362823709845543,
              0.0157460980117321,
              0.03200846165418625,
              0.013171562924981117,
              -0.0250796340405941,
              0.005325686186552048,
              -0.006157824769616127,
              0.01313080545514822,
              -0.009904146194458008,
              0.0034915851429104805,
              0.02168315090239048,
              -0.03043249249458313,
              -0.022158658131957054,
              0.010026419535279274,
              -0.0067623988725245,
              0.01692807301878929,
              -0.015460792928934097,
              -0.0007009492837823927,
              0.006840517744421959,
              0.028476117178797722,
              -0.04290438070893288,
              -0.027511516585946083,
              -0.005994793493300676,
              0.0066061606630682945,
              -0.01849045604467392,
              -0.005111707840114832,
              -0.006235943641513586,
              -0.01806929148733616,
              0.01493094116449356,
              -0.011466528289020061,
              0.005991396959871054,
              -0.011018192395567894,
              -0.003729338990524411,
              -0.009095782414078712,
              -0.0018952378304675221,
              -0.03757869452238083,
              -0.016955245286226273,
              0.0071801659651100636,
              0.016411809250712395,
              -0.006619746331125498,
              -0.009720736183226109,
              -0.010046797804534435,
              0.022593408823013306,
              -0.0049724518321454525,
              0.013667449355125427,
              -0.03423655405640602,
              -0.003525549778714776,
              0.0023265911731868982,
              0.007397541310638189,
              -0.011969207786023617,
              -0.042279426008462906,
              -0.004378067329525948,
              -0.018639901652932167,
              0.006735226605087519,
              -0.018599143251776695,
              -0.011344254948198795,
              0.006898257881402969,
              0.005896295420825481,
              -0.0025303801521658897,
              -0.013762551359832287,
              -0.00305683515034616,
              0.0001466856338083744,
              0.013803308829665184,
              -0.008348556235432625,
              0.017648128792643547,
              0.0006079705781303346,
              -0.012302063405513763,
              -0.020976681262254715,
              -0.029345616698265076,
              0.001504642190411687,
              -0.014849426224827766,
              -0.023394977673888206,
              -0.014672808349132538,
              0.031193304806947708,
              0.010685336776077747,
              -0.01571892574429512,
              -0.01691448874771595,
              -0.020840823650360107,
              -0.03855688124895096,
              -0.00805645901709795,
              -0.007723603397607803,
              0.017267722636461258,
              -0.0014180318685248494,
              0.017403582111001015,
              0.027593031525611877,
              0.02425089292228222,
              0.007105443626642227,
              -0.003943317569792271,
              0.0024964152835309505,
              0.000519237422849983,
              -0.0002857291838154197,
              -0.02801419608294964,
              -0.019061066210269928,
              -0.0009433733066543937,
              -0.047414910048246384,
              -0.000902615487575531,
              0.04279569163918495,
              -0.018667073920369148,
              0.017607370391488075,
              0.05630011111497879,
              -0.008993888273835182,
              -0.016425393521785736,
              -0.001525870175100863,
              0.015161902643740177,
              0.010196243412792683,
              0.02980753965675831,
              0.015977058559656143,
              -0.026207266375422478,
              -0.00731602543964982,
              0.026709945872426033,
              0.00895992387086153,
              -0.0329594761133194,
              -0.0018120239255949855,
              0.01653408259153366,
              -0.003362518735229969,
              0.003048344049602747,
              -0.019210509955883026,
              0.01650691032409668,
              -0.0030636282172054052,
              -0.008858028799295425,
              0.0046531823463737965,
              0.0147271528840065,
              -0.02967168018221855,
              -0.0028530461713671684,
              0.029101070016622543,
              -0.019400713965296745,
              -0.021397845819592476,
              0.022606994956731796,
              -0.01400709804147482,
              -0.0023690471425652504,
              0.016221605241298676,
              -0.014740738086402416,
              0.006843914277851582,
              -0.004707525949925184,
              0.0009951696265488863,
              -0.01847686991095543,
              -0.0004294428799767047,
              -0.029345616698265076,
              0.028122883290052414,
              -0.023150430992245674,
              0.030731383711099625,
              0.011106501333415508,
              -0.02942713350057602,
              0.006497472990304232,
              0.0005608443170785904,
              -0.020243041217327118,
              -0.014686394482851028,
              -0.014876597560942173,
              0.009299571625888348,
              -0.009007474407553673,
              0.014088613912463188,
              0.0007591140456497669,
              -0.01351121161133051,
              -0.015610238537192345,
              -0.00285983900539577,
              0.01832742430269718,
              -0.006847310811281204,
              -0.00011325149534968659,
              0.23715606331825256,
              -0.03877425566315651,
              0.010936676524579525,
              0.0009178996551781893,
              -0.019183339551091194,
              0.0034678096417337656,
              -0.008531966246664524,
              0.010950262658298016,
              -0.009170505218207836,
              -0.0037667001597583294,
              0.02168315090239048,
              0.002808891935274005,
              -0.010705715976655483,
              0.0050437781028449535,
              0.016411809250712395,
              -0.01331421546638012,
              -0.013090047053992748,
              -0.021139713004231453,
              -0.015542308799922466,
              0.005030191969126463,
              0.010006040334701538,
              0.020637033507227898,
              -0.013769344426691532,
              -0.02226734533905983,
              0.03228018060326576,
              -0.002003925386816263,
              0.0007862859056331217,
              -0.004143709782510996,
              0.0354321151971817,
              0.011446149088442326,
              -0.009041438810527325,
              0.005003020167350769,
              -0.000519237422849983,
              -0.0013653864152729511,
              -0.0029005969408899546,
              0.003051740350201726,
              0.006072912365198135,
              0.009428638033568859,
              0.019061066210269928,
              -0.0006410862552002072,
              0.04154578596353531,
              0.021166885271668434,
              -0.008708584122359753,
              -0.019781120121479034,
              0.010637786239385605,
              0.009795458056032658,
              -0.020759306848049164,
              -0.012302063405513763,
              -0.008008908480405807,
              0.029698852449655533,
              -0.032089974731206894,
              0.022905884310603142,
              0.02465846948325634,
              0.037306975573301315,
              -0.0007421316695399582,
              -0.01732206530869007,
              0.00954411830753088,
              0.0015487964265048504,
              0.03279644623398781,
              -0.012994945980608463,
              -0.017362823709845543,
              0.009292778559029102,
              -0.005828365683555603,
              0.023367807269096375,
              -0.0037395283579826355,
              0.02627519704401493,
              -0.038692738860845566,
              0.04312175512313843,
              0.02268850989639759,
              0.014102199114859104,
              0.008919165469706059,
              -0.00975470058619976,
              -0.013558762148022652,
              0.004568270407617092,
              -0.02624802477657795,
              -0.02490301802754402,
              0.016941659152507782,
              0.01709110476076603,
              0.015555894933640957,
              0.03448110073804855,
              0.002479432849213481,
              -0.019047480076551437,
              -0.027538688853383064,
              -0.02209072932600975,
              -0.018585557118058205,
              -0.04298589378595352,
              -0.0010792326647788286,
              -0.020025666803121567,
              0.0010299836285412312,
              -0.03521474078297615,
              0.01729489490389824,
              -0.032334521412849426,
              -0.006168013904243708,
              -0.013490832410752773,
              0.02327270433306694,
              0.010732888244092464,
              0.004731301683932543,
              0.018177980557084084,
              0.010848368518054485,
              -0.010026419535279274,
              -0.0349973663687706,
              0.056626174598932266,
              0.017580198124051094,
              -0.0033455363009124994,
              -0.014292402192950249,
              0.004659975413233042,
              0.0033828974701464176,
              0.0019954340532422066,
              0.012383579276502132,
              -0.01074647344648838,
              0.0038482157979160547,
              -0.01532493345439434,
              0.013354972936213017,
              -0.016221605241298676,
              0.0037157528568059206,
              0.0009289382142014802,
              0.011955621652305126,
              0.012369993142783642,
              0.03024228848516941,
              -0.018001362681388855,
              0.003722545923665166,
              -0.019414300099015236,
              -0.0002935835509561002,
              0.030079258605837822,
              0.0014358634361997247,
              -0.01493094116449356,
              0.003742924891412258,
              0.007234510034322739,
              -0.02310967445373535,
              0.0024318823125213385,
              -0.004102952312678099,
              -0.016004230827093124,
              0.01123556774109602,
              -0.02107178419828415,
              -0.03575817868113518,
              0.013993511907756329,
              0.009965282864868641,
              -0.004975848365575075,
              -0.017226964235305786,
              0.012118653394281864,
              0.00012758040975313634,
              0.004038419108837843,
              0.005233981180936098,
              -0.01830025389790535,
              -0.011758625507354736,
              -0.009584876708686352,
              0.013205528259277344,
              -0.015868371352553368,
              -0.006881275679916143,
              -0.008776513859629631,
              -0.01811004988849163,
              -0.028394602239131927,
              -0.0058079869486391544,
              -0.03524191305041313,
              0.020039252936840057,
              0.010603821836411953,
              -0.008389314636588097,
              -0.022376032546162605,
              0.017362823709845543,
              0.009109368547797203,
              -0.020840823650360107,
              -0.005729867611080408,
              0.0032860978972166777,
              -0.023924829438328743,
              -0.001446052803657949,
              -0.004181071184575558,
              -0.17389994859695435,
              -0.011317082680761814,
              0.016778629273176193,
              -0.040241535753011703,
              0.008898787200450897,
              -0.0012066008057445288,
              0.01709110476076603,
              -0.02171032316982746,
              -0.038502536714076996,
              0.012981359846889973,
              0.02085440792143345,
              0.00885123573243618,
              -0.027008837088942528,
              -0.017063932493329048,
              -0.017036762088537216,
              0.004619217477738857,
              0.0025337766855955124,
              0.021737493574619293,
              0.0030670245178043842,
              0.0015411543427035213,
              0.042034879326820374,
              -0.0326605848968029,
              0.017661714926362038,
              -0.014075027778744698,
              0.00945581030100584,
              -0.0028615372721105814,
              -0.002742660464718938,
              0.009985661134123802,
              -0.0026662396267056465,
              -0.03722545877099037,
              0.008599895983934402,
              0.006239340174943209,
              0.025691000744700432,
              -0.009184091351926327,
              0.0023147035390138626,
              0.017199791967868805,
              0.016438979655504227,
              -0.006888068746775389,
              -0.009347123093903065,
              0.0025949133560061455,
              0.031111789867281914,
              0.01929202675819397,
              0.016547666862607002,
              -0.01400709804147482,
              -0.019061066210269928,
              0.020786479115486145,
              0.020134354010224342,
              -0.014387504197657108,
              0.021723909303545952,
              -0.019726775586605072,
              0.00318420329131186,
              -0.00817193929105997,
              -0.0016048384131863713,
              0.013151183724403381,
              0.023965587839484215,
              0.009190884418785572,
              -0.0033795011695474386,
              0.01666994020342827,
              -0.014795082621276379,
              0.004391652997583151,
              -0.00019243202405050397,
              -0.024617712944746017,
              0.011330668814480305,
              -0.017145449295639992,
              -0.02725338377058506,
              -0.01262133289128542,
              -0.017756815999746323,
              0.0047245086170732975,
              0.0006156126619316638,
              0.011004606261849403,
              -0.015447206795215607,
              -0.013844067230820656,
              -0.0068608964793384075,
              0.00015676894690841436,
              0.010474755428731441,
              0.032905131578445435,
              -0.009238434955477715,
              -0.005444562993943691,
              0.019821878522634506,
              0.009978868998587132,
              -0.013185149058699608,
              0.032714929431676865,
              -0.018599143251776695,
              0.018965963274240494,
              -0.006385388784110546,
              0.0009892258094623685,
              0.0009255417389795184,
              0.018558386713266373,
              -0.010624200105667114,
              -0.01869424618780613,
              0.005186430178582668,
              -0.03024228848516941,
              0.013205528259277344,
              0.0012210358399897814,
              0.0019852446857839823,
              0.011371427215635777,
              0.02310967445373535,
              0.0039161453023552895,
              0.004201449919492006,
              -0.015474379062652588,
              0.0053902193903923035,
              0.003841422963887453,
              0.009319950826466084,
              0.029399961233139038,
              0.011588801629841328,
              0.027959851548075676,
              -0.024413922801613808,
              0.013816894963383675,
              0.029726022854447365,
              0.0038244405295699835,
              -0.02429164946079254,
              -0.016642769798636436,
              0.015053214505314827,
              0.020650619640946388,
              0.013796515762805939,
              0.015352105721831322,
              -0.0035459287464618683,
              -0.017947018146514893,
              0.011208395473659039,
              -0.014061441645026207,
              0.03415503725409508,
              0.0034763009753078222,
              -0.01793343387544155,
              0.02168315090239048,
              -0.02207714319229126,
              -0.04717036336660385,
              -0.09417769312858582,
              -0.038094960153102875,
              -0.019658846780657768,
              0.0047143190167844296,
              -0.003844819264486432,
              0.016574839130043983,
              0.02688656374812126,
              0.018830103799700737,
              0.00039420436951331794,
              0.03282361850142479,
              -0.019047480076551437,
              -0.013558762148022652,
              0.002316401805728674,
              -0.02687297761440277,
              0.008104009553790092,
              0.0024607523810118437,
              0.0016260665142908692,
              0.004710922483354807,
              -0.017960604280233383,
              0.01969960518181324,
              0.0030789123848080635,
              -0.030785726383328438,
              0.032714929431676865,
              -0.03184542804956436,
              0.031138960272073746,
              -0.026207266375422478,
              -0.031763914972543716,
              0.008307798765599728,
              -0.0024641486816108227,
              -0.016778629273176193,
              -0.012336027808487415,
              -0.02926410175859928,
              9.860416321316734e-05,
              -0.014944527298212051,
              0.0032657189294695854,
              -0.008756134659051895,
              -0.016194432973861694,
              -0.027348484843969345,
              -0.003950110170990229,
              -0.029970571398735046,
              0.0020327954553067684,
              0.0007306685438379645,
              0.0004997076466679573,
              -0.01592271402478218,
              -0.012132239528000355,
              -0.04002416133880615,
              -0.010963848792016506,
              0.014387504197657108,
              0.011317082680761814,
              -0.007295646704733372,
              -0.014876597560942173,
              -0.008919165469706059,
              -0.03399200737476349,
              -0.007064685691148043,
              0.01313080545514822,
              0.012852293439209461,
              0.0032708137296140194,
              0.004452789667993784,
              -0.011147258803248405,
              -0.02171032316982746,
              0.007954563945531845,
              -0.007397541310638189,
              -0.01892520673573017,
              0.0072888536378741264,
              0.0010936676990240812,
              -0.004371274262666702,
              -0.00934033002704382,
              -0.025568727403879166,
              0.012600953690707684,
              -0.0306226946413517,
              0.0029464494436979294,
              0.014469020068645477,
              0.0035798936150968075,
              0.019604502245783806,
              -0.021207643672823906,
              -0.010705715976655483,
              -0.015515136532485485,
              0.021574463695287704,
              0.005845348350703716,
              -0.01609933190047741,
              -0.03228018060326576,
              -0.022729268297553062,
              0.01729489490389824,
              -0.025025291368365288,
              -0.006803156342357397,
              -0.00766246672719717,
              -0.024644885212183,
              -0.00037127811810933053,
              -0.0038040615618228912,
              -0.029698852449655533,
              0.012668883427977562,
              0.03045966476202011,
              -0.0013450074475258589,
              -0.013191942125558853,
              -0.006235943641513586,
              0.0068608964793384075,
              -0.025758931413292885,
              0.004374670796096325,
              0.006096688099205494,
              0.045458536595106125,
              -0.01364027801901102,
              -0.00641595758497715,
              -0.052822113037109375,
              0.008803685195744038,
              0.0031909963581711054,
              -0.022946642711758614,
              0.009218056686222553,
              0.023775383830070496,
              0.037497177720069885,
              0.009421844966709614,
              -0.013470453210175037,
              0.030731383711099625,
              -0.01073968131095171,
              0.02168315090239048,
              0.013049289584159851,
              0.0001267312909476459,
              -0.012994945980608463,
              -0.024821501225233078,
              0.02385690063238144,
              -0.021968455985188484,
              0.015814026817679405,
              0.024726400151848793,
              -0.003197789192199707,
              -0.011378219351172447,
              0.012859086506068707,
              0.01253981702029705,
              -0.0019173149485141039,
              -0.0010130011942237616,
              -0.015433620661497116,
              0.014686394482851028,
              -0.01733565144240856,
              -0.02268850989639759,
              -0.002095630392432213,
              -0.019590916112065315,
              0.006246133241802454,
              0.00995169673115015,
              -0.010889125987887383,
              -0.002598309889435768,
              0.0007387351361103356,
              0.008830857463181019,
              0.02268850989639759,
              0.009462603367865086,
              -0.01194203644990921,
              -0.026112165302038193,
              0.02882935293018818,
              -0.025976305827498436,
              0.0027494532987475395,
              -0.0033964836038649082,
              0.004632803611457348,
              -0.002212808933109045,
              -0.003817647462710738,
              -0.006483886856585741,
              0.024155789986252785,
              0.02108537033200264,
              0.009435431100428104,
              -0.022973814979195595,
              -0.01631670631468296,
              0.005447959527373314,
              0.013551969081163406,
              0.026152923703193665,
              -0.026913736015558243,
              -0.009809044189751148,
              0.017607370391488075,
              0.007825498469173908,
              0.002411503344774246,
              -0.00325383129529655,
              0.0016421998152509332,
              0.012859086506068707,
              0.019237682223320007,
              -0.0017186206532642245,
              -0.009197677485644817,
              -0.034535445272922516,
              -0.0014018985675647855,
              -0.0013008532114326954,
              0.012071102857589722,
              0.011575215496122837,
              0.03564948961138725,
              0.006229150574654341,
              0.00017884607950691134,
              0.00716658029705286,
              -0.01463205087929964,
              0.03662768006324768,
              0.015596652403473854,
              -0.028965210542082787,
              -0.01342969574034214,
              -0.017770402133464813,
              0.004829799756407738,
              -0.0027358673978596926,
              0.006796363741159439,
              -0.0024556575808674097,
              -0.01402068417519331,
              -0.0037870791275054216,
              0.005712885409593582,
              0.007988529279828072,
              0.002396219177171588,
              0.0053800297901034355,
              -0.004089366178959608,
              0.009809044189751148,
              0.010984227992594242,
              0.00021461529831867665,
              0.012607746757566929,
              -0.013592727482318878,
              0.001277926960028708,
              0.0013322706799954176,
              -0.004595442209392786,
              -0.005634766072034836,
              -0.014536949805915356,
              0.020990267395973206,
              -0.019020307809114456,
              -0.028313087299466133,
              0.00040587977855466306,
              0.0034151640720665455,
              0.010298137553036213,
              0.020243041217327118,
              -0.015257003717124462,
              0.0015292667085304856,
              -0.02861197665333748,
              0.03423655405640602,
              0.013735379092395306,
              -0.017457924783229828,
              -0.008498001843690872,
              0.007995322346687317,
              0.007085064426064491,
              -0.00015952858666423708,
              0.028557633981108665,
              0.0013874635333195329,
              0.01592271402478218,
              0.021642392501235008,
              0.04121972247958183,
              -0.010481548495590687,
              0.00894633773714304,
              0.008287419565021992,
              -0.013253078795969486,
              -0.017145449295639992,
              -0.005529474932700396,
              -0.019020307809114456,
              -0.00023456964117940515,
              0.0033217607997357845,
              0.0027137903962284327,
              0.022172244265675545,
              -0.0034253536723554134,
              0.0892324149608612,
              0.02586761862039566,
              0.004996227100491524,
              0.0027341691311448812,
              -0.0010401731124147773,
              0.025813274085521698,
              0.019210509955883026,
              0.014346746727824211,
              -0.0023418753407895565,
              0.00481961015611887,
              0.02449543960392475,
              0.01633029244840145,
              0.006545023526996374,
              -0.012492266483604908,
              -0.014795082621276379,
              0.006239340174943209,
              0.011595594696700573,
              -0.0030924982856959105,
              0.01787908934056759,
              -0.013103633187711239,
              0.046056315302848816,
              -0.026492571458220482,
              0.007458677981048822,
              0.011955621652305126,
              -0.03461695834994316,
              -0.006049137096852064,
              0.010760059580206871,
              0.015664581209421158,
              -0.018667073920369148,
              -0.039073146879673004,
              0.019658846780657768,
              -0.002095630392432213,
              -0.028150055557489395,
              -0.012845500372350216,
              0.022213002666831017,
              -0.002409805078059435,
              -0.001168390386737883,
              -0.005818176083266735,
              0.018245909363031387,
              0.010617407038807869,
              0.01501245703548193,
              0.01690090261399746,
              -0.004459582734853029,
              -0.03497019410133362,
              -0.01194203644990921,
              -0.0059676216915249825,
              -0.02008001133799553,
              -0.027348484843969345,
              -0.016588425263762474
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "What is the purpose of BERT?"
          },
          "output": {
            "jsonAnswer": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            },
            "existingAnswer": true
          },
          "start_time": 1694832910.628216,
          "end_time": 1694832912.441883,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "[2023-09-15T21:55:11+0000] Search index aoaikb already exists\n[2023-09-15T21:55:12+0000] KB Search Count: 1\n",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 1.82934
      },
      "result": {
        "jsonAnswer": {
          "data_points": [
            "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
          ],
          "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
          "error": ""
        },
        "existingAnswer": true
      }
    },
    {
      "node": "followup_questions",
      "flow_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6",
      "run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_followup_questions_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.022036384791135788,
          -0.0164797380566597,
          0.0005999039276503026,
          -0.007227716967463493,
          0.003868594765663147,
          0.01381010189652443,
          -0.006500869523733854,
          -0.011711074970662594,
          -0.01653408259153366,
          -0.015121144242584705,
          0.0027137903962284327,
          0.027362070977687836,
          -0.025758931413292885,
          0.00944222416728735,
          3.94045164284762e-05,
          0.0016226699808612466,
          0.016561252996325493,
          0.015487965196371078,
          0.004052004776895046,
          0.0009892258094623685,
          -0.005791004281491041,
          -0.014170128852128983,
          -0.019210509955883026,
          0.020990267395973206,
          -0.006324252113699913,
          -0.02627519704401493,
          0.05154503509402275,
          -0.033502914011478424,
          0.00033030801569111645,
          -0.013558762148022652,
          0.027144696563482285,
          0.030269460752606392,
          -0.010019626468420029,
          -0.008131181821227074,
          -0.013069668784737587,
          0.0003339167742524296,
          0.01770247146487236,
          -0.002297721104696393,
          0.017009589821100235,
          -0.01224092673510313,
          0.015800440683960915,
          0.01892520673573017,
          0.013878031633794308,
          -0.001744094304740429,
          -0.0005765530513599515,
          0.003309873165562749,
          0.01252623088657856,
          -0.004153899382799864,
          0.0005892898770980537,
          -0.023788969963788986,
          0.018653487786650658,
          0.01949581503868103,
          -0.02384331449866295,
          -0.002343573607504368,
          -0.03442675620317459,
          -0.0030534386169165373,
          -0.0077575682662427425,
          0.02582686021924019,
          0.009068611077964306,
          -0.02248472161591053,
          0.010753266513347626,
          0.0006746265571564436,
          -0.015664581209421158,
          0.019156167283654213,
          0.0048841433599591255,
          -0.009299571625888348,
          -0.022946642711758614,
          0.029101070016622543,
          0.010603821836411953,
          0.008885201066732407,
          0.007628501858562231,
          0.020623447373509407,
          0.019455058500170708,
          -0.011086122132837772,
          0.03654616326093674,
          -0.002844554837793112,
          -0.01502604316920042,
          -0.005852140951901674,
          0.008036079816520214,
          0.0004491849394980818,
          0.006405767984688282,
          0.02306891605257988,
          -0.02109895646572113,
          0.020922338590025902,
          0.013062875717878342,
          -0.007594536989927292,
          0.013103633187711239,
          0.01690090261399746,
          -0.019577331840991974,
          -0.0003455921832937747,
          0.006201978772878647,
          0.002939656376838684,
          0.03314967826008797,
          0.004282965790480375,
          0.000431353400927037,
          0.03024228848516941,
          -0.004602235276252031,
          0.029101070016622543,
          0.0016498418990522623,
          -0.035106055438518524,
          0.009027853608131409,
          -0.0075266072526574135,
          -0.007940978743135929,
          -0.01224092673510313,
          -0.019224096089601517,
          0.0021533705294132233,
          0.00983621645718813,
          -0.010372860357165337,
          0.028150055557489395,
          -0.008076838217675686,
          -0.007716810330748558,
          0.03203563392162323,
          -0.0034202588722109795,
          -0.020215868949890137,
          0.005529474932700396,
          0.021533705294132233,
          0.0010299836285412312,
          0.0005413145408965647,
          0.017457924783229828,
          0.020732134580612183,
          0.03328553959727287,
          -0.002212808933109045,
          0.0011565026361495256,
          -0.01827308163046837,
          0.022348862141370773,
          -0.008137974888086319,
          -0.021615220233798027,
          -0.019998494535684586,
          0.008701791055500507,
          -0.029481476172804832,
          0.006055930163711309,
          -0.0036444268189370632,
          0.0013798214495182037,
          -0.023816142231225967,
          -0.002878519706428051,
          0.023191189393401146,
          -0.00796135701239109,
          -0.010107934474945068,
          -0.026424642652273178,
          -0.01768888533115387,
          0.016819385811686516,
          0.022593408823013306,
          -0.023177603259682655,
          -0.03518756851553917,
          -0.026601258665323257,
          0.02546004019677639,
          0.0007625105208717287,
          0.02842177450656891,
          0.010114727541804314,
          0.002683222061023116,
          0.0042456043884158134,
          -0.031138960272073746,
          -0.018055707216262817,
          0.009591669775545597,
          0.0022875317372381687,
          0.013979925774037838,
          -0.00032075541093945503,
          0.0026458606589585543,
          -0.000471262086648494,
          -0.00011494974023662508,
          0.0030279650818556547,
          0.014550535008311272,
          0.011554837226867676,
          -0.009598462842404842,
          0.03760586678981781,
          0.04051325470209122,
          -0.004279569257050753,
          -0.0026611448265612125,
          0.012397164478898048,
          0.003573100548237562,
          -0.008688204921782017,
          0.01570533961057663,
          -0.026981664821505547,
          0.004629407078027725,
          0.015433620661497116,
          0.00746547058224678,
          0.017430752515792847,
          0.007805119268596172,
          0.0007680298294872046,
          -0.013361766003072262,
          -0.0051320865750312805,
          0.006398974917829037,
          0.03399200737476349,
          0.005003020167350769,
          -0.015257003717124462,
          -0.002204317832365632,
          0.003770096693187952,
          -0.035350602120161057,
          0.0028004006016999483,
          -0.044643379747867584,
          0.018205150961875916,
          0.019061066210269928,
          0.011711074970662594,
          -0.004673561546951532,
          -0.6364738345146179,
          -0.014455433934926987,
          0.016629183664917946,
          -0.00835534930229187,
          0.021329917013645172,
          -0.013083254918456078,
          -0.010352482087910175,
          -0.00566193787381053,
          -0.01171786803752184,
          0.03263341262936592,
          0.005447959527373314,
          -0.012607746757566929,
          -0.005906485021114349,
          -0.01502604316920042,
          -0.0013458565808832645,
          -0.022511892020702362,
          -0.00936070829629898,
          -0.00402143644168973,
          0.01869424618780613,
          -0.003973885904997587,
          -0.02446826733648777,
          0.02124840021133423,
          -0.04380105063319206,
          0.005852140951901674,
          -0.01752585545182228,
          0.007132615428417921,
          0.002350366674363613,
          -0.009313157759606838,
          0.007295646704733372,
          -0.007268474902957678,
          -0.01323269959539175,
          0.03336705267429352,
          0.03956224024295807,
          -0.015284175984561443,
          0.031709570437669754,
          0.011860520578920841,
          -0.002382633276283741,
          0.015759684145450592,
          -0.031193304806947708,
          0.0256638303399086,
          -0.00866782572120428,
          -0.019618088379502296,
          0.02329987660050392,
          -0.007010342087596655,
          0.03217149153351784,
          0.007248095702379942,
          0.01232923474162817,
          0.021425018087029457,
          0.0018884448800235987,
          0.007431505713611841,
          -0.027606617659330368,
          0.008722169324755669,
          0.0014333160361275077,
          -0.008464036509394646,
          0.014401090331375599,
          -0.0018341010436415672,
          0.022837955504655838,
          -0.014061441645026207,
          0.0002553731028456241,
          -0.022606994956731796,
          0.011643145233392715,
          0.034345243126153946,
          -0.010902712121605873,
          -0.02567741461098194,
          -0.011004606261849403,
          0.015229832381010056,
          0.0035323428455740213,
          0.012770777568221092,
          0.011887691915035248,
          -0.04477923735976219,
          0.010481548495590687,
          0.024984532967209816,
          0.0006393880466930568,
          -0.0006792966742068529,
          0.02008001133799553,
          0.017743229866027832,
          -0.00228923000395298,
          -0.015121144242584705,
          -0.0004844234499614686,
          0.022973814979195595,
          -0.009252021089196205,
          -0.005587215535342693,
          0.012607746757566929,
          -0.0028190813027322292,
          -0.018748588860034943,
          0.017797574400901794,
          -0.006918637081980705,
          -0.02309608832001686,
          0.005352857988327742,
          0.026207266375422478,
          -0.0012125446228310466,
          0.02782399207353592,
          -0.021601635962724686,
          -0.03942637890577316,
          0.02248472161591053,
          0.012369993142783642,
          0.003501774510368705,
          -0.016642769798636436,
          0.03247038275003433,
          -0.007241302635520697,
          0.002798702334985137,
          -0.017607370391488075,
          0.020243041217327118,
          0.023761799558997154,
          0.00856593158096075,
          0.016438979655504227,
          0.02270209603011608,
          0.020637033507227898,
          0.002071854891255498,
          -0.017199791967868805,
          0.004792438354343176,
          -0.016357464715838432,
          0.00825345516204834,
          0.008776513859629631,
          -0.006633332464843988,
          -0.0329594761133194,
          0.014183714985847473,
          0.0027817199006676674,
          0.012220547534525394,
          -0.021234814077615738,
          0.0019427885999903083,
          -0.009598462842404842,
          0.012444715946912766,
          -0.016642769798636436,
          0.019210509955883026,
          0.017675301060080528,
          -0.0031536349561065435,
          0.004310137592256069,
          -0.020650619640946388,
          -0.005298514384776354,
          -0.021832596510648727,
          -0.006426146719604731,
          0.0053800297901034355,
          -0.012152617797255516,
          0.04393691197037697,
          0.03899163007736206,
          0.03260624036192894,
          -0.0015937999123707414,
          -0.02050117403268814,
          0.005006416700780392,
          -0.012315649539232254,
          -0.010495133697986603,
          0.01567816734313965,
          -0.005740057211369276,
          -0.052034128457307816,
          -0.016167260706424713,
          0.008891994133591652,
          0.003070421051234007,
          0.001690599718131125,
          -0.011554837226867676,
          0.0015428526094183326,
          -0.02624802477657795,
          -0.018585557118058205,
          -0.005899691954255104,
          -0.013090047053992748,
          -0.028557633981108665,
          -0.009408259764313698,
          -0.04668127000331879,
          -0.019536573439836502,
          -0.031709570437669754,
          0.010325309820473194,
          0.00373273529112339,
          -0.011989586986601353,
          -0.012770777568221092,
          0.0011378219351172447,
          -0.03497019410133362,
          0.005515889264643192,
          0.022213002666831017,
          -0.027525102719664574,
          -0.033910490572452545,
          0.028938040137290955,
          -0.01846328377723694,
          -0.01033210288733244,
          0.005774022080004215,
          -0.026682773604989052,
          0.013776137493550777,
          -0.03325836732983589,
          -0.016017816960811615,
          0.0003791324852500111,
          -0.016044987365603447,
          0.005030191969126463,
          0.030704211443662643,
          -0.010012833401560783,
          -0.009224848821759224,
          0.012220547534525394,
          0.0020344937220215797,
          0.017172621563076973,
          -0.00310948072001338,
          -0.02488943189382553,
          0.00974111445248127,
          0.00874934159219265,
          0.026587672531604767,
          -0.003061929950490594,
          0.02090875245630741,
          0.012573782354593277,
          0.006110273767262697,
          0.016357464715838432,
          -0.009122954681515694,
          0.0008754436275921762,
          -0.005172844510525465,
          0.025297008454799652,
          0.010732888244092464,
          0.00034325712476857007,
          -0.012940602377057076,
          -0.008294212631881237,
          0.017457924783229828,
          -8.363840606762096e-05,
          -0.016438979655504227,
          0.008219489827752113,
          -0.00042880602995865047,
          0.01461846474558115,
          -0.029508648440241814,
          -0.02325911819934845,
          -0.0005476829828694463,
          -0.008708584122359753,
          0.021044611930847168,
          -0.009068611077964306,
          0.02289229817688465,
          -0.034263726323843,
          0.0033591222018003464,
          0.003783682594075799,
          -0.01793343387544155,
          0.004975848365575075,
          0.0009900749428197742,
          -0.011765418574213982,
          0.006297080311924219,
          -0.008097216486930847,
          -0.012702848762273788,
          0.012410750612616539,
          -0.022362448275089264,
          0.0005455601494759321,
          0.028285915032029152,
          0.02999774180352688,
          0.020025666803121567,
          -0.0012032042723149061,
          -0.00856593158096075,
          0.014469020068645477,
          -0.010196243412792683,
          0.025595899671316147,
          -0.017159035429358482,
          0.0026017064228653908,
          0.01154804416000843,
          0.013110426254570484,
          -0.0015776666114106774,
          0.03545928746461868,
          0.015814026817679405,
          0.03244321048259735,
          0.010651372373104095,
          -0.016072159633040428,
          0.02287871204316616,
          -0.01431957446038723,
          -0.008878407999873161,
          -0.030731383711099625,
          0.0011514079524204135,
          0.0240606889128685,
          -0.00924522802233696,
          0.0040757800452411175,
          0.012899843975901604,
          -0.0057094888761639595,
          0.033883318305015564,
          0.016357464715838432,
          0.008362142369151115,
          0.022797197103500366,
          0.007805119268596172,
          0.019183339551091194,
          -0.014455433934926987,
          -0.014591293409466743,
          -0.0058079869486391544,
          -0.009184091351926327,
          -0.004938486963510513,
          -0.017254136502742767,
          -0.015080386772751808,
          0.0010206432780250907,
          -0.010617407038807869,
          0.01441467646509409,
          0.03024228848516941,
          -0.02490301802754402,
          0.021438604220747948,
          -0.0067929672077298164,
          0.034127864986658096,
          -0.0023045141715556383,
          -0.029916226863861084,
          0.03160088136792183,
          -0.006130652967840433,
          -0.027959851548075676,
          -0.0171318631619215,
          0.0032283575274050236,
          -0.02169673703610897,
          -0.004269379656761885,
          -0.018830103799700737,
          0.0016956944018602371,
          0.009007474407553673,
          -0.0019852446857839823,
          0.025120392441749573,
          0.00111829221714288,
          0.013293836265802383,
          0.007302439771592617,
          -0.01986263506114483,
          0.004432410933077335,
          0.004486754536628723,
          0.0019190132152289152,
          0.00785946287214756,
          -0.006490679923444986,
          -0.028965210542082787,
          0.02029738575220108,
          0.001125934300944209,
          -0.029209759086370468,
          -0.016642769798636436,
          -0.016466151922941208,
          -0.015977058559656143,
          0.038257990032434464,
          0.009462603367865086,
          0.01868066005408764,
          -0.019346369430422783,
          -5.301698547555134e-05,
          0.0292369294911623,
          0.007893427275121212,
          -0.006996755953878164,
          0.008511587977409363,
          0.004408635664731264,
          -0.028313087299466133,
          -0.012268098071217537,
          -0.01671069860458374,
          0.008436865173280239,
          0.04317609965801239,
          0.04755076766014099,
          -0.004174278117716312,
          0.019020307809114456,
          -0.012757192365825176,
          0.03635596111416817,
          -0.025758931413292885,
          -0.0389372855424881,
          -0.021194057539105415,
          -0.01649332419037819,
          0.0015802140114828944,
          -0.02664201706647873,
          0.006531437858939171,
          -0.022810783237218857,
          0.005254359915852547,
          0.01025058701634407,
          -0.0003099291061516851,
          -0.02626161091029644,
          0.033747460693120956,
          -0.016547666862607002,
          -0.0009994152933359146,
          0.013701414689421654,
          0.006076308898627758,
          0.010467962361872196,
          0.001453694887459278,
          -0.019427886232733727,
          -0.012254512868821621,
          0.010807610116899014,
          -0.0035323428455740213,
          -0.011588801629841328,
          -0.016561252996325493,
          -0.022579822689294815,
          -0.01753944158554077,
          0.00787304900586605,
          -0.02051476016640663,
          0.03939921036362648,
          0.001176881487481296,
          0.006545023526996374,
          0.011004606261849403,
          0.020025666803121567,
          0.0029600353445857763,
          0.02662843093276024,
          -0.002523587318137288,
          -0.01540644932538271,
          7.790684321662411e-05,
          -0.030378147959709167,
          0.004031626041978598,
          0.016574839130043983,
          0.011317082680761814,
          -0.002888709306716919,
          0.0012337726075202227,
          -0.004809420555830002,
          -0.04111103713512421,
          -0.005013209767639637,
          0.014469020068645477,
          -0.014699980616569519,
          0.012471887283027172,
          0.010196243412792683,
          -0.01827308163046837,
          -0.02385690063238144,
          -0.017403582111001015,
          -0.014047855511307716,
          0.01872141659259796,
          -0.007275267504155636,
          0.00365801271982491,
          0.004639596678316593,
          -0.017362823709845543,
          -0.0244275089353323,
          -0.015596652403473854,
          0.04627368971705437,
          -0.008212696760892868,
          -0.04032305255532265,
          -0.04727904871106148,
          -0.016765043139457703,
          0.01221375446766615,
          0.011486907489597797,
          0.03260624036192894,
          -0.020786479115486145,
          -0.001669371617026627,
          -0.0023147035390138626,
          -0.007574158255010843,
          -0.03445392847061157,
          0.0223081037402153,
          -0.04051325470209122,
          0.0038006650283932686,
          -0.01102498546242714,
          -0.00746547058224678,
          -0.027565861120820045,
          -0.019332783296704292,
          0.0354321151971817,
          0.002744358731433749,
          0.005644955672323704,
          0.02961733564734459,
          -0.028965210542082787,
          0.01846328377723694,
          0.019984908401966095,
          0.0072005451656877995,
          0.005634766072034836,
          0.012886258773505688,
          -0.028313087299466133,
          -0.011955621652305126,
          -0.009285985492169857,
          -0.010420411825180054,
          -0.03325836732983589,
          0.03165522590279579,
          -0.006307269912213087,
          0.0036274443846195936,
          -0.021126126870512962,
          0.01733565144240856,
          -0.01072609517723322,
          0.008925958536565304,
          -0.023394977673888206,
          -0.00310948072001338,
          -0.01711827702820301,
          0.0313563346862793,
          -0.0038855771999806166,
          -0.001333968946710229,
          0.024970946833491325,
          -0.012268098071217537,
          -0.0019478832837194204,
          0.0025490608531981707,
          -0.02308250218629837,
          0.016588425263762474,
          0.019006721675395966,
          -0.0026017064228653908,
          0.021520119160413742,
          0.005498906597495079,
          -0.01688731648027897,
          -0.007037513889372349,
          0.025609485805034637,
          -0.024509025737643242,
          0.01668352633714676,
          -0.0017101294361054897,
          -0.0278511643409729,
          -0.03143785148859024,
          -0.016832971945405006,
          -0.002584723988547921,
          0.008932751603424549,
          -0.01461846474558115,
          -0.0012813233770430088,
          0.012179790064692497,
          -0.0006381143466569483,
          0.009992454200983047,
          -0.013225906528532505,
          0.01889803446829319,
          -0.022960228845477104,
          -0.014808667823672295,
          0.008531966246664524,
          0.001946185017004609,
          0.03206280618906021,
          -0.017634542658925056,
          0.02999774180352688,
          -0.030079258605837822,
          -0.0076692597940564156,
          0.0005748548428528011,
          -0.02983471192419529,
          -0.0004895181627944112,
          0.004462979268282652,
          0.03673636540770531,
          0.02324553392827511,
          0.03184542804956436,
          -0.000315448414767161,
          0.01082798931747675,
          0.01501245703548193,
          -0.0043169306591153145,
          -0.006378596182912588,
          -0.028394602239131927,
          -0.003245339961722493,
          -0.022117899730801582,
          0.017172621563076973,
          -0.001829006359912455,
          -0.013633484952151775,
          -0.008260248228907585,
          0.0026662396267056465,
          0.003905955934897065,
          0.002542268019169569,
          0.0028343654703348875,
          0.006945808883756399,
          0.0010180958779528737,
          -0.005757039412856102,
          -0.03635596111416817,
          0.02325911819934845,
          -0.004198053851723671,
          -0.0004954620380885899,
          -0.042279426008462906,
          -0.025595899671316147,
          0.013484039343893528,
          0.011065742932260036,
          0.014903769828379154,
          0.012397164478898048,
          0.013721792958676815,
          -0.015243417583405972,
          0.0006584932561963797,
          -0.008531966246664524,
          -0.003461016807705164,
          -0.001688052318058908,
          -0.014890183694660664,
          -0.029454305768013,
          -0.016846558079123497,
          0.013966340571641922,
          0.028557633981108665,
          -0.021139713004231453,
          -0.003255529562011361,
          -0.010603821836411953,
          0.01628953404724598,
          0.008321384899318218,
          0.01709110476076603,
          -0.008606689050793648,
          0.004378067329525948,
          -0.0013467057142406702,
          -0.009768286719918251,
          -0.03638312965631485,
          -0.028693493455648422,
          -0.0037633036263287067,
          0.036301616579294205,
          0.0034525254741311073,
          -0.02328629046678543,
          0.026519743725657463,
          0.011969207786023617,
          0.0026203871238976717,
          -0.01569175347685814,
          0.004496944136917591,
          0.05230584740638733,
          0.02051476016640663,
          0.011887691915035248,
          0.029481476172804832,
          -0.0005655144923366606,
          0.0016761645674705505,
          0.01870783045887947,
          -0.0005846197018399835,
          -0.014346746727824211,
          -0.004975848365575075,
          -0.007594536989927292,
          0.006755605805665255,
          -0.008321384899318218,
          0.02429164946079254,
          0.02844894677400589,
          -0.018802933394908905,
          -0.01164993830025196,
          0.0006028758361935616,
          -0.00596422515809536,
          0.012471887283027172,
          0.013572348281741142,
          -0.01690090261399746,
          -0.00231130700558424,
          0.012397164478898048,
          -0.026981664821505547,
          0.016602011397480965,
          -0.006602764129638672,
          -0.005070949904620647,
          0.012132239528000355,
          0.006830328144133091,
          -0.012927016243338585,
          0.014428261667490005,
          0.020392486825585365,
          0.0012550007086247206,
          0.001064797630533576,
          0.02070496417582035,
          -0.015841199085116386,
          -0.027389243245124817,
          0.02861197665333748,
          0.030514007434248924,
          -0.014958113431930542,
          0.015134730376303196,
          0.008973509073257446,
          0.014958113431930542,
          0.01589554361999035,
          0.012763985432684422,
          -0.00374971772544086,
          0.027008837088942528,
          -0.035731006413698196,
          -0.008593102917075157,
          -0.013253078795969486,
          -0.024128619581460953,
          0.025568727403879166,
          -0.0280685406178236,
          0.003950110170990229,
          -0.0007968999561853707,
          -0.017362823709845543,
          0.0157460980117321,
          0.03200846165418625,
          0.013171562924981117,
          -0.0250796340405941,
          0.005325686186552048,
          -0.006157824769616127,
          0.01313080545514822,
          -0.009904146194458008,
          0.0034915851429104805,
          0.02168315090239048,
          -0.03043249249458313,
          -0.022158658131957054,
          0.010026419535279274,
          -0.0067623988725245,
          0.01692807301878929,
          -0.015460792928934097,
          -0.0007009492837823927,
          0.006840517744421959,
          0.028476117178797722,
          -0.04290438070893288,
          -0.027511516585946083,
          -0.005994793493300676,
          0.0066061606630682945,
          -0.01849045604467392,
          -0.005111707840114832,
          -0.006235943641513586,
          -0.01806929148733616,
          0.01493094116449356,
          -0.011466528289020061,
          0.005991396959871054,
          -0.011018192395567894,
          -0.003729338990524411,
          -0.009095782414078712,
          -0.0018952378304675221,
          -0.03757869452238083,
          -0.016955245286226273,
          0.0071801659651100636,
          0.016411809250712395,
          -0.006619746331125498,
          -0.009720736183226109,
          -0.010046797804534435,
          0.022593408823013306,
          -0.0049724518321454525,
          0.013667449355125427,
          -0.03423655405640602,
          -0.003525549778714776,
          0.0023265911731868982,
          0.007397541310638189,
          -0.011969207786023617,
          -0.042279426008462906,
          -0.004378067329525948,
          -0.018639901652932167,
          0.006735226605087519,
          -0.018599143251776695,
          -0.011344254948198795,
          0.006898257881402969,
          0.005896295420825481,
          -0.0025303801521658897,
          -0.013762551359832287,
          -0.00305683515034616,
          0.0001466856338083744,
          0.013803308829665184,
          -0.008348556235432625,
          0.017648128792643547,
          0.0006079705781303346,
          -0.012302063405513763,
          -0.020976681262254715,
          -0.029345616698265076,
          0.001504642190411687,
          -0.014849426224827766,
          -0.023394977673888206,
          -0.014672808349132538,
          0.031193304806947708,
          0.010685336776077747,
          -0.01571892574429512,
          -0.01691448874771595,
          -0.020840823650360107,
          -0.03855688124895096,
          -0.00805645901709795,
          -0.007723603397607803,
          0.017267722636461258,
          -0.0014180318685248494,
          0.017403582111001015,
          0.027593031525611877,
          0.02425089292228222,
          0.007105443626642227,
          -0.003943317569792271,
          0.0024964152835309505,
          0.000519237422849983,
          -0.0002857291838154197,
          -0.02801419608294964,
          -0.019061066210269928,
          -0.0009433733066543937,
          -0.047414910048246384,
          -0.000902615487575531,
          0.04279569163918495,
          -0.018667073920369148,
          0.017607370391488075,
          0.05630011111497879,
          -0.008993888273835182,
          -0.016425393521785736,
          -0.001525870175100863,
          0.015161902643740177,
          0.010196243412792683,
          0.02980753965675831,
          0.015977058559656143,
          -0.026207266375422478,
          -0.00731602543964982,
          0.026709945872426033,
          0.00895992387086153,
          -0.0329594761133194,
          -0.0018120239255949855,
          0.01653408259153366,
          -0.003362518735229969,
          0.003048344049602747,
          -0.019210509955883026,
          0.01650691032409668,
          -0.0030636282172054052,
          -0.008858028799295425,
          0.0046531823463737965,
          0.0147271528840065,
          -0.02967168018221855,
          -0.0028530461713671684,
          0.029101070016622543,
          -0.019400713965296745,
          -0.021397845819592476,
          0.022606994956731796,
          -0.01400709804147482,
          -0.0023690471425652504,
          0.016221605241298676,
          -0.014740738086402416,
          0.006843914277851582,
          -0.004707525949925184,
          0.0009951696265488863,
          -0.01847686991095543,
          -0.0004294428799767047,
          -0.029345616698265076,
          0.028122883290052414,
          -0.023150430992245674,
          0.030731383711099625,
          0.011106501333415508,
          -0.02942713350057602,
          0.006497472990304232,
          0.0005608443170785904,
          -0.020243041217327118,
          -0.014686394482851028,
          -0.014876597560942173,
          0.009299571625888348,
          -0.009007474407553673,
          0.014088613912463188,
          0.0007591140456497669,
          -0.01351121161133051,
          -0.015610238537192345,
          -0.00285983900539577,
          0.01832742430269718,
          -0.006847310811281204,
          -0.00011325149534968659,
          0.23715606331825256,
          -0.03877425566315651,
          0.010936676524579525,
          0.0009178996551781893,
          -0.019183339551091194,
          0.0034678096417337656,
          -0.008531966246664524,
          0.010950262658298016,
          -0.009170505218207836,
          -0.0037667001597583294,
          0.02168315090239048,
          0.002808891935274005,
          -0.010705715976655483,
          0.0050437781028449535,
          0.016411809250712395,
          -0.01331421546638012,
          -0.013090047053992748,
          -0.021139713004231453,
          -0.015542308799922466,
          0.005030191969126463,
          0.010006040334701538,
          0.020637033507227898,
          -0.013769344426691532,
          -0.02226734533905983,
          0.03228018060326576,
          -0.002003925386816263,
          0.0007862859056331217,
          -0.004143709782510996,
          0.0354321151971817,
          0.011446149088442326,
          -0.009041438810527325,
          0.005003020167350769,
          -0.000519237422849983,
          -0.0013653864152729511,
          -0.0029005969408899546,
          0.003051740350201726,
          0.006072912365198135,
          0.009428638033568859,
          0.019061066210269928,
          -0.0006410862552002072,
          0.04154578596353531,
          0.021166885271668434,
          -0.008708584122359753,
          -0.019781120121479034,
          0.010637786239385605,
          0.009795458056032658,
          -0.020759306848049164,
          -0.012302063405513763,
          -0.008008908480405807,
          0.029698852449655533,
          -0.032089974731206894,
          0.022905884310603142,
          0.02465846948325634,
          0.037306975573301315,
          -0.0007421316695399582,
          -0.01732206530869007,
          0.00954411830753088,
          0.0015487964265048504,
          0.03279644623398781,
          -0.012994945980608463,
          -0.017362823709845543,
          0.009292778559029102,
          -0.005828365683555603,
          0.023367807269096375,
          -0.0037395283579826355,
          0.02627519704401493,
          -0.038692738860845566,
          0.04312175512313843,
          0.02268850989639759,
          0.014102199114859104,
          0.008919165469706059,
          -0.00975470058619976,
          -0.013558762148022652,
          0.004568270407617092,
          -0.02624802477657795,
          -0.02490301802754402,
          0.016941659152507782,
          0.01709110476076603,
          0.015555894933640957,
          0.03448110073804855,
          0.002479432849213481,
          -0.019047480076551437,
          -0.027538688853383064,
          -0.02209072932600975,
          -0.018585557118058205,
          -0.04298589378595352,
          -0.0010792326647788286,
          -0.020025666803121567,
          0.0010299836285412312,
          -0.03521474078297615,
          0.01729489490389824,
          -0.032334521412849426,
          -0.006168013904243708,
          -0.013490832410752773,
          0.02327270433306694,
          0.010732888244092464,
          0.004731301683932543,
          0.018177980557084084,
          0.010848368518054485,
          -0.010026419535279274,
          -0.0349973663687706,
          0.056626174598932266,
          0.017580198124051094,
          -0.0033455363009124994,
          -0.014292402192950249,
          0.004659975413233042,
          0.0033828974701464176,
          0.0019954340532422066,
          0.012383579276502132,
          -0.01074647344648838,
          0.0038482157979160547,
          -0.01532493345439434,
          0.013354972936213017,
          -0.016221605241298676,
          0.0037157528568059206,
          0.0009289382142014802,
          0.011955621652305126,
          0.012369993142783642,
          0.03024228848516941,
          -0.018001362681388855,
          0.003722545923665166,
          -0.019414300099015236,
          -0.0002935835509561002,
          0.030079258605837822,
          0.0014358634361997247,
          -0.01493094116449356,
          0.003742924891412258,
          0.007234510034322739,
          -0.02310967445373535,
          0.0024318823125213385,
          -0.004102952312678099,
          -0.016004230827093124,
          0.01123556774109602,
          -0.02107178419828415,
          -0.03575817868113518,
          0.013993511907756329,
          0.009965282864868641,
          -0.004975848365575075,
          -0.017226964235305786,
          0.012118653394281864,
          0.00012758040975313634,
          0.004038419108837843,
          0.005233981180936098,
          -0.01830025389790535,
          -0.011758625507354736,
          -0.009584876708686352,
          0.013205528259277344,
          -0.015868371352553368,
          -0.006881275679916143,
          -0.008776513859629631,
          -0.01811004988849163,
          -0.028394602239131927,
          -0.0058079869486391544,
          -0.03524191305041313,
          0.020039252936840057,
          0.010603821836411953,
          -0.008389314636588097,
          -0.022376032546162605,
          0.017362823709845543,
          0.009109368547797203,
          -0.020840823650360107,
          -0.005729867611080408,
          0.0032860978972166777,
          -0.023924829438328743,
          -0.001446052803657949,
          -0.004181071184575558,
          -0.17389994859695435,
          -0.011317082680761814,
          0.016778629273176193,
          -0.040241535753011703,
          0.008898787200450897,
          -0.0012066008057445288,
          0.01709110476076603,
          -0.02171032316982746,
          -0.038502536714076996,
          0.012981359846889973,
          0.02085440792143345,
          0.00885123573243618,
          -0.027008837088942528,
          -0.017063932493329048,
          -0.017036762088537216,
          0.004619217477738857,
          0.0025337766855955124,
          0.021737493574619293,
          0.0030670245178043842,
          0.0015411543427035213,
          0.042034879326820374,
          -0.0326605848968029,
          0.017661714926362038,
          -0.014075027778744698,
          0.00945581030100584,
          -0.0028615372721105814,
          -0.002742660464718938,
          0.009985661134123802,
          -0.0026662396267056465,
          -0.03722545877099037,
          0.008599895983934402,
          0.006239340174943209,
          0.025691000744700432,
          -0.009184091351926327,
          0.0023147035390138626,
          0.017199791967868805,
          0.016438979655504227,
          -0.006888068746775389,
          -0.009347123093903065,
          0.0025949133560061455,
          0.031111789867281914,
          0.01929202675819397,
          0.016547666862607002,
          -0.01400709804147482,
          -0.019061066210269928,
          0.020786479115486145,
          0.020134354010224342,
          -0.014387504197657108,
          0.021723909303545952,
          -0.019726775586605072,
          0.00318420329131186,
          -0.00817193929105997,
          -0.0016048384131863713,
          0.013151183724403381,
          0.023965587839484215,
          0.009190884418785572,
          -0.0033795011695474386,
          0.01666994020342827,
          -0.014795082621276379,
          0.004391652997583151,
          -0.00019243202405050397,
          -0.024617712944746017,
          0.011330668814480305,
          -0.017145449295639992,
          -0.02725338377058506,
          -0.01262133289128542,
          -0.017756815999746323,
          0.0047245086170732975,
          0.0006156126619316638,
          0.011004606261849403,
          -0.015447206795215607,
          -0.013844067230820656,
          -0.0068608964793384075,
          0.00015676894690841436,
          0.010474755428731441,
          0.032905131578445435,
          -0.009238434955477715,
          -0.005444562993943691,
          0.019821878522634506,
          0.009978868998587132,
          -0.013185149058699608,
          0.032714929431676865,
          -0.018599143251776695,
          0.018965963274240494,
          -0.006385388784110546,
          0.0009892258094623685,
          0.0009255417389795184,
          0.018558386713266373,
          -0.010624200105667114,
          -0.01869424618780613,
          0.005186430178582668,
          -0.03024228848516941,
          0.013205528259277344,
          0.0012210358399897814,
          0.0019852446857839823,
          0.011371427215635777,
          0.02310967445373535,
          0.0039161453023552895,
          0.004201449919492006,
          -0.015474379062652588,
          0.0053902193903923035,
          0.003841422963887453,
          0.009319950826466084,
          0.029399961233139038,
          0.011588801629841328,
          0.027959851548075676,
          -0.024413922801613808,
          0.013816894963383675,
          0.029726022854447365,
          0.0038244405295699835,
          -0.02429164946079254,
          -0.016642769798636436,
          0.015053214505314827,
          0.020650619640946388,
          0.013796515762805939,
          0.015352105721831322,
          -0.0035459287464618683,
          -0.017947018146514893,
          0.011208395473659039,
          -0.014061441645026207,
          0.03415503725409508,
          0.0034763009753078222,
          -0.01793343387544155,
          0.02168315090239048,
          -0.02207714319229126,
          -0.04717036336660385,
          -0.09417769312858582,
          -0.038094960153102875,
          -0.019658846780657768,
          0.0047143190167844296,
          -0.003844819264486432,
          0.016574839130043983,
          0.02688656374812126,
          0.018830103799700737,
          0.00039420436951331794,
          0.03282361850142479,
          -0.019047480076551437,
          -0.013558762148022652,
          0.002316401805728674,
          -0.02687297761440277,
          0.008104009553790092,
          0.0024607523810118437,
          0.0016260665142908692,
          0.004710922483354807,
          -0.017960604280233383,
          0.01969960518181324,
          0.0030789123848080635,
          -0.030785726383328438,
          0.032714929431676865,
          -0.03184542804956436,
          0.031138960272073746,
          -0.026207266375422478,
          -0.031763914972543716,
          0.008307798765599728,
          -0.0024641486816108227,
          -0.016778629273176193,
          -0.012336027808487415,
          -0.02926410175859928,
          9.860416321316734e-05,
          -0.014944527298212051,
          0.0032657189294695854,
          -0.008756134659051895,
          -0.016194432973861694,
          -0.027348484843969345,
          -0.003950110170990229,
          -0.029970571398735046,
          0.0020327954553067684,
          0.0007306685438379645,
          0.0004997076466679573,
          -0.01592271402478218,
          -0.012132239528000355,
          -0.04002416133880615,
          -0.010963848792016506,
          0.014387504197657108,
          0.011317082680761814,
          -0.007295646704733372,
          -0.014876597560942173,
          -0.008919165469706059,
          -0.03399200737476349,
          -0.007064685691148043,
          0.01313080545514822,
          0.012852293439209461,
          0.0032708137296140194,
          0.004452789667993784,
          -0.011147258803248405,
          -0.02171032316982746,
          0.007954563945531845,
          -0.007397541310638189,
          -0.01892520673573017,
          0.0072888536378741264,
          0.0010936676990240812,
          -0.004371274262666702,
          -0.00934033002704382,
          -0.025568727403879166,
          0.012600953690707684,
          -0.0306226946413517,
          0.0029464494436979294,
          0.014469020068645477,
          0.0035798936150968075,
          0.019604502245783806,
          -0.021207643672823906,
          -0.010705715976655483,
          -0.015515136532485485,
          0.021574463695287704,
          0.005845348350703716,
          -0.01609933190047741,
          -0.03228018060326576,
          -0.022729268297553062,
          0.01729489490389824,
          -0.025025291368365288,
          -0.006803156342357397,
          -0.00766246672719717,
          -0.024644885212183,
          -0.00037127811810933053,
          -0.0038040615618228912,
          -0.029698852449655533,
          0.012668883427977562,
          0.03045966476202011,
          -0.0013450074475258589,
          -0.013191942125558853,
          -0.006235943641513586,
          0.0068608964793384075,
          -0.025758931413292885,
          0.004374670796096325,
          0.006096688099205494,
          0.045458536595106125,
          -0.01364027801901102,
          -0.00641595758497715,
          -0.052822113037109375,
          0.008803685195744038,
          0.0031909963581711054,
          -0.022946642711758614,
          0.009218056686222553,
          0.023775383830070496,
          0.037497177720069885,
          0.009421844966709614,
          -0.013470453210175037,
          0.030731383711099625,
          -0.01073968131095171,
          0.02168315090239048,
          0.013049289584159851,
          0.0001267312909476459,
          -0.012994945980608463,
          -0.024821501225233078,
          0.02385690063238144,
          -0.021968455985188484,
          0.015814026817679405,
          0.024726400151848793,
          -0.003197789192199707,
          -0.011378219351172447,
          0.012859086506068707,
          0.01253981702029705,
          -0.0019173149485141039,
          -0.0010130011942237616,
          -0.015433620661497116,
          0.014686394482851028,
          -0.01733565144240856,
          -0.02268850989639759,
          -0.002095630392432213,
          -0.019590916112065315,
          0.006246133241802454,
          0.00995169673115015,
          -0.010889125987887383,
          -0.002598309889435768,
          0.0007387351361103356,
          0.008830857463181019,
          0.02268850989639759,
          0.009462603367865086,
          -0.01194203644990921,
          -0.026112165302038193,
          0.02882935293018818,
          -0.025976305827498436,
          0.0027494532987475395,
          -0.0033964836038649082,
          0.004632803611457348,
          -0.002212808933109045,
          -0.003817647462710738,
          -0.006483886856585741,
          0.024155789986252785,
          0.02108537033200264,
          0.009435431100428104,
          -0.022973814979195595,
          -0.01631670631468296,
          0.005447959527373314,
          0.013551969081163406,
          0.026152923703193665,
          -0.026913736015558243,
          -0.009809044189751148,
          0.017607370391488075,
          0.007825498469173908,
          0.002411503344774246,
          -0.00325383129529655,
          0.0016421998152509332,
          0.012859086506068707,
          0.019237682223320007,
          -0.0017186206532642245,
          -0.009197677485644817,
          -0.034535445272922516,
          -0.0014018985675647855,
          -0.0013008532114326954,
          0.012071102857589722,
          0.011575215496122837,
          0.03564948961138725,
          0.006229150574654341,
          0.00017884607950691134,
          0.00716658029705286,
          -0.01463205087929964,
          0.03662768006324768,
          0.015596652403473854,
          -0.028965210542082787,
          -0.01342969574034214,
          -0.017770402133464813,
          0.004829799756407738,
          -0.0027358673978596926,
          0.006796363741159439,
          -0.0024556575808674097,
          -0.01402068417519331,
          -0.0037870791275054216,
          0.005712885409593582,
          0.007988529279828072,
          0.002396219177171588,
          0.0053800297901034355,
          -0.004089366178959608,
          0.009809044189751148,
          0.010984227992594242,
          0.00021461529831867665,
          0.012607746757566929,
          -0.013592727482318878,
          0.001277926960028708,
          0.0013322706799954176,
          -0.004595442209392786,
          -0.005634766072034836,
          -0.014536949805915356,
          0.020990267395973206,
          -0.019020307809114456,
          -0.028313087299466133,
          0.00040587977855466306,
          0.0034151640720665455,
          0.010298137553036213,
          0.020243041217327118,
          -0.015257003717124462,
          0.0015292667085304856,
          -0.02861197665333748,
          0.03423655405640602,
          0.013735379092395306,
          -0.017457924783229828,
          -0.008498001843690872,
          0.007995322346687317,
          0.007085064426064491,
          -0.00015952858666423708,
          0.028557633981108665,
          0.0013874635333195329,
          0.01592271402478218,
          0.021642392501235008,
          0.04121972247958183,
          -0.010481548495590687,
          0.00894633773714304,
          0.008287419565021992,
          -0.013253078795969486,
          -0.017145449295639992,
          -0.005529474932700396,
          -0.019020307809114456,
          -0.00023456964117940515,
          0.0033217607997357845,
          0.0027137903962284327,
          0.022172244265675545,
          -0.0034253536723554134,
          0.0892324149608612,
          0.02586761862039566,
          0.004996227100491524,
          0.0027341691311448812,
          -0.0010401731124147773,
          0.025813274085521698,
          0.019210509955883026,
          0.014346746727824211,
          -0.0023418753407895565,
          0.00481961015611887,
          0.02449543960392475,
          0.01633029244840145,
          0.006545023526996374,
          -0.012492266483604908,
          -0.014795082621276379,
          0.006239340174943209,
          0.011595594696700573,
          -0.0030924982856959105,
          0.01787908934056759,
          -0.013103633187711239,
          0.046056315302848816,
          -0.026492571458220482,
          0.007458677981048822,
          0.011955621652305126,
          -0.03461695834994316,
          -0.006049137096852064,
          0.010760059580206871,
          0.015664581209421158,
          -0.018667073920369148,
          -0.039073146879673004,
          0.019658846780657768,
          -0.002095630392432213,
          -0.028150055557489395,
          -0.012845500372350216,
          0.022213002666831017,
          -0.002409805078059435,
          -0.001168390386737883,
          -0.005818176083266735,
          0.018245909363031387,
          0.010617407038807869,
          0.01501245703548193,
          0.01690090261399746,
          -0.004459582734853029,
          -0.03497019410133362,
          -0.01194203644990921,
          -0.0059676216915249825,
          -0.02008001133799553,
          -0.027348484843969345,
          -0.016588425263762474
        ],
        "existingAnswer": true,
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "jsonAnswer": {
          "data_points": [
            "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
          ],
          "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
          "error": ""
        },
        "llm": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
        "modifiedAnswer": "",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "promptTemplate": "",
        "question": "What is the purpose of BERT?",
        "retrievedDocs": ""
      },
      "output": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            }
          }
        ]
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "cac0b8be-4e0f-4191-aa05-2c0219e315c6_0",
      "start_time": "2023-09-15T21:55:12.457556Z",
      "end_time": "2023-09-15T21:55:12.490871Z",
      "index": 0,
      "api_calls": [
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.022036384791135788,
              -0.0164797380566597,
              0.0005999039276503026,
              -0.007227716967463493,
              0.003868594765663147,
              0.01381010189652443,
              -0.006500869523733854,
              -0.011711074970662594,
              -0.01653408259153366,
              -0.015121144242584705,
              0.0027137903962284327,
              0.027362070977687836,
              -0.025758931413292885,
              0.00944222416728735,
              3.94045164284762e-05,
              0.0016226699808612466,
              0.016561252996325493,
              0.015487965196371078,
              0.004052004776895046,
              0.0009892258094623685,
              -0.005791004281491041,
              -0.014170128852128983,
              -0.019210509955883026,
              0.020990267395973206,
              -0.006324252113699913,
              -0.02627519704401493,
              0.05154503509402275,
              -0.033502914011478424,
              0.00033030801569111645,
              -0.013558762148022652,
              0.027144696563482285,
              0.030269460752606392,
              -0.010019626468420029,
              -0.008131181821227074,
              -0.013069668784737587,
              0.0003339167742524296,
              0.01770247146487236,
              -0.002297721104696393,
              0.017009589821100235,
              -0.01224092673510313,
              0.015800440683960915,
              0.01892520673573017,
              0.013878031633794308,
              -0.001744094304740429,
              -0.0005765530513599515,
              0.003309873165562749,
              0.01252623088657856,
              -0.004153899382799864,
              0.0005892898770980537,
              -0.023788969963788986,
              0.018653487786650658,
              0.01949581503868103,
              -0.02384331449866295,
              -0.002343573607504368,
              -0.03442675620317459,
              -0.0030534386169165373,
              -0.0077575682662427425,
              0.02582686021924019,
              0.009068611077964306,
              -0.02248472161591053,
              0.010753266513347626,
              0.0006746265571564436,
              -0.015664581209421158,
              0.019156167283654213,
              0.0048841433599591255,
              -0.009299571625888348,
              -0.022946642711758614,
              0.029101070016622543,
              0.010603821836411953,
              0.008885201066732407,
              0.007628501858562231,
              0.020623447373509407,
              0.019455058500170708,
              -0.011086122132837772,
              0.03654616326093674,
              -0.002844554837793112,
              -0.01502604316920042,
              -0.005852140951901674,
              0.008036079816520214,
              0.0004491849394980818,
              0.006405767984688282,
              0.02306891605257988,
              -0.02109895646572113,
              0.020922338590025902,
              0.013062875717878342,
              -0.007594536989927292,
              0.013103633187711239,
              0.01690090261399746,
              -0.019577331840991974,
              -0.0003455921832937747,
              0.006201978772878647,
              0.002939656376838684,
              0.03314967826008797,
              0.004282965790480375,
              0.000431353400927037,
              0.03024228848516941,
              -0.004602235276252031,
              0.029101070016622543,
              0.0016498418990522623,
              -0.035106055438518524,
              0.009027853608131409,
              -0.0075266072526574135,
              -0.007940978743135929,
              -0.01224092673510313,
              -0.019224096089601517,
              0.0021533705294132233,
              0.00983621645718813,
              -0.010372860357165337,
              0.028150055557489395,
              -0.008076838217675686,
              -0.007716810330748558,
              0.03203563392162323,
              -0.0034202588722109795,
              -0.020215868949890137,
              0.005529474932700396,
              0.021533705294132233,
              0.0010299836285412312,
              0.0005413145408965647,
              0.017457924783229828,
              0.020732134580612183,
              0.03328553959727287,
              -0.002212808933109045,
              0.0011565026361495256,
              -0.01827308163046837,
              0.022348862141370773,
              -0.008137974888086319,
              -0.021615220233798027,
              -0.019998494535684586,
              0.008701791055500507,
              -0.029481476172804832,
              0.006055930163711309,
              -0.0036444268189370632,
              0.0013798214495182037,
              -0.023816142231225967,
              -0.002878519706428051,
              0.023191189393401146,
              -0.00796135701239109,
              -0.010107934474945068,
              -0.026424642652273178,
              -0.01768888533115387,
              0.016819385811686516,
              0.022593408823013306,
              -0.023177603259682655,
              -0.03518756851553917,
              -0.026601258665323257,
              0.02546004019677639,
              0.0007625105208717287,
              0.02842177450656891,
              0.010114727541804314,
              0.002683222061023116,
              0.0042456043884158134,
              -0.031138960272073746,
              -0.018055707216262817,
              0.009591669775545597,
              0.0022875317372381687,
              0.013979925774037838,
              -0.00032075541093945503,
              0.0026458606589585543,
              -0.000471262086648494,
              -0.00011494974023662508,
              0.0030279650818556547,
              0.014550535008311272,
              0.011554837226867676,
              -0.009598462842404842,
              0.03760586678981781,
              0.04051325470209122,
              -0.004279569257050753,
              -0.0026611448265612125,
              0.012397164478898048,
              0.003573100548237562,
              -0.008688204921782017,
              0.01570533961057663,
              -0.026981664821505547,
              0.004629407078027725,
              0.015433620661497116,
              0.00746547058224678,
              0.017430752515792847,
              0.007805119268596172,
              0.0007680298294872046,
              -0.013361766003072262,
              -0.0051320865750312805,
              0.006398974917829037,
              0.03399200737476349,
              0.005003020167350769,
              -0.015257003717124462,
              -0.002204317832365632,
              0.003770096693187952,
              -0.035350602120161057,
              0.0028004006016999483,
              -0.044643379747867584,
              0.018205150961875916,
              0.019061066210269928,
              0.011711074970662594,
              -0.004673561546951532,
              -0.6364738345146179,
              -0.014455433934926987,
              0.016629183664917946,
              -0.00835534930229187,
              0.021329917013645172,
              -0.013083254918456078,
              -0.010352482087910175,
              -0.00566193787381053,
              -0.01171786803752184,
              0.03263341262936592,
              0.005447959527373314,
              -0.012607746757566929,
              -0.005906485021114349,
              -0.01502604316920042,
              -0.0013458565808832645,
              -0.022511892020702362,
              -0.00936070829629898,
              -0.00402143644168973,
              0.01869424618780613,
              -0.003973885904997587,
              -0.02446826733648777,
              0.02124840021133423,
              -0.04380105063319206,
              0.005852140951901674,
              -0.01752585545182228,
              0.007132615428417921,
              0.002350366674363613,
              -0.009313157759606838,
              0.007295646704733372,
              -0.007268474902957678,
              -0.01323269959539175,
              0.03336705267429352,
              0.03956224024295807,
              -0.015284175984561443,
              0.031709570437669754,
              0.011860520578920841,
              -0.002382633276283741,
              0.015759684145450592,
              -0.031193304806947708,
              0.0256638303399086,
              -0.00866782572120428,
              -0.019618088379502296,
              0.02329987660050392,
              -0.007010342087596655,
              0.03217149153351784,
              0.007248095702379942,
              0.01232923474162817,
              0.021425018087029457,
              0.0018884448800235987,
              0.007431505713611841,
              -0.027606617659330368,
              0.008722169324755669,
              0.0014333160361275077,
              -0.008464036509394646,
              0.014401090331375599,
              -0.0018341010436415672,
              0.022837955504655838,
              -0.014061441645026207,
              0.0002553731028456241,
              -0.022606994956731796,
              0.011643145233392715,
              0.034345243126153946,
              -0.010902712121605873,
              -0.02567741461098194,
              -0.011004606261849403,
              0.015229832381010056,
              0.0035323428455740213,
              0.012770777568221092,
              0.011887691915035248,
              -0.04477923735976219,
              0.010481548495590687,
              0.024984532967209816,
              0.0006393880466930568,
              -0.0006792966742068529,
              0.02008001133799553,
              0.017743229866027832,
              -0.00228923000395298,
              -0.015121144242584705,
              -0.0004844234499614686,
              0.022973814979195595,
              -0.009252021089196205,
              -0.005587215535342693,
              0.012607746757566929,
              -0.0028190813027322292,
              -0.018748588860034943,
              0.017797574400901794,
              -0.006918637081980705,
              -0.02309608832001686,
              0.005352857988327742,
              0.026207266375422478,
              -0.0012125446228310466,
              0.02782399207353592,
              -0.021601635962724686,
              -0.03942637890577316,
              0.02248472161591053,
              0.012369993142783642,
              0.003501774510368705,
              -0.016642769798636436,
              0.03247038275003433,
              -0.007241302635520697,
              0.002798702334985137,
              -0.017607370391488075,
              0.020243041217327118,
              0.023761799558997154,
              0.00856593158096075,
              0.016438979655504227,
              0.02270209603011608,
              0.020637033507227898,
              0.002071854891255498,
              -0.017199791967868805,
              0.004792438354343176,
              -0.016357464715838432,
              0.00825345516204834,
              0.008776513859629631,
              -0.006633332464843988,
              -0.0329594761133194,
              0.014183714985847473,
              0.0027817199006676674,
              0.012220547534525394,
              -0.021234814077615738,
              0.0019427885999903083,
              -0.009598462842404842,
              0.012444715946912766,
              -0.016642769798636436,
              0.019210509955883026,
              0.017675301060080528,
              -0.0031536349561065435,
              0.004310137592256069,
              -0.020650619640946388,
              -0.005298514384776354,
              -0.021832596510648727,
              -0.006426146719604731,
              0.0053800297901034355,
              -0.012152617797255516,
              0.04393691197037697,
              0.03899163007736206,
              0.03260624036192894,
              -0.0015937999123707414,
              -0.02050117403268814,
              0.005006416700780392,
              -0.012315649539232254,
              -0.010495133697986603,
              0.01567816734313965,
              -0.005740057211369276,
              -0.052034128457307816,
              -0.016167260706424713,
              0.008891994133591652,
              0.003070421051234007,
              0.001690599718131125,
              -0.011554837226867676,
              0.0015428526094183326,
              -0.02624802477657795,
              -0.018585557118058205,
              -0.005899691954255104,
              -0.013090047053992748,
              -0.028557633981108665,
              -0.009408259764313698,
              -0.04668127000331879,
              -0.019536573439836502,
              -0.031709570437669754,
              0.010325309820473194,
              0.00373273529112339,
              -0.011989586986601353,
              -0.012770777568221092,
              0.0011378219351172447,
              -0.03497019410133362,
              0.005515889264643192,
              0.022213002666831017,
              -0.027525102719664574,
              -0.033910490572452545,
              0.028938040137290955,
              -0.01846328377723694,
              -0.01033210288733244,
              0.005774022080004215,
              -0.026682773604989052,
              0.013776137493550777,
              -0.03325836732983589,
              -0.016017816960811615,
              0.0003791324852500111,
              -0.016044987365603447,
              0.005030191969126463,
              0.030704211443662643,
              -0.010012833401560783,
              -0.009224848821759224,
              0.012220547534525394,
              0.0020344937220215797,
              0.017172621563076973,
              -0.00310948072001338,
              -0.02488943189382553,
              0.00974111445248127,
              0.00874934159219265,
              0.026587672531604767,
              -0.003061929950490594,
              0.02090875245630741,
              0.012573782354593277,
              0.006110273767262697,
              0.016357464715838432,
              -0.009122954681515694,
              0.0008754436275921762,
              -0.005172844510525465,
              0.025297008454799652,
              0.010732888244092464,
              0.00034325712476857007,
              -0.012940602377057076,
              -0.008294212631881237,
              0.017457924783229828,
              -8.363840606762096e-05,
              -0.016438979655504227,
              0.008219489827752113,
              -0.00042880602995865047,
              0.01461846474558115,
              -0.029508648440241814,
              -0.02325911819934845,
              -0.0005476829828694463,
              -0.008708584122359753,
              0.021044611930847168,
              -0.009068611077964306,
              0.02289229817688465,
              -0.034263726323843,
              0.0033591222018003464,
              0.003783682594075799,
              -0.01793343387544155,
              0.004975848365575075,
              0.0009900749428197742,
              -0.011765418574213982,
              0.006297080311924219,
              -0.008097216486930847,
              -0.012702848762273788,
              0.012410750612616539,
              -0.022362448275089264,
              0.0005455601494759321,
              0.028285915032029152,
              0.02999774180352688,
              0.020025666803121567,
              -0.0012032042723149061,
              -0.00856593158096075,
              0.014469020068645477,
              -0.010196243412792683,
              0.025595899671316147,
              -0.017159035429358482,
              0.0026017064228653908,
              0.01154804416000843,
              0.013110426254570484,
              -0.0015776666114106774,
              0.03545928746461868,
              0.015814026817679405,
              0.03244321048259735,
              0.010651372373104095,
              -0.016072159633040428,
              0.02287871204316616,
              -0.01431957446038723,
              -0.008878407999873161,
              -0.030731383711099625,
              0.0011514079524204135,
              0.0240606889128685,
              -0.00924522802233696,
              0.0040757800452411175,
              0.012899843975901604,
              -0.0057094888761639595,
              0.033883318305015564,
              0.016357464715838432,
              0.008362142369151115,
              0.022797197103500366,
              0.007805119268596172,
              0.019183339551091194,
              -0.014455433934926987,
              -0.014591293409466743,
              -0.0058079869486391544,
              -0.009184091351926327,
              -0.004938486963510513,
              -0.017254136502742767,
              -0.015080386772751808,
              0.0010206432780250907,
              -0.010617407038807869,
              0.01441467646509409,
              0.03024228848516941,
              -0.02490301802754402,
              0.021438604220747948,
              -0.0067929672077298164,
              0.034127864986658096,
              -0.0023045141715556383,
              -0.029916226863861084,
              0.03160088136792183,
              -0.006130652967840433,
              -0.027959851548075676,
              -0.0171318631619215,
              0.0032283575274050236,
              -0.02169673703610897,
              -0.004269379656761885,
              -0.018830103799700737,
              0.0016956944018602371,
              0.009007474407553673,
              -0.0019852446857839823,
              0.025120392441749573,
              0.00111829221714288,
              0.013293836265802383,
              0.007302439771592617,
              -0.01986263506114483,
              0.004432410933077335,
              0.004486754536628723,
              0.0019190132152289152,
              0.00785946287214756,
              -0.006490679923444986,
              -0.028965210542082787,
              0.02029738575220108,
              0.001125934300944209,
              -0.029209759086370468,
              -0.016642769798636436,
              -0.016466151922941208,
              -0.015977058559656143,
              0.038257990032434464,
              0.009462603367865086,
              0.01868066005408764,
              -0.019346369430422783,
              -5.301698547555134e-05,
              0.0292369294911623,
              0.007893427275121212,
              -0.006996755953878164,
              0.008511587977409363,
              0.004408635664731264,
              -0.028313087299466133,
              -0.012268098071217537,
              -0.01671069860458374,
              0.008436865173280239,
              0.04317609965801239,
              0.04755076766014099,
              -0.004174278117716312,
              0.019020307809114456,
              -0.012757192365825176,
              0.03635596111416817,
              -0.025758931413292885,
              -0.0389372855424881,
              -0.021194057539105415,
              -0.01649332419037819,
              0.0015802140114828944,
              -0.02664201706647873,
              0.006531437858939171,
              -0.022810783237218857,
              0.005254359915852547,
              0.01025058701634407,
              -0.0003099291061516851,
              -0.02626161091029644,
              0.033747460693120956,
              -0.016547666862607002,
              -0.0009994152933359146,
              0.013701414689421654,
              0.006076308898627758,
              0.010467962361872196,
              0.001453694887459278,
              -0.019427886232733727,
              -0.012254512868821621,
              0.010807610116899014,
              -0.0035323428455740213,
              -0.011588801629841328,
              -0.016561252996325493,
              -0.022579822689294815,
              -0.01753944158554077,
              0.00787304900586605,
              -0.02051476016640663,
              0.03939921036362648,
              0.001176881487481296,
              0.006545023526996374,
              0.011004606261849403,
              0.020025666803121567,
              0.0029600353445857763,
              0.02662843093276024,
              -0.002523587318137288,
              -0.01540644932538271,
              7.790684321662411e-05,
              -0.030378147959709167,
              0.004031626041978598,
              0.016574839130043983,
              0.011317082680761814,
              -0.002888709306716919,
              0.0012337726075202227,
              -0.004809420555830002,
              -0.04111103713512421,
              -0.005013209767639637,
              0.014469020068645477,
              -0.014699980616569519,
              0.012471887283027172,
              0.010196243412792683,
              -0.01827308163046837,
              -0.02385690063238144,
              -0.017403582111001015,
              -0.014047855511307716,
              0.01872141659259796,
              -0.007275267504155636,
              0.00365801271982491,
              0.004639596678316593,
              -0.017362823709845543,
              -0.0244275089353323,
              -0.015596652403473854,
              0.04627368971705437,
              -0.008212696760892868,
              -0.04032305255532265,
              -0.04727904871106148,
              -0.016765043139457703,
              0.01221375446766615,
              0.011486907489597797,
              0.03260624036192894,
              -0.020786479115486145,
              -0.001669371617026627,
              -0.0023147035390138626,
              -0.007574158255010843,
              -0.03445392847061157,
              0.0223081037402153,
              -0.04051325470209122,
              0.0038006650283932686,
              -0.01102498546242714,
              -0.00746547058224678,
              -0.027565861120820045,
              -0.019332783296704292,
              0.0354321151971817,
              0.002744358731433749,
              0.005644955672323704,
              0.02961733564734459,
              -0.028965210542082787,
              0.01846328377723694,
              0.019984908401966095,
              0.0072005451656877995,
              0.005634766072034836,
              0.012886258773505688,
              -0.028313087299466133,
              -0.011955621652305126,
              -0.009285985492169857,
              -0.010420411825180054,
              -0.03325836732983589,
              0.03165522590279579,
              -0.006307269912213087,
              0.0036274443846195936,
              -0.021126126870512962,
              0.01733565144240856,
              -0.01072609517723322,
              0.008925958536565304,
              -0.023394977673888206,
              -0.00310948072001338,
              -0.01711827702820301,
              0.0313563346862793,
              -0.0038855771999806166,
              -0.001333968946710229,
              0.024970946833491325,
              -0.012268098071217537,
              -0.0019478832837194204,
              0.0025490608531981707,
              -0.02308250218629837,
              0.016588425263762474,
              0.019006721675395966,
              -0.0026017064228653908,
              0.021520119160413742,
              0.005498906597495079,
              -0.01688731648027897,
              -0.007037513889372349,
              0.025609485805034637,
              -0.024509025737643242,
              0.01668352633714676,
              -0.0017101294361054897,
              -0.0278511643409729,
              -0.03143785148859024,
              -0.016832971945405006,
              -0.002584723988547921,
              0.008932751603424549,
              -0.01461846474558115,
              -0.0012813233770430088,
              0.012179790064692497,
              -0.0006381143466569483,
              0.009992454200983047,
              -0.013225906528532505,
              0.01889803446829319,
              -0.022960228845477104,
              -0.014808667823672295,
              0.008531966246664524,
              0.001946185017004609,
              0.03206280618906021,
              -0.017634542658925056,
              0.02999774180352688,
              -0.030079258605837822,
              -0.0076692597940564156,
              0.0005748548428528011,
              -0.02983471192419529,
              -0.0004895181627944112,
              0.004462979268282652,
              0.03673636540770531,
              0.02324553392827511,
              0.03184542804956436,
              -0.000315448414767161,
              0.01082798931747675,
              0.01501245703548193,
              -0.0043169306591153145,
              -0.006378596182912588,
              -0.028394602239131927,
              -0.003245339961722493,
              -0.022117899730801582,
              0.017172621563076973,
              -0.001829006359912455,
              -0.013633484952151775,
              -0.008260248228907585,
              0.0026662396267056465,
              0.003905955934897065,
              0.002542268019169569,
              0.0028343654703348875,
              0.006945808883756399,
              0.0010180958779528737,
              -0.005757039412856102,
              -0.03635596111416817,
              0.02325911819934845,
              -0.004198053851723671,
              -0.0004954620380885899,
              -0.042279426008462906,
              -0.025595899671316147,
              0.013484039343893528,
              0.011065742932260036,
              0.014903769828379154,
              0.012397164478898048,
              0.013721792958676815,
              -0.015243417583405972,
              0.0006584932561963797,
              -0.008531966246664524,
              -0.003461016807705164,
              -0.001688052318058908,
              -0.014890183694660664,
              -0.029454305768013,
              -0.016846558079123497,
              0.013966340571641922,
              0.028557633981108665,
              -0.021139713004231453,
              -0.003255529562011361,
              -0.010603821836411953,
              0.01628953404724598,
              0.008321384899318218,
              0.01709110476076603,
              -0.008606689050793648,
              0.004378067329525948,
              -0.0013467057142406702,
              -0.009768286719918251,
              -0.03638312965631485,
              -0.028693493455648422,
              -0.0037633036263287067,
              0.036301616579294205,
              0.0034525254741311073,
              -0.02328629046678543,
              0.026519743725657463,
              0.011969207786023617,
              0.0026203871238976717,
              -0.01569175347685814,
              0.004496944136917591,
              0.05230584740638733,
              0.02051476016640663,
              0.011887691915035248,
              0.029481476172804832,
              -0.0005655144923366606,
              0.0016761645674705505,
              0.01870783045887947,
              -0.0005846197018399835,
              -0.014346746727824211,
              -0.004975848365575075,
              -0.007594536989927292,
              0.006755605805665255,
              -0.008321384899318218,
              0.02429164946079254,
              0.02844894677400589,
              -0.018802933394908905,
              -0.01164993830025196,
              0.0006028758361935616,
              -0.00596422515809536,
              0.012471887283027172,
              0.013572348281741142,
              -0.01690090261399746,
              -0.00231130700558424,
              0.012397164478898048,
              -0.026981664821505547,
              0.016602011397480965,
              -0.006602764129638672,
              -0.005070949904620647,
              0.012132239528000355,
              0.006830328144133091,
              -0.012927016243338585,
              0.014428261667490005,
              0.020392486825585365,
              0.0012550007086247206,
              0.001064797630533576,
              0.02070496417582035,
              -0.015841199085116386,
              -0.027389243245124817,
              0.02861197665333748,
              0.030514007434248924,
              -0.014958113431930542,
              0.015134730376303196,
              0.008973509073257446,
              0.014958113431930542,
              0.01589554361999035,
              0.012763985432684422,
              -0.00374971772544086,
              0.027008837088942528,
              -0.035731006413698196,
              -0.008593102917075157,
              -0.013253078795969486,
              -0.024128619581460953,
              0.025568727403879166,
              -0.0280685406178236,
              0.003950110170990229,
              -0.0007968999561853707,
              -0.017362823709845543,
              0.0157460980117321,
              0.03200846165418625,
              0.013171562924981117,
              -0.0250796340405941,
              0.005325686186552048,
              -0.006157824769616127,
              0.01313080545514822,
              -0.009904146194458008,
              0.0034915851429104805,
              0.02168315090239048,
              -0.03043249249458313,
              -0.022158658131957054,
              0.010026419535279274,
              -0.0067623988725245,
              0.01692807301878929,
              -0.015460792928934097,
              -0.0007009492837823927,
              0.006840517744421959,
              0.028476117178797722,
              -0.04290438070893288,
              -0.027511516585946083,
              -0.005994793493300676,
              0.0066061606630682945,
              -0.01849045604467392,
              -0.005111707840114832,
              -0.006235943641513586,
              -0.01806929148733616,
              0.01493094116449356,
              -0.011466528289020061,
              0.005991396959871054,
              -0.011018192395567894,
              -0.003729338990524411,
              -0.009095782414078712,
              -0.0018952378304675221,
              -0.03757869452238083,
              -0.016955245286226273,
              0.0071801659651100636,
              0.016411809250712395,
              -0.006619746331125498,
              -0.009720736183226109,
              -0.010046797804534435,
              0.022593408823013306,
              -0.0049724518321454525,
              0.013667449355125427,
              -0.03423655405640602,
              -0.003525549778714776,
              0.0023265911731868982,
              0.007397541310638189,
              -0.011969207786023617,
              -0.042279426008462906,
              -0.004378067329525948,
              -0.018639901652932167,
              0.006735226605087519,
              -0.018599143251776695,
              -0.011344254948198795,
              0.006898257881402969,
              0.005896295420825481,
              -0.0025303801521658897,
              -0.013762551359832287,
              -0.00305683515034616,
              0.0001466856338083744,
              0.013803308829665184,
              -0.008348556235432625,
              0.017648128792643547,
              0.0006079705781303346,
              -0.012302063405513763,
              -0.020976681262254715,
              -0.029345616698265076,
              0.001504642190411687,
              -0.014849426224827766,
              -0.023394977673888206,
              -0.014672808349132538,
              0.031193304806947708,
              0.010685336776077747,
              -0.01571892574429512,
              -0.01691448874771595,
              -0.020840823650360107,
              -0.03855688124895096,
              -0.00805645901709795,
              -0.007723603397607803,
              0.017267722636461258,
              -0.0014180318685248494,
              0.017403582111001015,
              0.027593031525611877,
              0.02425089292228222,
              0.007105443626642227,
              -0.003943317569792271,
              0.0024964152835309505,
              0.000519237422849983,
              -0.0002857291838154197,
              -0.02801419608294964,
              -0.019061066210269928,
              -0.0009433733066543937,
              -0.047414910048246384,
              -0.000902615487575531,
              0.04279569163918495,
              -0.018667073920369148,
              0.017607370391488075,
              0.05630011111497879,
              -0.008993888273835182,
              -0.016425393521785736,
              -0.001525870175100863,
              0.015161902643740177,
              0.010196243412792683,
              0.02980753965675831,
              0.015977058559656143,
              -0.026207266375422478,
              -0.00731602543964982,
              0.026709945872426033,
              0.00895992387086153,
              -0.0329594761133194,
              -0.0018120239255949855,
              0.01653408259153366,
              -0.003362518735229969,
              0.003048344049602747,
              -0.019210509955883026,
              0.01650691032409668,
              -0.0030636282172054052,
              -0.008858028799295425,
              0.0046531823463737965,
              0.0147271528840065,
              -0.02967168018221855,
              -0.0028530461713671684,
              0.029101070016622543,
              -0.019400713965296745,
              -0.021397845819592476,
              0.022606994956731796,
              -0.01400709804147482,
              -0.0023690471425652504,
              0.016221605241298676,
              -0.014740738086402416,
              0.006843914277851582,
              -0.004707525949925184,
              0.0009951696265488863,
              -0.01847686991095543,
              -0.0004294428799767047,
              -0.029345616698265076,
              0.028122883290052414,
              -0.023150430992245674,
              0.030731383711099625,
              0.011106501333415508,
              -0.02942713350057602,
              0.006497472990304232,
              0.0005608443170785904,
              -0.020243041217327118,
              -0.014686394482851028,
              -0.014876597560942173,
              0.009299571625888348,
              -0.009007474407553673,
              0.014088613912463188,
              0.0007591140456497669,
              -0.01351121161133051,
              -0.015610238537192345,
              -0.00285983900539577,
              0.01832742430269718,
              -0.006847310811281204,
              -0.00011325149534968659,
              0.23715606331825256,
              -0.03877425566315651,
              0.010936676524579525,
              0.0009178996551781893,
              -0.019183339551091194,
              0.0034678096417337656,
              -0.008531966246664524,
              0.010950262658298016,
              -0.009170505218207836,
              -0.0037667001597583294,
              0.02168315090239048,
              0.002808891935274005,
              -0.010705715976655483,
              0.0050437781028449535,
              0.016411809250712395,
              -0.01331421546638012,
              -0.013090047053992748,
              -0.021139713004231453,
              -0.015542308799922466,
              0.005030191969126463,
              0.010006040334701538,
              0.020637033507227898,
              -0.013769344426691532,
              -0.02226734533905983,
              0.03228018060326576,
              -0.002003925386816263,
              0.0007862859056331217,
              -0.004143709782510996,
              0.0354321151971817,
              0.011446149088442326,
              -0.009041438810527325,
              0.005003020167350769,
              -0.000519237422849983,
              -0.0013653864152729511,
              -0.0029005969408899546,
              0.003051740350201726,
              0.006072912365198135,
              0.009428638033568859,
              0.019061066210269928,
              -0.0006410862552002072,
              0.04154578596353531,
              0.021166885271668434,
              -0.008708584122359753,
              -0.019781120121479034,
              0.010637786239385605,
              0.009795458056032658,
              -0.020759306848049164,
              -0.012302063405513763,
              -0.008008908480405807,
              0.029698852449655533,
              -0.032089974731206894,
              0.022905884310603142,
              0.02465846948325634,
              0.037306975573301315,
              -0.0007421316695399582,
              -0.01732206530869007,
              0.00954411830753088,
              0.0015487964265048504,
              0.03279644623398781,
              -0.012994945980608463,
              -0.017362823709845543,
              0.009292778559029102,
              -0.005828365683555603,
              0.023367807269096375,
              -0.0037395283579826355,
              0.02627519704401493,
              -0.038692738860845566,
              0.04312175512313843,
              0.02268850989639759,
              0.014102199114859104,
              0.008919165469706059,
              -0.00975470058619976,
              -0.013558762148022652,
              0.004568270407617092,
              -0.02624802477657795,
              -0.02490301802754402,
              0.016941659152507782,
              0.01709110476076603,
              0.015555894933640957,
              0.03448110073804855,
              0.002479432849213481,
              -0.019047480076551437,
              -0.027538688853383064,
              -0.02209072932600975,
              -0.018585557118058205,
              -0.04298589378595352,
              -0.0010792326647788286,
              -0.020025666803121567,
              0.0010299836285412312,
              -0.03521474078297615,
              0.01729489490389824,
              -0.032334521412849426,
              -0.006168013904243708,
              -0.013490832410752773,
              0.02327270433306694,
              0.010732888244092464,
              0.004731301683932543,
              0.018177980557084084,
              0.010848368518054485,
              -0.010026419535279274,
              -0.0349973663687706,
              0.056626174598932266,
              0.017580198124051094,
              -0.0033455363009124994,
              -0.014292402192950249,
              0.004659975413233042,
              0.0033828974701464176,
              0.0019954340532422066,
              0.012383579276502132,
              -0.01074647344648838,
              0.0038482157979160547,
              -0.01532493345439434,
              0.013354972936213017,
              -0.016221605241298676,
              0.0037157528568059206,
              0.0009289382142014802,
              0.011955621652305126,
              0.012369993142783642,
              0.03024228848516941,
              -0.018001362681388855,
              0.003722545923665166,
              -0.019414300099015236,
              -0.0002935835509561002,
              0.030079258605837822,
              0.0014358634361997247,
              -0.01493094116449356,
              0.003742924891412258,
              0.007234510034322739,
              -0.02310967445373535,
              0.0024318823125213385,
              -0.004102952312678099,
              -0.016004230827093124,
              0.01123556774109602,
              -0.02107178419828415,
              -0.03575817868113518,
              0.013993511907756329,
              0.009965282864868641,
              -0.004975848365575075,
              -0.017226964235305786,
              0.012118653394281864,
              0.00012758040975313634,
              0.004038419108837843,
              0.005233981180936098,
              -0.01830025389790535,
              -0.011758625507354736,
              -0.009584876708686352,
              0.013205528259277344,
              -0.015868371352553368,
              -0.006881275679916143,
              -0.008776513859629631,
              -0.01811004988849163,
              -0.028394602239131927,
              -0.0058079869486391544,
              -0.03524191305041313,
              0.020039252936840057,
              0.010603821836411953,
              -0.008389314636588097,
              -0.022376032546162605,
              0.017362823709845543,
              0.009109368547797203,
              -0.020840823650360107,
              -0.005729867611080408,
              0.0032860978972166777,
              -0.023924829438328743,
              -0.001446052803657949,
              -0.004181071184575558,
              -0.17389994859695435,
              -0.011317082680761814,
              0.016778629273176193,
              -0.040241535753011703,
              0.008898787200450897,
              -0.0012066008057445288,
              0.01709110476076603,
              -0.02171032316982746,
              -0.038502536714076996,
              0.012981359846889973,
              0.02085440792143345,
              0.00885123573243618,
              -0.027008837088942528,
              -0.017063932493329048,
              -0.017036762088537216,
              0.004619217477738857,
              0.0025337766855955124,
              0.021737493574619293,
              0.0030670245178043842,
              0.0015411543427035213,
              0.042034879326820374,
              -0.0326605848968029,
              0.017661714926362038,
              -0.014075027778744698,
              0.00945581030100584,
              -0.0028615372721105814,
              -0.002742660464718938,
              0.009985661134123802,
              -0.0026662396267056465,
              -0.03722545877099037,
              0.008599895983934402,
              0.006239340174943209,
              0.025691000744700432,
              -0.009184091351926327,
              0.0023147035390138626,
              0.017199791967868805,
              0.016438979655504227,
              -0.006888068746775389,
              -0.009347123093903065,
              0.0025949133560061455,
              0.031111789867281914,
              0.01929202675819397,
              0.016547666862607002,
              -0.01400709804147482,
              -0.019061066210269928,
              0.020786479115486145,
              0.020134354010224342,
              -0.014387504197657108,
              0.021723909303545952,
              -0.019726775586605072,
              0.00318420329131186,
              -0.00817193929105997,
              -0.0016048384131863713,
              0.013151183724403381,
              0.023965587839484215,
              0.009190884418785572,
              -0.0033795011695474386,
              0.01666994020342827,
              -0.014795082621276379,
              0.004391652997583151,
              -0.00019243202405050397,
              -0.024617712944746017,
              0.011330668814480305,
              -0.017145449295639992,
              -0.02725338377058506,
              -0.01262133289128542,
              -0.017756815999746323,
              0.0047245086170732975,
              0.0006156126619316638,
              0.011004606261849403,
              -0.015447206795215607,
              -0.013844067230820656,
              -0.0068608964793384075,
              0.00015676894690841436,
              0.010474755428731441,
              0.032905131578445435,
              -0.009238434955477715,
              -0.005444562993943691,
              0.019821878522634506,
              0.009978868998587132,
              -0.013185149058699608,
              0.032714929431676865,
              -0.018599143251776695,
              0.018965963274240494,
              -0.006385388784110546,
              0.0009892258094623685,
              0.0009255417389795184,
              0.018558386713266373,
              -0.010624200105667114,
              -0.01869424618780613,
              0.005186430178582668,
              -0.03024228848516941,
              0.013205528259277344,
              0.0012210358399897814,
              0.0019852446857839823,
              0.011371427215635777,
              0.02310967445373535,
              0.0039161453023552895,
              0.004201449919492006,
              -0.015474379062652588,
              0.0053902193903923035,
              0.003841422963887453,
              0.009319950826466084,
              0.029399961233139038,
              0.011588801629841328,
              0.027959851548075676,
              -0.024413922801613808,
              0.013816894963383675,
              0.029726022854447365,
              0.0038244405295699835,
              -0.02429164946079254,
              -0.016642769798636436,
              0.015053214505314827,
              0.020650619640946388,
              0.013796515762805939,
              0.015352105721831322,
              -0.0035459287464618683,
              -0.017947018146514893,
              0.011208395473659039,
              -0.014061441645026207,
              0.03415503725409508,
              0.0034763009753078222,
              -0.01793343387544155,
              0.02168315090239048,
              -0.02207714319229126,
              -0.04717036336660385,
              -0.09417769312858582,
              -0.038094960153102875,
              -0.019658846780657768,
              0.0047143190167844296,
              -0.003844819264486432,
              0.016574839130043983,
              0.02688656374812126,
              0.018830103799700737,
              0.00039420436951331794,
              0.03282361850142479,
              -0.019047480076551437,
              -0.013558762148022652,
              0.002316401805728674,
              -0.02687297761440277,
              0.008104009553790092,
              0.0024607523810118437,
              0.0016260665142908692,
              0.004710922483354807,
              -0.017960604280233383,
              0.01969960518181324,
              0.0030789123848080635,
              -0.030785726383328438,
              0.032714929431676865,
              -0.03184542804956436,
              0.031138960272073746,
              -0.026207266375422478,
              -0.031763914972543716,
              0.008307798765599728,
              -0.0024641486816108227,
              -0.016778629273176193,
              -0.012336027808487415,
              -0.02926410175859928,
              9.860416321316734e-05,
              -0.014944527298212051,
              0.0032657189294695854,
              -0.008756134659051895,
              -0.016194432973861694,
              -0.027348484843969345,
              -0.003950110170990229,
              -0.029970571398735046,
              0.0020327954553067684,
              0.0007306685438379645,
              0.0004997076466679573,
              -0.01592271402478218,
              -0.012132239528000355,
              -0.04002416133880615,
              -0.010963848792016506,
              0.014387504197657108,
              0.011317082680761814,
              -0.007295646704733372,
              -0.014876597560942173,
              -0.008919165469706059,
              -0.03399200737476349,
              -0.007064685691148043,
              0.01313080545514822,
              0.012852293439209461,
              0.0032708137296140194,
              0.004452789667993784,
              -0.011147258803248405,
              -0.02171032316982746,
              0.007954563945531845,
              -0.007397541310638189,
              -0.01892520673573017,
              0.0072888536378741264,
              0.0010936676990240812,
              -0.004371274262666702,
              -0.00934033002704382,
              -0.025568727403879166,
              0.012600953690707684,
              -0.0306226946413517,
              0.0029464494436979294,
              0.014469020068645477,
              0.0035798936150968075,
              0.019604502245783806,
              -0.021207643672823906,
              -0.010705715976655483,
              -0.015515136532485485,
              0.021574463695287704,
              0.005845348350703716,
              -0.01609933190047741,
              -0.03228018060326576,
              -0.022729268297553062,
              0.01729489490389824,
              -0.025025291368365288,
              -0.006803156342357397,
              -0.00766246672719717,
              -0.024644885212183,
              -0.00037127811810933053,
              -0.0038040615618228912,
              -0.029698852449655533,
              0.012668883427977562,
              0.03045966476202011,
              -0.0013450074475258589,
              -0.013191942125558853,
              -0.006235943641513586,
              0.0068608964793384075,
              -0.025758931413292885,
              0.004374670796096325,
              0.006096688099205494,
              0.045458536595106125,
              -0.01364027801901102,
              -0.00641595758497715,
              -0.052822113037109375,
              0.008803685195744038,
              0.0031909963581711054,
              -0.022946642711758614,
              0.009218056686222553,
              0.023775383830070496,
              0.037497177720069885,
              0.009421844966709614,
              -0.013470453210175037,
              0.030731383711099625,
              -0.01073968131095171,
              0.02168315090239048,
              0.013049289584159851,
              0.0001267312909476459,
              -0.012994945980608463,
              -0.024821501225233078,
              0.02385690063238144,
              -0.021968455985188484,
              0.015814026817679405,
              0.024726400151848793,
              -0.003197789192199707,
              -0.011378219351172447,
              0.012859086506068707,
              0.01253981702029705,
              -0.0019173149485141039,
              -0.0010130011942237616,
              -0.015433620661497116,
              0.014686394482851028,
              -0.01733565144240856,
              -0.02268850989639759,
              -0.002095630392432213,
              -0.019590916112065315,
              0.006246133241802454,
              0.00995169673115015,
              -0.010889125987887383,
              -0.002598309889435768,
              0.0007387351361103356,
              0.008830857463181019,
              0.02268850989639759,
              0.009462603367865086,
              -0.01194203644990921,
              -0.026112165302038193,
              0.02882935293018818,
              -0.025976305827498436,
              0.0027494532987475395,
              -0.0033964836038649082,
              0.004632803611457348,
              -0.002212808933109045,
              -0.003817647462710738,
              -0.006483886856585741,
              0.024155789986252785,
              0.02108537033200264,
              0.009435431100428104,
              -0.022973814979195595,
              -0.01631670631468296,
              0.005447959527373314,
              0.013551969081163406,
              0.026152923703193665,
              -0.026913736015558243,
              -0.009809044189751148,
              0.017607370391488075,
              0.007825498469173908,
              0.002411503344774246,
              -0.00325383129529655,
              0.0016421998152509332,
              0.012859086506068707,
              0.019237682223320007,
              -0.0017186206532642245,
              -0.009197677485644817,
              -0.034535445272922516,
              -0.0014018985675647855,
              -0.0013008532114326954,
              0.012071102857589722,
              0.011575215496122837,
              0.03564948961138725,
              0.006229150574654341,
              0.00017884607950691134,
              0.00716658029705286,
              -0.01463205087929964,
              0.03662768006324768,
              0.015596652403473854,
              -0.028965210542082787,
              -0.01342969574034214,
              -0.017770402133464813,
              0.004829799756407738,
              -0.0027358673978596926,
              0.006796363741159439,
              -0.0024556575808674097,
              -0.01402068417519331,
              -0.0037870791275054216,
              0.005712885409593582,
              0.007988529279828072,
              0.002396219177171588,
              0.0053800297901034355,
              -0.004089366178959608,
              0.009809044189751148,
              0.010984227992594242,
              0.00021461529831867665,
              0.012607746757566929,
              -0.013592727482318878,
              0.001277926960028708,
              0.0013322706799954176,
              -0.004595442209392786,
              -0.005634766072034836,
              -0.014536949805915356,
              0.020990267395973206,
              -0.019020307809114456,
              -0.028313087299466133,
              0.00040587977855466306,
              0.0034151640720665455,
              0.010298137553036213,
              0.020243041217327118,
              -0.015257003717124462,
              0.0015292667085304856,
              -0.02861197665333748,
              0.03423655405640602,
              0.013735379092395306,
              -0.017457924783229828,
              -0.008498001843690872,
              0.007995322346687317,
              0.007085064426064491,
              -0.00015952858666423708,
              0.028557633981108665,
              0.0013874635333195329,
              0.01592271402478218,
              0.021642392501235008,
              0.04121972247958183,
              -0.010481548495590687,
              0.00894633773714304,
              0.008287419565021992,
              -0.013253078795969486,
              -0.017145449295639992,
              -0.005529474932700396,
              -0.019020307809114456,
              -0.00023456964117940515,
              0.0033217607997357845,
              0.0027137903962284327,
              0.022172244265675545,
              -0.0034253536723554134,
              0.0892324149608612,
              0.02586761862039566,
              0.004996227100491524,
              0.0027341691311448812,
              -0.0010401731124147773,
              0.025813274085521698,
              0.019210509955883026,
              0.014346746727824211,
              -0.0023418753407895565,
              0.00481961015611887,
              0.02449543960392475,
              0.01633029244840145,
              0.006545023526996374,
              -0.012492266483604908,
              -0.014795082621276379,
              0.006239340174943209,
              0.011595594696700573,
              -0.0030924982856959105,
              0.01787908934056759,
              -0.013103633187711239,
              0.046056315302848816,
              -0.026492571458220482,
              0.007458677981048822,
              0.011955621652305126,
              -0.03461695834994316,
              -0.006049137096852064,
              0.010760059580206871,
              0.015664581209421158,
              -0.018667073920369148,
              -0.039073146879673004,
              0.019658846780657768,
              -0.002095630392432213,
              -0.028150055557489395,
              -0.012845500372350216,
              0.022213002666831017,
              -0.002409805078059435,
              -0.001168390386737883,
              -0.005818176083266735,
              0.018245909363031387,
              0.010617407038807869,
              0.01501245703548193,
              0.01690090261399746,
              -0.004459582734853029,
              -0.03497019410133362,
              -0.01194203644990921,
              -0.0059676216915249825,
              -0.02008001133799553,
              -0.027348484843969345,
              -0.016588425263762474
            ],
            "existingAnswer": true,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            },
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "",
            "question": "What is the purpose of BERT?",
            "retrievedDocs": ""
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
                  ],
                  "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1694832912.457556,
          "end_time": 1694832912.476555,
          "error": null,
          "children": null,
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.033315
      },
      "result": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\n\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n9\n1\n0\n2\n\ny\na\nM\n4\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n0\n8\n4\n0\n.\n0\n1\n8\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\n\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n\n1\n\nIntroduction\n\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\n\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\n\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations\nfrom Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\n\n \n \n \n \n \n \n\fword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\nIn addi-\ntrain a deep bidirectional Transformer.\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n\n2 Related Work\n\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n\n2.1 Unsupervised Feature-based Approaches\n\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\n\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\n\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n\n2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc."
              ],
              "answer": "The purpose of BERT is to pre-train deep bidirectional representations from unlabeled text and then fine-tune these representations for a wide range of natural language processing tasks.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be \ufb01ne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeci\ufb01c architecture modi\ufb01cations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce \ufb01ne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and \ufb01ne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speci\ufb01c architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The \ufb01ne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speci\ufb01c parameters, and is trained on the\\ndownstream tasks by simply \ufb01ne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the \ufb01ne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying \ufb01ne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the \ufb01ne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a \u201cmasked lan-\\nguage model\u201d (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na \u201cnext sentence prediction\u201d task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n\u2022 We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n\u2022 We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speci\ufb01c architectures.\\n\\n\u2022 BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brie\ufb02y review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signi\ufb01cant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speci\ufb01c architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.']<br>        Question: What is the purpose of BERT?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<1> What are the two unsupervised tasks used to pre-train BERT?\n<2> How are sentence pairs represented in the input to BERT?\n<3> How does BERT compare to other models in terms of performance on natural language processing tasks?",
              "error": ""
            }
          }
        ]
      }
    }
  ]
}